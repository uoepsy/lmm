[
  {
    "objectID": "00_datasets.html",
    "href": "00_datasets.html",
    "title": "Practice Datasets",
    "section": "",
    "text": "FORTHCOMING"
  },
  {
    "objectID": "00_lm_assumpt.html",
    "href": "00_lm_assumpt.html",
    "title": "LM Troubleshooting",
    "section": "",
    "text": "In the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below."
  },
  {
    "objectID": "00_lm_assumpt.html#tests-of-the-coefficients",
    "href": "00_lm_assumpt.html#tests-of-the-coefficients",
    "title": "LM Troubleshooting",
    "section": "Tests of the coefficients",
    "text": "Tests of the coefficients\n\nlibrary(lmtest)\nlibrary(sandwich)\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC0\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.0383561  0.8635215 -0.0444  0.96466  \nx            0.4924743  0.2631998  1.8711  0.06438 .\nx2b          1.2305743  0.7625359  1.6138  0.10985  \nx2c         -0.0010129  0.9210642 -0.0011  0.99912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "00_lm_assumpt.html#model-comparisons",
    "href": "00_lm_assumpt.html#model-comparisons",
    "title": "LM Troubleshooting",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nmod_res &lt;- lm(y ~ 1 + x, data = troubledf2)\nmod_unres &lt;- lm(y ~ 1 + x + x2, data = troubledf2)\nwaldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = \"HC0\"))\n\nWald test\n\nModel 1: y ~ 1 + x\nModel 2: y ~ 1 + x + x2\n  Res.Df Df      F Pr(&gt;F)\n1     98                 \n2     96  2 1.8704 0.1596"
  },
  {
    "objectID": "00_lm_assumpt.html#boostrapped-coefficients",
    "href": "00_lm_assumpt.html#boostrapped-coefficients",
    "title": "LM Troubleshooting",
    "section": "Boostrapped Coefficients",
    "text": "Boostrapped Coefficients\nWe can get out some bootstrapped confidence intervals for our coefficients using the car package:\n\nlibrary(car)\n# bootstrap our model coefficients\nboot_mod &lt;- Boot(mod)\n# compute confidence intervals\nConfint(boot_mod)\n\nBootstrap bca confidence intervals\n\n              Estimate        2.5 %    97.5 %\n(Intercept)  1.5156272  0.269082523 3.0150279\nx            0.3769504  0.005839124 0.7201455\nx2b          0.2497345 -0.718176725 1.3009887\nx2c         -0.1305828 -1.015342466 0.6681926\nx2d          1.1534433  0.031319608 2.4027965"
  },
  {
    "objectID": "00_lm_assumpt.html#bootstrapped-anova",
    "href": "00_lm_assumpt.html#bootstrapped-anova",
    "title": "LM Troubleshooting",
    "section": "Bootstrapped ANOVA",
    "text": "Bootstrapped ANOVA\nIf we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the incremental addition of each predictor, we can get bootstrapped p-values from the ANOVA.boot function in the lmboot package.\nOur original ANOVA:\n\nanova( lm(y~x+x2, data = df) )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nx          1  20.64 20.6427  5.4098 0.02215 *\nx2         3  25.60  8.5331  2.2363 0.08902 .\nResiduals 95 362.50  3.8158                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd our bootstrapped p-values:\n\nlibrary(lmboot)\nmy_anova &lt;- ANOVA.boot(y~x+x2, data = df, \n                       B = 1000)\n# these are our bootstrapped p-values:\nmy_anova$`p-values`\n\n[1] 0.023 0.100\n\n#let's put them alongside our original ANOVA table:\ncbind(\n  anova( lm(y~x+x2, data = df) ),\n  p_bootstrap = c(my_anova$`p-values`,NA)\n)\n\n          Df    Sum Sq   Mean Sq  F value     Pr(&gt;F) p_bootstrap\nx          1  20.64273 20.642727 5.409835 0.02215056       0.023\nx2         3  25.59936  8.533122 2.236273 0.08902175       0.100\nResiduals 95 362.49886  3.815777       NA         NA          NA"
  },
  {
    "objectID": "00_lm_assumpt.html#other-things",
    "href": "00_lm_assumpt.html#other-things",
    "title": "LM Troubleshooting",
    "section": "Other things",
    "text": "Other things\nWe can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.\nFor example, to bootstrap the \\(R^2\\) for the model lm(y~x+x2), we would create a little function called rsq:\n\nrsq &lt;- function(data, indices){\n  # this is the bootstrap resample\n  bdata &lt;- data[indices,]\n  # this is the model, fitted to the resample\n  fit &lt;- lm(y ~ x + x2, data = bdata)\n  # this returns the R squared\n  return(summary(fit)$r.square)\n}\n\nWe then use the boot package, giving 1) our original data and 2) our custom function to the boot() function, and compute some confidence intervals:\n\nlibrary(boot)\nbootrsq_results &lt;- boot(data = df, statistic = rsq, R = 1000)\nboot.ci(bootrsq_results, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootrsq_results, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.0174,  0.2196 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable"
  },
  {
    "objectID": "00_lm_assumpt.html#footnotes",
    "href": "00_lm_assumpt.html#footnotes",
    "title": "LM Troubleshooting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nThis method finds an appropriate value for \\(\\lambda\\) such that the transformation \\((sign(x) |x|^{\\lambda}-1)/\\lambda\\) results in a close to normal distribution.↩︎\nThis is a special formulation of something called a ‘Sandwich’ estimator!↩︎\nor \\(sign( rank(|y|) )\\)↩︎"
  },
  {
    "objectID": "01_clustered.html",
    "href": "01_clustered.html",
    "title": "1: Group-Structured Data",
    "section": "",
    "text": "This reading:\n\nA refresher on the linear regression model\n\nAn introduction to group-structured (or ‘clustered’) data\nWorking with group-structured data (sample sizes, ICC, visualisations)"
  },
  {
    "objectID": "01_clustered.html#clusters-clusters-everywhere",
    "href": "01_clustered.html#clusters-clusters-everywhere",
    "title": "1: Group-Structured Data",
    "section": "Clusters clusters everywhere",
    "text": "Clusters clusters everywhere\nThe idea of observing “children in schools” is just one such example of clustering that we might come across. This same hierarchical data structure can be found in other settings, such as patients within medical practices, employees within departments, people within towns etc. These sort of groups are higher level observations that we might sample (i.e. I randomly sample 20 schools, and then from each school randomly sample 30 children). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a Repeated Measures study we have individual experimental trials clustered within participants. Longitudinal studies exhibit the same data structure but have time-ordered observations clustered within people.\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters4. Table 1 shows just a few examples of different levels of clustering that may arise from different types of study.\n\n\n\n\n\n\n\nTable 1:  Various different study designs will give rise to clustered data. \n  \n    \n       \n      Cross Sectional\n      Repeated Measures\n      Longitudinal\n    \n  \n  \n    Level n\n...\n...\n...\n    ...\n...\n...\n...\n    Level 3\nSchool\n...\nFamilies\n    Level 2\nClassroom\nParticipants\nPeople\n    Level 1 (Observations)\nChildren\nExperimental Stimuli\nTime\n  \n  \n  \n\n\n\n\n\nThe common thread throughout all these designs is the hierarchy. At the lowest level of our hierarchy is the individual observed thing. For some designs, individual people might be the lowest observation level, for others, people might be the clusters (i.e. we have multiple data points per person)."
  },
  {
    "objectID": "01_clustered.html#what-are-these-groupsclusters",
    "href": "01_clustered.html#what-are-these-groupsclusters",
    "title": "1: Group-Structured Data",
    "section": "What are these ‘groups’/‘clusters’?",
    "text": "What are these ‘groups’/‘clusters’?\nAt the fundamental level, we are using the term ‘cluster’ here to refer to a grouping of observations. In fact, we use the terms “clusters” and “groups” interchangeably in this context, so it’s worth taking a bit of time to try and understand the kind of groupings that we’re talking about (and how we think about them).\n\n\n“Clusters” are just “groups”.\n\nWhen we talk about clustered data, the groups we are discussing are typically those that can be thought of as a random sample of higher level units.\n\nOften the specific group-differences are not of interest.\n\n\nContrast the idea of ‘clusters’ with how we think about other sorts of groupings. In a study that looks at “how does heart rate differ between people taking placebo vs aspirin vs beta-blockers?” (Figure 7 LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we’ll use the same drugs (they’re not just a random sample of drugs - the x-axis of our LH plot in Figure 7 will be the same).\nIf we are interested in “what is the average grade at GCSE?”, and we have children grouped into different schools (Figure 7 RH plot), we are probably not interested in all the specific differences between grades in Broughton High School vs Gryffe High School etc. If we were to run our study again, we don’t collect data from the same set of schools. We can view these schools as ‘clusters’ - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).\n\n\n\n\n\nFigure 7: Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\n\n\n\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the type of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as “mixed effects modelling”), which is a regression modelling technique that allows us to explore questions such as these (and many more).5\n\n\n\n\n\n\noptional “univariate”and “multivariate”\n\n\n\n\n\nIn “univariate” statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.\nIn “multivariate” statistics, there are more sources of variation. For the “children in schools” example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another.\n\n\n\n\n\n\n\n\n\noptional: “Panel data”\n\n\n\n\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as ‘panel data’. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:\n\n\n\n\n\nFigure 8: Panels of data\n\n\n\n\n\n\n\n\n\nFigure 9: Panels of panels of data"
  },
  {
    "objectID": "01_clustered.html#determining-sample-sizes",
    "href": "01_clustered.html#determining-sample-sizes",
    "title": "1: Group-Structured Data",
    "section": "Determining Sample Sizes",
    "text": "Determining Sample Sizes\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different levels.\nIn the description of the SchoolMot data above we are told the relevant numbers:\n\n\n\n\n\n\n  \n    \n       \n      Unit\n      Sample Size\n    \n  \n  \n    Level 2\nSchool\n30\n    Level 1 (Observations)\nChildren\n900\n  \n  \n  \n\n\n\n\nWe can check this in our data:\n\nschoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n\n[1] 900\n\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n\n[1] 30\n\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from Calderglen High School and only 10 from Broughton High School, or do we have the same number in each?\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the summary() function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):\n\nschoolmot |&gt;\n  count(schoolid) |&gt;\n  summary()\n\n   schoolid               n     \n Length:30          Min.   :30  \n Class :character   1st Qu.:30  \n Mode  :character   Median :30  \n                    Mean   :30  \n                    3rd Qu.:30  \n                    Max.   :30"
  },
  {
    "objectID": "01_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "href": "01_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "title": "1: Group-Structured Data",
    "section": "ICC - Quantifying clustering in an outcome variable",
    "text": "ICC - Quantifying clustering in an outcome variable\nThe IntraClass Correlation Coefficient (ICC) is a measure of how much variation in a variable is attributable to the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol \\(\\rho\\):7\n\\[\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n\\]\nThis is illustrated in the Figure 10 below, in which our continuous outcome variable (children’s grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the “between-group variance” as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the “within-group variance” as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\n\nCode\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n\n\n\n\n\nFigure 10: Variance in grades between schools. Data from https://uoepsy.github.io/data/schoolmot.csv\n\n\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can extract it from a fitted model.\nIn the school motivation data (visualised above), it’s estimated that 22% of the variance in grades is due to school-related differences:\n\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n\n[1] 0.2191859\n\n\n\n\n\n\n\n\noptional: calculating ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC:\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a couple of new columns. The first is the overall mean of \\(y\\), and the second is the mean of \\(y\\) for each group. Note that we calculate this by first using group_by to make the subsequent operation (the mutate) be applied to each group. To ensure that the grouping does not persist after this, we’ve passed it to ungroup at the end.\n\nschoolmot &lt;- \n  schoolmot |&gt; \n  mutate(\n    grand_mean = mean(grade)\n  ) |&gt;\n  group_by(schoolid) |&gt;\n  mutate(\n    group_mean = mean(grade)\n  ) |&gt;\n  ungroup()\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nschoolmot &lt;- schoolmot |&gt; \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n\nFinally, we divide them by the degrees of freedom. Our degrees of freedom for our between group variance \\(30 \\text{ groups} - 1 \\text{ grand mean}=29\\)\nOur degrees of freedom for our within group variance is \\(900 \\text{ observations} - 30 \\text{ groups}=870\\)\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n\nAnd calculate the ICC!!!\nThe 29 here is the \\(k-1\\) in the formula above, where \\(k\\) is the number of observations within each group.\n\n# ICC\n(msb-mse) /(msb + (29*mse))\n\n[1] 0.2191859\n\n\n\n\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\nWe can think of it as the average correlation between all possible pairs of observations from the same group. Suppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (Figure 11). The ICC is the correlation between such pairs.\n\n\n\n\n\nFigure 11: ICC is the correlation of randomly drawn pairs from the same group\n\n\n\n\n\n\n\n\n\n\noptional: a little simulation\n\n\n\n\n\nWe can actually do the “randomly drawn pair of observations from the same group” via simulation.\nThe code below creates a function for us to use. Can you figure out how it works?\n\nget_random_pair &lt;- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n\nTry it out, by running it several times.\n\nget_random_pair()\n\n[1] 28.35 50.84\n\n\nNow let’s make our computer do it loads and loads of times:\n\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims &lt;- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims &lt;- t(sims)\ncor(sims[,1], sims[,2])\n\n[1] 0.2097805\n\n\n\n\n\n\n\n\n\n\n\noptional: correlations from group-structured data\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n# A tibble: 7 × 3\n  cluster observation y    \n* &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 group_1 1           4    \n2 group_1 2           2    \n3 group_2 1           4    \n4 group_2 2           2    \n5 group_3 1           7    \n6 group_3 2           5    \n7 ...     ...         ...  \n\n\nThe ICC for this data is 0.18.\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, they’re just random draws from a group.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen any of these:\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\nWhy ICC?\nThe ICC tells us the proportion of the total variability in an outcome variable that is attributable to the differences between groups/clusters. It ranges from 0 to 1.\nThis helps us to assess the appropriateness of using a multilevel approach. If the ICC is high, it suggests that a large amount of the variance is at the cluster level (justifying the use of multilevel modeling to account for this structure).\nThere are no cut-offs - the interpretation of ICC values is inherently field-specific, as what constitutes a high or low ICC depends on the nature of the outcome variable, and the hierarchical structure within a particular research context."
  },
  {
    "objectID": "01_clustered.html#visualisations",
    "href": "01_clustered.html#visualisations",
    "title": "1: Group-Structured Data",
    "section": "Visualisations",
    "text": "Visualisations\nWhen we’re visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. For instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens within a given school (see e.g. Figure 5 from earlier).\nSome useful ggplot tools here are:\n\nfacet_wrap() - make a separate little plot for each level of a grouping variable\nthe group aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n\n\nfacets\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point() +\n  facet_wrap(~schoolid)\n\n\n\n\n\n\n\n\n\n\ngroup\n\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)"
  },
  {
    "objectID": "01_clustered.html#footnotes",
    "href": "01_clustered.html#footnotes",
    "title": "1: Group-Structured Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nWith the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we can actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).↩︎\nor “mean squares residual”↩︎\nIt’s “turtles all the way down”↩︎\nWhile multilevel models are great for multilevel questions, sometimes we may be interested in only things that occur at “level 1” (the lowest observation level). While not the focus of this course, there are also many other methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply “account for the nuisance clustering”.↩︎\nNote, this is not true for a set of analytical methods called “cluster analysis”, which attempts to identify clusters that haven’t been measured/observed (or may not even ‘exist’ in any real sense of the word).↩︎\nalthough this symbol get used for lots of other correlation-y things too!↩︎\nanother way to think of this is that we could figure out the the estimated grade ~ funding difference exactly from the set of grade ~ schoolid coefficients.↩︎"
  },
  {
    "objectID": "02_lmm.html",
    "href": "02_lmm.html",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "",
    "text": "This reading:\n\nIntroducing the multilevel model (MLM)\nHow the MLM achieves partial pooling\nFitting multilevel models in R\nModel estimation and convergence\n\n\n\n\n\n\n\ndifferent names for the same thing\n\n\n\n\n\nThe methods we’re going to start to look at are known by lots of different names (see Figure 1). The core idea is that model parameters vary at more than one level..\n\n\n\n\n\nFigure 1: size weighted by hits on google scholar search (sept 2020)"
  },
  {
    "objectID": "02_lmm.html#random-intercepts",
    "href": "02_lmm.html#random-intercepts",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "random intercepts",
    "text": "random intercepts\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.1 Then, we can take a coefficient \\(b_?\\) and allow it to be different for each cluster \\(i\\) by adding the suffix \\(b_{?i}\\). Below, we have done this for our intercept \\(b_0\\), which has become \\(b_{0i}\\).\nHowever, we also need to define these differences in some way, and the multilevel model does this by expressing each cluster’s intercept as a deviation (\\(\\zeta_{0i}\\) for cluster \\(i\\), below) from a fixed number (\\(\\gamma_{00}\\), below). Because these differences are to do with the clusters (and not the individual observations within them), we often write these as a “level 2 equation”:\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nmixed-effects notation\n\n\n\n\n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:\n\\[\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n\\]\nThis notation typically corresponds with the “mixed effects” terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:\n\\[\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n\\]\n\n\n\nReturning to our school children’s grade example, we can fit a model with “random intercepts for schools”, which would account for some schools having higher grades, some having lower grades, etc.\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n\\] If we consider one of our schools (e.g. “Beeslack Community High School”) we can see that our model predicts that this school has higher grades than most other schools (Figure 3). We can see how this is modelled as a deviation \\(\\zeta_{0\\text{B}}\\) (B for Beeslack) from some fixed value \\(\\gamma_{00}\\).\n\n\n\n\n\nFigure 3: Fitted values from a multilevel model with random intercepts for schools\n\n\n\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of lm(grade ~ motiv + schoolid)), which would also estimate a difference for each cluster?\n\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we can get these out). We are estimating a distribution of deviations.\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the variance of a normal distribution of clusters.\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n\n\n\na fixed intercept \\(\\gamma_{00}\\)\n\nthe variance with which schools deviate from the fixed intercept \\(\\sigma^2_0\\)\n\na fixed slope for motiv \\(b_1\\)\n\nand we also need the residual variance too \\(\\sigma^2_\\varepsilon\\)\n\n\n\n\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nRemember, \\(\\sim N(m,s)\\) is a way of writing “are normally distributed with a mean of \\(m\\) and a standard deviation of \\(s\\)”. So the \\(\\zeta_{0i} \\sim N(0,\\sigma_0)\\) bit is saying that the school deviations from the fixed intercept are modelled as a normal distribution, with a mean of 0, and a standard deviation of \\(\\sigma_0\\) (which gets estimated by our model).\nThis can be seen in Figure 4 - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.\n\n\n\n\n\nFigure 4: grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance of school-level deviations)."
  },
  {
    "objectID": "02_lmm.html#random-slopes",
    "href": "02_lmm.html#random-slopes",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "random slopes",
    "text": "random slopes\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). For instance, we can allow the slope of \\(x\\) on \\(y\\) to be different for each cluster, by specifying in our model that \\(b_{1i}\\) is a distribution of cluster deviations \\(\\zeta_{1i}\\) around the fixed slope \\(\\gamma_{10}\\).\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\nWhen we have random intercepts and random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying “random intercepts and slopes are normally distributed with mean of 0 and standard deviations of \\(\\sigma_0\\) and \\(\\sigma_1\\) respectively, and with a correlation of \\(\\rho_{01}\\)”. We’ll see more on this in future weeks, so don’t worry too much right now.\nIn Figure 5, we can see now that both the intercept and the slope of grades across motivation are varying by-school.\n\n\n\n\n\nFigure 5: predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\n\n\n\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations \\(\\zeta_{1i}\\) around a fixed estimate \\(\\gamma_{10}\\).\nSo each group (school) now has, as visualised in Figure 6:\n\na deviation from the fixed intercept\na deviation from the fixed slope\n\n\n\n\n\n\nFigure 6: random intercepts and random slopes\n\n\n\n\nWhile it’s possible to show the distribution of intercepts on the left hand side of our grade ~ motiv plot, it’s hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in Figure 7. We can see, for instance, that “Hutcheson’s Grammar School” has a higher intercept, but a lower slope.\n\n\n\n\n\nFigure 7: grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance of school-level deviations)\n\n\n\n\n\n\n\n\n\n\noptional: joint distribution of intercept and slopes\n\n\n\n\n\nWhen we have random intercepts and slopes in our model, we don’t just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. This comes back to the part of the equation we mentioned briefly above, where we used:\n\n\\(\\sigma_0\\) to represent the standard deviation of intercept deviations\n\\(\\sigma_1\\) to represent the standard deviation of slope deviations\n\\(\\rho_{01}\\) to represent the correlation between intercept deviations and slope deviations\n\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\] For a visual intuition about this, see Figure 8, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).\n\n\n\n\n\nFigure 8: Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values"
  },
  {
    "objectID": "02_lmm.html#extracting-model-parameters",
    "href": "02_lmm.html#extracting-model-parameters",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Extracting model parameters",
    "text": "Extracting model parameters\nAlongside summary(), there are some useful functions in R that allow us to extract the parameters estimated by the model:\n\nfixed effects\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\nfixef(smod3)\n\n       (Intercept)              motiv       fundingstate motiv:fundingstate \n         40.314271           2.629403         -17.253134           2.848511 \n\n\n\n\nrandom effect variances\nThe random effect variances (sometimes referred to as the “variance components”) represent the estimated spread with which clusters vary around the fixed effects.\n\nVarCorr(smod3)\n\n Groups   Name        Std.Dev. Corr  \n schoolid (Intercept) 10.2531        \n          motiv        1.6108  -0.481\n Residual             11.7911"
  },
  {
    "objectID": "02_lmm.html#making-model-predictions",
    "href": "02_lmm.html#making-model-predictions",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Making model predictions",
    "text": "Making model predictions\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models\n\nrandom effects\nOften referred to as the “random effects”, the deviations for each cluster from the fixed effects can be obtained using ranef().\nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that “Anderson High School” has an estimated intercept that is 9.39 higher than average, and an estimate slope of motivation that is 0.61 lower than average.\n\nranef(smod3)\n\n$schoolid\n                                       (Intercept)       motiv\nAnderson High School                    9.386006   -0.60628851\nArdnamurchan High School               -3.413536    0.15035243\nBalwearie High School                  -13.599480   1.12438549\nBeeslack Community High School          9.658259   -0.14963519\n...                                     ...         ...\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.\n\ndotplot.ranef.mer(ranef(smod3))\n\n$schoolid\n\n\n\n\n\n\n\n\n\n\n\ncluster coefficients\nRather than looking at deviations from fixed effects, we can calculate the intercept and slopes for each cluster.\nWe can get these out using coef()\n\ncoef(smod3)\n\n$schoolid\n                               (Intercept) motiv       fundingstate  motiv:fundingstate\nAnderson High School           49.70028    2.02311498  -17.25313     2.848511\nArdnamurchan High School       36.90073    2.77975592  -17.25313     2.848511\nBalwearie High School          26.71479    3.75378898  -17.25313     2.848511\nBeeslack Community High School 49.97253    2.47976830  -17.25313     2.848511\n...                            ...         ...         ...           ...\n\n\nfixef() + ranef() = coef()\n\n\nFor example, if we are estimating that “Anderson High School” has an intercept that is 9.39 higher than average, and the average (our fixed effect intercept) is 40.31, then we know that this has an intercept of 40.31 + 9.39 = 49.70.\nHowever, note that we also have slopes for things that aren’t present in the output of ranef() - we have the slopes for the fixed predictors for each cluster too, but because these are fixed they are simply the same number of each cluster.\nBecause “Anderson High School” is a state-funded school, the intercept is not just “9.39 above our fixed effect intercept” - it is estimated to be 17.25 lower than that because it is state-funded. So our estimated intercept for this school is 40.31 + 9.39 - 17.25 = 32.45.\nIn fact, we can essentially write out a regression equation for each individual cluster. For instance, “Anderson High School” is a state-funded school, so the expected grade for a child \\(i\\) from this school would be: \\[\n\\begin{align}\n\\widehat{grade_i} &= 49.70 + (2.02 \\cdot motiv_i) - (17.25 \\cdot 1) + (2.85 \\cdot motiv_i \\cdot 1) \\\\\n&= 32.45 + (4.87 \\cdot motiv_i) \\\\\n\\end{align}\n\\] And for Beeslack Community High School, which is a private funded school, the expected grade would be: \\[\n\\begin{align}\n\\widehat{grade_i} &= 49.97 + (2.48 \\cdot motiv_i) - (17.25 \\cdot 0) + (2.85 \\cdot motiv_i \\cdot 0) \\\\\n&= 49.97 + (2.48 \\cdot motiv_i) \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "02_lmm.html#visualising-models",
    "href": "02_lmm.html#visualising-models",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Visualising models",
    "text": "Visualising models\nIn the vast majority of analyses using multilevel models, the bit that people are most interested in is the fixed effects - these represent the estimated effects averaged across the clusters in our sample.\nIt’s tempting, then, to think that we could take the model-predicted values for each observation (predict()/fitted()/augment() etc.) and average them up to get our estimated effects. If every cluster had the same amount of data at every level of the predictor then this would work but we would have more issues in estimating the uncertainty.\nInstead, it is common to use helpful packages that can provide us with a small dataframe of estimated effects and uncertainty, that we can then plot. One such package is the effects package, which has the effect() function, that can be used as so:\n\nlibrary(effects)\neffect(term = \"motiv*funding\", mod = smod3) |&gt;\n  as.data.frame()\n\n   motiv funding      fit       se    lower    upper\n1    0.4 private 41.36603 4.376160 32.77731 49.95475\n2    3.0 private 48.20248 3.072408 42.17253 54.23244\n3    5.0 private 53.46129 2.975888 47.62077 59.30181\n4    7.0 private 58.72010 3.776268 51.30874 66.13146\n5    9.0 private 63.97890 5.064835 54.03858 73.91922\n6    0.4   state 25.25230 3.187004 18.99744 31.50717\n7    3.0   state 39.49488 2.314485 34.95244 44.03732\n8    5.0   state 50.45071 2.257980 46.01916 54.88225\n9    7.0   state 61.40654 2.797545 55.91603 66.89704\n10   9.0   state 72.36236 3.679698 65.14053 79.58419\n\n\nWe can then use this to plot:\n\neffect(term = \"motiv*funding\", mod = smod3) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x = motiv, col = funding, fill = funding)) +\n  geom_line(aes(y = fit)) + \n  geom_ribbon(aes(y = fit, ymin = lower, ymax = upper), alpha = .3)\n\n\n\n\n\n\n\n\nHowever, as with plotting all estimated effects, it is very useful to also display the underlying data, to make clear the variability we would actually expect. The plot above shows us what we would expect for children from the schools on average. It doesn’t clearly depict how schools and children vary, and a reader could easily take away the idea that we expect most children to fall within those bands.\nOne easy way is to adjust plot is to simply show the raw data behind our estimated effects, which requires giving the ggplot() code data from two different places:\n\nefplot &lt;- effect(term = \"motiv*funding\", mod = smod3) |&gt;\n  as.data.frame()\n\nggplot(data = schoolmot, \n       aes(x = motiv, col = funding, fill = funding)) +\n  geom_point(aes(y = grade), alpha = .3) + \n  geom_line(data = efplot, aes(y = fit)) + \n  geom_ribbon(data = efplot, \n              aes(y = fit, ymin = lower, ymax = upper), alpha = .3)"
  },
  {
    "objectID": "02_lmm.html#convergence-warnings-singular-fits",
    "href": "02_lmm.html#convergence-warnings-singular-fits",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "convergence warnings & singular fits",
    "text": "convergence warnings & singular fits\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different ‘optimisers’.\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nTechnical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn’t possible.\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:\n\nA typical convergence warning:\n\n\nwarning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)\n\nA singular fit:\n\n\nboundary (singular) fit: see ?isSingular\n\n\n\nDo not trust the results of a model that does not converge\n\nThere are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them), but for the time being, if lmer() gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))"
  },
  {
    "objectID": "02_lmm.html#footnotes",
    "href": "02_lmm.html#footnotes",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome books use “cluster \\(j\\) &gt;&gt; observation \\(i\\)”, others use “cluster \\(i\\) &gt;&gt; observation \\(j\\)”. We use the latter here↩︎\nthis exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added↩︎\nremember, variance = standard deviation squared↩︎\nit’s a bit like n-1 being in the denominator of the formula for standard deviation↩︎"
  },
  {
    "objectID": "03_inference.html",
    "href": "03_inference.html",
    "title": "3: Inference for MLM",
    "section": "",
    "text": "This reading:\nConducting inference (i.e. getting confidence intervals or p-values, model comparisons) for MLMs can be tricky partly because there are a variety of different methods that have been developed.\nThis reading briefly explains why getting p-values from lmer() is not as easy as it was for lm(), before giving an outline of some of the main approaches people tend to take. Don’t feel like you have to remember all of these, just be aware that they exist, and refer back to this page whenever you need to."
  },
  {
    "objectID": "03_inference.html#summary",
    "href": "03_inference.html#summary",
    "title": "3: Inference for MLM",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood based\nparametric bootstrap\n\n\n\n\ntests/CIs of individual parameters\nTests of individual parameters can be done by refitting with lmerTest::lmer(...) for the Satterthwaite (S) method, or using parameters::model_parameters(model, ci_method=\"kr\") for Kenward Rogers (KR).\nProfile likelihood CIs for individual parameters can be obtained via confint(m, method=\"profile\"), but this can be computationally demanding.\nParametric Bootstrapped CIs for individual parameters can be obtained via confint(m, method=\"boot\")\n\n\nmodel comparisons(different fixed effects, same random effects)\nComparisons of models that differ only in their fixed effects can be done via \\(F\\) tests in the pbkrtest package:SATmodcomp(m2, m1) for S and KRmodcomp(m2, m1) for KR.\nComparisons of models that differ only in their fixed effects can be done via LRT using anova(m1, m2)\nComparisons of models that differ only in their fixed effects can be done via a bootstrapped LRT using PBmodcomp(m2, m1) from the pbkrtest package.\n\n\n\nFor KR, models must be fitted with REML=TRUE (a good option for small samples). For S, models can be fitted with either.\nFor likelihood based methods for fixed effects, models must be fitted with REML=FALSE.Likelihood based methods are asymptotic (i.e. hold when \\(n \\rightarrow \\infty\\)). Best avoided with smaller sample sizes (i.e. a small number of groups)\nTime consuming, but considered best available method (can be problematic with unstable models)\n\n\n\n\n\n\n\n\n\noptional: testing random effects?\n\n\n\n\n\nTests of random effects are difficult because the null hypothesis (the random effect variance is zero) lies on a boundary (you can’t have a negative variance). Comparisons of models that differ only in their random effects can be done by comparing ratio of likelihoods when fitted with REML=TRUE (this has to be done manually), but these tests should be treated with caution.\nWe can obtain confidence intervals for our random effect variances using both the profile likelihood and the parametric boostrap methods discussed above.\nAs random effects are typically part of the experimental design, there is often little need to test their significance. In most cases, the maximal random effect structure can be conceptualised without reference to the data or any tests, and the inclusion/exclusion of specific random effects is more a matter of what simplifications are required for the model to converge. Inclusion/exclusion of parameters based on significance testing is rarely, if ever a sensible approach."
  },
  {
    "objectID": "03_inference.html#footnotes",
    "href": "03_inference.html#footnotes",
    "title": "3: Inference for MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(n\\) observations minus \\(k\\) parameters (slope of x) minus 1 intercept↩︎"
  },
  {
    "objectID": "04_log.html",
    "href": "04_log.html",
    "title": "4: Example: Logistic MLM",
    "section": "",
    "text": "This reading:\nAn example: Logistic multilevel models (lm() is to glm() as lmer() is to glmer())"
  },
  {
    "objectID": "04_log.html#example",
    "href": "04_log.html#example",
    "title": "4: Example: Logistic MLM",
    "section": "Example",
    "text": "Example\n\nData: msmr_monkeystatus.csv\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a “dominant” or “subordinate” status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either “easy” or “difficult”, and the researchers recorded whether or not the monkey solved each problem.\nWe’re interested in how the social status of monkeys is associated with the ability to solve problems.\nThe data is available at https://uoepsy.github.io/data/msmr_monkeystatus.csv.\n\n\ngetting to know my monkeys\nWe know from the study background that we have a series group of monkeys who have each attempted to solve some problems. If we look at our data, we can see that it is already in long format, in that each row represents the lowest unit of observation (a single problem attempted). We also have the variable monkeyID which indicates what monkey each problem has been attempted by. We can see the status of each monkey, and the difficulty of each task, along with whether it was solved:\n\nlibrary(tidyverse)\nlibrary(lme4)\nmstat &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\nhead(mstat)\n\n# A tibble: 6 × 4\n  status      difficulty monkeyID solved\n  &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;\n1 subordinate easy       Seunghoo      1\n2 subordinate easy       Seunghoo      0\n3 subordinate difficult  Seunghoo      0\n4 subordinate easy       Seunghoo      1\n5 subordinate difficult  Seunghoo      0\n6 subordinate easy       Seunghoo      1\n\n\nWe can do some quick exploring to see how many monkeys we have (50), and how many problems each one attempted (min = 3, max = 11:\n\nmstat |&gt; \n  count(monkeyID) |&gt; # count the monkeys!  \n  summary()\n\n   monkeyID               n        \n Length:50          Min.   : 3.00  \n Class :character   1st Qu.: 6.25  \n Mode  :character   Median : 8.00  \n                    Mean   : 7.94  \n                    3rd Qu.:10.00  \n                    Max.   :11.00  \n\n\nLet’s also see how many monkeys of different statuses we have in our sample:\n\nmstat |&gt; \n  group_by(status) |&gt; # group statuses\n  summarise(\n    # count the distinct monkeys\n    nmonkey = n_distinct(monkeyID)\n  ) \n\n# A tibble: 3 × 2\n  status      nmonkey\n  &lt;chr&gt;         &lt;int&gt;\n1 adolescent       16\n2 dominant         23\n3 subordinate      11\n\n\nIt’s often worth plotting as much as you can to get to a sense of what we’re working with. Here are the counts of easy/difficult problems that each monkey attempted. We can see that Richard only did difficult problems, and Nadheera only did easy ones, but most of the monkeys did both types of problem.\n\n# which monkeys did what type of problems? \nmstat |&gt; count(status, monkeyID, difficulty) |&gt;\n  ggplot(aes(x=difficulty,y=n, fill=status))+\n  geom_col()+\n  facet_wrap(~monkeyID) +\n  scale_x_discrete(labels=abbreviate) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhen working with binary outcomes, it’s often useful to calculate and plot proportions. In this case, the proportions of problems solved for each status of monkey. At first glance it looks like “subordinate” monkeys solve more problems, and adolescents solve fewer (makes sense - they’re still learning!).\n\n# a quick look at proportions of problems solved:\nggplot(mstat, aes(x=difficulty, y=solved,\n                       col=status))+\n  stat_summary(geom=\"pointrange\",size=1)+\n  facet_wrap(~status)\n\n\n\n\n\n\n\n\n\n\nmodels of monkeys\nNow we come to fitting our model.\nRecall that we are interested in how the ability to solve problems differs between monkeys of different statuses. It’s very likely that difficulty of a problem is going to influence that it is solved, so we’ll control for difficulty.\nglmer(solved ~ difficulty + status + \n      ...\n      data = mstat, family = binomial)\nWe know that we have multiple datapoints for each monkey, and it also makes sense that there will be monkey-to-monkey variability in the ability to solve problems (e.g. Brianna may be more likely to solve problems than Jonathan).\nglmer(solved ~ difficulty + status + \n      (1 + ... | monkeyID),\n      data = mstat, family = binomial)\nFinally, it also makes sense that effects of problem-difficulty might vary by monkey (e.g., if Brianna is just really good at solving problems, problem-difficulty might not make much difference. Whereas if Jonathan is struggling with the easy problems, he’s likely to really really struggle with the difficult ones!).\nFirst, we’ll relevel the difficulty variable so that the reference level is “easy”:\n\nmstat &lt;- mstat |&gt; mutate(\n  difficulty = fct_relevel(factor(difficulty), \"easy\")\n)\n\nand fit our model:\n\nmmod &lt;- glmer(solved ~ difficulty + status + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nsummary(mmod)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n   Data: mstat\n\n     AIC      BIC   logLik deviance df.resid \n   503.7    531.6   -244.8    489.7      390 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9358 -0.6325 -0.3975  0.6748  2.5160 \n\nRandom effects:\n Groups   Name                Variance Std.Dev. Corr \n monkeyID (Intercept)         1.551    1.246         \n          difficultydifficult 1.371    1.171    -0.44\nNumber of obs: 397, groups:  monkeyID, 50\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)          -0.3945     0.3867  -1.020  0.30767   \ndifficultydifficult  -0.8586     0.3053  -2.812  0.00492 **\nstatusdominant        0.6682     0.4714   1.417  0.15637   \nstatussubordinate     1.4596     0.5692   2.564  0.01033 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) dffclt sttsdm\ndffcltydffc -0.333              \nstatusdmnnt -0.721 -0.031       \nstatssbrdnt -0.594 -0.033  0.497\n\n\n\n\ntest and visualisations of monkey status\nTo examine if monkey status has an effect, we can compare with the model without status:\n\n\nCode\nmmod0 &lt;- glmer(solved ~ difficulty + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nanova(mmod0, mmod)\n\n\nData: mstat\nModels:\nmmod0: solved ~ difficulty + (1 + difficulty | monkeyID)\nmmod: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmmod0    5 506.13 526.05 -248.07   496.13                       \nmmod     7 503.70 531.58 -244.85   489.70 6.4367  2    0.04002 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd we can see that the status of monkeys is associated with differences in the probability of successful problem solving (\\(\\chi^2(2)\\) = 6.44, p &lt; 0.05).\nAnd if we want to visualise the relevant effect, we can (as we did with glm()) plot on the predicted probability scale, which is much easier to interpret:\n\n\nCode\nlibrary(effects)\neffect(term=c(\"status\",\"difficulty\"), mod=mmod) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=difficulty, y=fit))+\n  geom_pointrange(aes(ymin=lower,ymax=upper, col=status),\n                  size=1, lwd=1,\n                  position=position_dodge(width=.3)) +\n  labs(x = \"problem difficulty\", y = \"predicted probability\")\n\n\n\n\n\n\n\n\n\n\n\ninterpretation\nAnd just with the single level logistic models, our fixed effects can be converted to odds ratios (OR), by exponentiation:\n\ncbind(\n  fixef(mmod), # the fixed effects\n  confint(mmod, method=\"Wald\", parm=\"beta_\") # Wald CIs for fixed effects\n) |&gt;\n  exp()\n\n                                  2.5 %     97.5 %\n(Intercept)         0.6740221 0.3158658  1.4382872\ndifficultydifficult 0.4237565 0.2329242  0.7709359\nstatusdominant      1.9506650 0.7743099  4.9141746\nstatussubordinate   4.3042614 1.4106434 13.1334867\n\n\n\n\n\n\n\n\n  \n    \n      term\n      est\n      OR\n      OR interpretation\n    \n  \n  \n    (Intercept)\n-0.39\n0.67\nestimated odds of an adolescent monkey solving an easy problem\n    difficultydifficult\n-0.86\n0.42\nodds of successful problem solving are more than halved (0.42 times the odds) when the average monkey moves from an easy to a difficult problem\n    statusdominant\n0.67\n1.95\nodds of success would be almost doubled (1.95 times the odds) if the average monkey were to change from adolescent to dominant status (NB this is non-significant)\n    statussubordinate\n1.46\n4.30\nodds of success would quadruple (4.3 times the odds) if the average monkey were to change from adolescent to subordinate status\n  \n  \n  \n\n\n\n\n\n\n\n\nSide note\nContrast this with what we would get from a linear multilevel model. If we were instead modelling a “problem score” with lmer(), rather than “solved yes/no” with glmer(), our coefficients would be interpreted as the estimated difference in scores between adolescent and subordinate monkeys.\nNote that estimating differences between groups is not quite the same idea as estimating the effect “if a particular (the average) monkey changed from adolescent to subordinate”. In the linear world, these two things are the same, but our odds ratios give us only the latter."
  },
  {
    "objectID": "04_log.html#footnotes",
    "href": "04_log.html#footnotes",
    "title": "4: Example: Logistic MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that binary outcomes are just a special case of the binomial↩︎"
  },
  {
    "objectID": "05_long.html",
    "href": "05_long.html",
    "title": "5: Example: Longitudinal MLM",
    "section": "",
    "text": "This reading:\nAn example of “change over time” - fitting multilevel models to longitudinal data.\n- The application of multilevel models to longitudinal data is very much just that - we are taking the same sort of models we have already learned about and simply applying them to a different context in which “time” is a predictor."
  },
  {
    "objectID": "05_long.html#example",
    "href": "05_long.html#example",
    "title": "5: Example: Longitudinal MLM",
    "section": "Example",
    "text": "Example\n\nData: lmm_mindfuldecline.csv\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.\nThe data are available at: https://uoepsy.github.io/data/lmm_mindfuldecline.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    sitename\nName of the site where the study was conducted\n    ppt\nParticipant Identifier\n    condition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n    visit\nStudy Visit Number (1 - 10)\n    age\nAge (in years) at study visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n  \n  \n  \n\n\n\n\n\n\nexploring the data\n\nlibrary(tidyverse)\nlibrary(lme4)\nmmd &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_mindfuldecline.csv\")\nhead(mmd)\n\n# A tibble: 6 × 7\n  sitename ppt   condition visit   age   ACE imp  \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 Sncbk    PPT_1 control       1    60  84.5 unimp\n2 Sncbk    PPT_1 control       2    62  85.6 imp  \n3 Sncbk    PPT_1 control       3    64  84.5 imp  \n4 Sncbk    PPT_1 control       4    66  83.1 imp  \n5 Sncbk    PPT_1 control       5    68  82.3 imp  \n6 Sncbk    PPT_1 control       6    70  83.3 imp  \n\n\nHow many participants in each condition? We know from the description there should be 10 in each, but lets check!\n\nmmd |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    n_ppt = n_distinct(ppt)\n  )\n\n# A tibble: 2 × 2\n  condition   n_ppt\n  &lt;chr&gt;       &lt;int&gt;\n1 control        10\n2 mindfulness    10\n\n\nHow many observations does each participant have? With only 20 participants, we could go straight to plotting as a way of getting lots of information all at once. From the plot below, we can see that on the whole participants’ cognitive scores tend to decrease. Most participants have data at every time point, but 4 or 5 people are missing a few. The control participants look (to me) like they have a slightly steeper decline than the mindfulness group:\n\nggplot(mmd, aes(x = age, y = ACE, col = condition)) + \n  geom_point() +\n  geom_line(aes(group=ppt), alpha=.4)+\n  facet_wrap(~ppt)\n\n\n\n\n\n\n\n\n\n\nmodelling change over time\nInitially, we’ll just model how cognition changes over time across our entire sample (i.e. ignoring the condition the participants are in). Note that both the variables study_visit and age represent exactly the same information (time), so we have a choice of which one to use.\n\n\n\n\n\n\nWhy the age variable (currently) causes problems\n\n\n\n\n\nAs it is, the age variable we have starts at 60 and goes up to 78 or so.\nIf we try and use this in a model, we get an error!\n\nmod1 &lt;- lmer(ACE ~ 1 + age + \n               (1 + age | ppt), \n             data = mmd)\n\nModel failed to converge with max|grad| = 0.366837 (tol = 0.002, component 1)\nThis is because of the fact that intercepts and slopes are inherently dependent upon one another. Remember that the intercept is “when all predictors are zero”. So in this case it is the estimate cognition of new-born babies. But all our data comes from people who are 65+ years old!\nThis means that trying to fit (1 + age | ppt) will try to estimate the variability in people’s change in cognition over time, and the variability in cognition at age zero. As we can see in Figure 1, because the intercept is so far away from the data, the angle of each persons’ slope has a huge influence over where their intercept is. The more upwards a persons’ slope is, the lower down their intercept is.\n\n\n\n\n\nFigure 1: lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the ‘intercept’ is estimated back at age 0\n\n\n\n\nThis results in issues for estimating our model, because the intercepts and slopes are perfectly correlated! The estimation process has hit a boundary (a perfect correlation):\n\nVarCorr(mod1)\n\n Groups   Name        Std.Dev. Corr  \n ppt      (Intercept) 7.51567        \n          age         0.12696  -0.999\n Residual             0.51536        \n\n\nSo what we can do is either center age on 60 (so that the random intercept is the estimated variability in cognition at aged 60, i.e. the start of the study), or use the study_visit variable.\nEither will do, we just need to remember the units they are measured in!\n\n\n\nLet’s center age on 60:\n\nmmd$ageC &lt;- mmd$age-60\n\nAnd fit our model:\n\nmod1 &lt;- lmer(ACE ~ 1 + ageC + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom our fixed effects, we can see that scores on the ACE tend to decrease by about 0.18 for every 1 year older people get (as a very rough rule of thumb, \\(t\\) statistics that are \\(&gt;|2\\text{-ish}|\\) are probably going to be significant when assessed properly).\n\nsummary(mod1)\n\n...\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 85.22558    0.10198 835.735\nageC        -0.17938    0.03666  -4.893\nWe’re now ready to add in group differences in their trajectories of cognition:\n\nmod2 &lt;- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom this model, we can see that for the control group the estimated score on the ACE at age 60 is 85 (that’s the (Intercept)). For these participants, scores are estimated to decrease by -0.27 points every year (that’s the slope of ageC). For the participants in the mindfulness condition, they do not score significantly differently from the control group at age 60 (the condition [mindfulness] coefficient). For the mindfulness group, there is a reduction in the decline of cognition compared to the control group, such that this group decline 0.17 less than the control group every year.\n(note, there are always lots of ways to frame interactions. A “reduction in decline” feels most appropriate to me here)\nGiven that we have a fairly small number of clusters here (20 participants), Kenward Rogers is a good method of inference as it allows us to use REML (meaning unbiased estimates of the random effect variances) and it includes a small sample adjustment to our standard errors.\n\nlibrary(parameters)\nmodel_parameters(mod2, ci_method=\"kr\", ci_random=FALSE)\n\n# Fixed Effects\n\nParameter                      | Coefficient |   SE |         95% CI |      t |    df |      p\n----------------------------------------------------------------------------------------------\n(Intercept)                    |       85.20 | 0.15 | [84.89, 85.52] | 568.00 | 17.75 | &lt; .001\nageC                           |       -0.27 | 0.04 | [-0.36, -0.17] |  -5.93 | 17.95 | &lt; .001\ncondition [mindfulness]        |        0.05 | 0.21 | [-0.39,  0.49] |   0.23 | 17.49 | 0.821 \nageC × condition [mindfulness] |        0.17 | 0.06 | [ 0.04,  0.31] |   2.73 | 17.99 | 0.014 \n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: ppt)       |        0.35\nSD (ageC: ppt)            |        0.14\nCor (Intercept~ageC: ppt) |        0.26\nSD (Residual)             |        0.49\n\n\nFrom those parameters and our interpretation above, we are able to start putting a picture together - two groups that start at the same point, one goes less steeply down over time than the other.\nAnd that’s exactly what we see when we visualise those fixed effects:\n\n\nCode\nlibrary(effects)\neffect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=ageC+60,y=fit,\n             ymin=lower,ymax=upper,\n             col=condition, fill = condition))+\n  geom_line(lwd=1)+\n  geom_ribbon(alpha=.2, col=NA) +\n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nSometimes it is more helpful for a reader if we add in the actual observed trajectories to these plots. To do so, we need to combine two data sources - the fixed effects estimation from effect(), and the data itself:\n\n\nCode\nploteff &lt;- effect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame()\n\nmmd |&gt;\n  ggplot(aes(x=ageC+60,col=condition,fill=condition))+\n  geom_line(aes(y=ACE,group=ppt), alpha=.4) +\n  geom_line(data = ploteff, aes(y=fit), lwd=1)+\n  geom_ribbon(data = ploteff, aes(y=fit,ymin=lower,ymax=upper),\n              alpha=.2, col=NA) + \n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nThis plot gives us more a lot more context. To a lay reader, our initial plot potentially could be interpreted as if we would expect every person’s cognitive trajectories to fall in the blue and red bands. But those bands are representing the uncertainty in the fixed effects - i.e. the uncertainty in the average persons’ trajectory. When we add in the observed trajectories, we see the variability in people’s trajectories (one person even goes up over time!).\nOur model represents this variability in the random effects part. While the estimated average slope is -0.27 for the control group (and -0.27+0.17=-0.09 for the mindfulness group), people are estimated to vary in their slopes with a standard deviation of 0.14 (remember we can extract this info using VarCorr(), or just look in the output of summary(model)).\n\nVarCorr(mod2)\n\n Groups   Name        Std.Dev. Corr \n ppt      (Intercept) 0.34615       \n          ageC        0.13866  0.260\n Residual             0.49450       \n\n\n\n\n\n\n\n\n\nFigure 2: Two normal distributions with mean of -0.27 (purple) and -.09 (green) and a standard deviation of 0.14\n\n\n\n\nIf you think about what this means - it means that some participants we would expect to actually increase in their slopes. If we have a normal distribution with a mean of -0.3 or -0.09 and a standard distribution of 0.14, then we would expect some values to to positive (see e.g., Figure 2)."
  },
  {
    "objectID": "05_long.html#footnotes",
    "href": "05_long.html#footnotes",
    "title": "5: Example: Longitudinal MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nassuming that it is people we are studying!↩︎"
  },
  {
    "objectID": "06_poly.html",
    "href": "06_poly.html",
    "title": "6: Polynomial Growth in MLM",
    "section": "",
    "text": "This reading:\n\nThe basics of modelling non-linear change via polynomial terms.\nAn example with MLM\n\nFor additional reading, Winter & Wieling, 2016 is pretty good (mainly focus on sections 1-3)\nWe have already seen in the last couple of weeks that we can use MLM to study something ‘over the course of X’. This might be “over the course of adolescence” (i.e. y ~ age), or “over the course of an experiment” (y ~ trial_number). The term “longitudinal” is commonly used to refer to any data in which repeated measurements are taken over a continuous domain. This opened up the potential for observations to be unevenly spaced, or missing at certain points.\nIt also, as will be the focus of this week, opens the door to thinking about how many effects of interest may display patterns that are non-linear. There are lots of techniques to try and summarise non-linear trajectories, and here we are going to focus on the method of including higher-order polynomials as predcitors."
  },
  {
    "objectID": "06_poly.html#raw-polynomials",
    "href": "06_poly.html#raw-polynomials",
    "title": "6: Polynomial Growth in MLM",
    "section": "Raw Polynomials",
    "text": "Raw Polynomials\nThere are two types of polynomial we can construct. “Raw” (or “Natural”) polynomials are the straightforward ones that you would expect (example in the table below), where the original value of \\(x\\) is squared/cubed.\n\n\n\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n1\n1\n1\n\n\n2\n4\n8\n\n\n3\n9\n27\n\n\n4\n16\n64\n\n\n5\n25\n125\n\n\n…\n…\n…\n\n\n\nWe can quickly get these in R using the poly() function. As we want to create “raw” polynomials, we need to make sure to specify raw = TRUE or we get something else (we’ll talk about what they are in a second!).\n\npoly(1:10, degree = 3, raw=TRUE)\n\n       1   2    3\n [1,]  1   1    1\n [2,]  2   4    8\n [3,]  3   9   27\n [4,]  4  16   64\n [5,]  5  25  125\n [6,]  6  36  216\n [7,]  7  49  343\n [8,]  8  64  512\n [9,]  9  81  729\n[10,] 10 100 1000\nattr(,\"degree\")\n[1] 1 2 3\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nLet’s now use these with our example data we had been plotting above.\nFirst lets add new variables to the dataset, which are the polynomials of our \\(x\\) variable:\n\nsyndat &lt;- \n  syndat |&gt; \n    mutate(\n      # poly1 is the first column\n      poly1 = poly(age, degree = 3, raw = TRUE)[,1],\n      # poly2 is the second\n      poly2 = poly(age, degree = 3, raw = TRUE)[,2],\n      # poly3 is the third\n      poly3 = poly(age, degree = 3, raw = TRUE)[,3]\n    )\n\nAnd now lets use them in our model as predictors:\n\ncubicmod &lt;- lm(syndens ~ poly1 + poly2 + poly3, data = syndat)\n\n\n\n\n\n\n\nother ways to get polynomials into the model\n\n\n\n\n\nAs we’re working with raw polynomials, we could just do:\n\nsyndat |&gt; \n  mutate(\n    poly1 = age,\n    poly2 = age^2,\n    poly3 = age^3\n  )\n\nOr we could even just specify the calculations for each term inside the call to lm():\n\nlm(syndens ~ age + I(age^2) + I(age^3), data = syndat)\n\nOr even use the poly() function:\n\nlm(syndens ~ poly(age, degree=3, raw=TRUE), data = syndat)\n\n\n\n\n\n\n\n\n\n\nA handy function from Dan\n\n\n\n\n\nDan has a nice function that may be handy. It adds the polynomials to your dataset for you:\n\n# import Dan's code and make it available in our own R session\n# you must do this in every script you want to use this function\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\nsyndat &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_synapticdens.csv\")\nsyndat &lt;- code_poly(df = syndat, predictor = 'age', poly.order = 3, \n                    orthogonal = FALSE, draw.poly = FALSE)\nhead(syndat)\n\n# A tibble: 6 × 6\n    age syndens age.Index poly1 poly2 poly3\n  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.1   0.615         1   0.1  0.01 0.001\n2   0.2   0.908         2   0.2  0.04 0.008\n3   0.6   0.1           3   0.6  0.36 0.216\n4   0.7   1.81          4   0.7  0.49 0.343\n5   0.9   1.44          5   0.9  0.81 0.729\n6   1     0.615         6   1    1    1    \n\n\n\n\n\nJust to see it in action, let’s take a look at the predicted values from our model.\nTake for instance, the 9th row below. The predicted value of y (shown in the .fitted column) is:\n\\(\\hat y_9 = b_0 + b_1 \\cdot x_9 + b_2 \\cdot x^2_9 + b_3 \\cdot x^3_9\\)\n\\(\\hat y_9 = b_0 + b_1 \\cdot 2 + b_2 \\cdot 4 + b_3 \\cdot 8\\)\n\\(\\hat y_9 = -1.843 + 3.375 \\cdot 2 + -0.332 \\cdot 4 + 0.0097 \\cdot 8\\)\n\\(\\hat y_9 = 3.66\\).\n\nlibrary(broom)\naugment(cubicmod) \n\n# A tibble: 74 × 10\n   syndens poly1 poly2 poly3 .fitted  .resid   .hat .sigma   .cooksd .std.resid\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1   0.615   0.1  0.01 0.001 -1.51    2.12   0.170    1.62 0.104         1.42  \n 2   0.908   0.2  0.04 0.008 -1.18    2.09   0.155    1.63 0.0886        1.39  \n 3   0.1     0.6  0.36 0.216  0.0651  0.0349 0.109    1.65 0.0000155     0.0226\n 4   1.81    0.7  0.49 0.343  0.361   1.45   0.0992   1.64 0.0240        0.933 \n 5   1.44    0.9  0.81 0.729  0.933   0.510  0.0829   1.65 0.00239       0.326 \n 6   0.615   1    1    1      1.21   -0.596  0.0759   1.65 0.00294      -0.379 \n 7   0.615   1.6  2.56 4.10   2.75   -2.13   0.0465   1.63 0.0217       -1.33  \n 8   0.310   1.7  2.89 4.91   2.98   -2.67   0.0433   1.62 0.0316       -1.67  \n 9   1.21    2    4    8      3.66   -2.45   0.0361   1.62 0.0217       -1.52  \n10   2.19    2.1  4.41 9.26   3.87   -1.68   0.0343   1.64 0.00968      -1.04  \n# ℹ 64 more rows\n\n\nIf we plot the predictions with poly1 on the x-axis (poly1 is just the same as our age variable with a different name!), we can see that we are able to model a non-linear relationship between y and x (between synaptic density and age), via a combination of linear parameters!\n\nlibrary(broom)\naugment(cubicmod, interval=\"confidence\") |&gt;\n  ggplot(aes(x=poly1))+\n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n  geom_line(aes(y=.fitted),col=\"darkorange\") +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper),fill=\"darkorange\", alpha=.2)+\n  labs(x=\"age\") # our x-axis, \"poly1\", is just age!  \n\n\n\n\nFigure 6: a cubic model\n\n\n\n\nNow lets look at our coefficients:\n\nsummary(cubicmod)\n\n...\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.842656   0.704193  -2.617   0.0109 *  \npoly1        3.375159   0.345570   9.767 1.06e-14 ***\npoly2       -0.331747   0.044664  -7.428 2.06e-10 ***\npoly3        0.009685   0.001614   6.001 7.79e-08 ***\n---\nWith polynomials the interpretation is a little tricky because we have 3 coefficients that together explain the curvy line we see in Figure 6, and these coefficients are all dependent upon one another.\n\n(Intercept) = When all predictors are zero, i.e. the synaptic density at age 0.\n\npoly1 coefficient = The instantaneous change in \\(y\\) when \\(x=0\\).\npoly2 coefficient = Represents “rate of change of the rate of change” at \\(x=0\\). In other words, the curvature at \\(x=0\\).\n\npoly3 coefficient = Represents how the curvature is changing. It gets more abstract as the order of polynomials increase, so the easiest way to think about it is “the wiggliness”\n\nI’ve tried to represent what each term adds in Figure 7. The intercept is the purple point where age is zero. The poly1 coefficient is represented by the dashed blue line - the tangent of the curve at age zero. The poly2 coef, rperesented by the dashed green line, is how the angle of the blue line is changing at age zero. Finally, the poly3 coefficient tells us how much this curvature is changing (which gets us to our dashed orange line).\nNote that these interpretations are all dependent upon the others - e.g. the interpretation of poly2 refers to how the angle of poly1 is changing.\n\n\n\n\n\nFigure 7: the instantaneous rate of change at x=0 (blue), the rate of change in the rate of change (i.e. curvature, green), and ‘rate of change in rate of change in rate of change’ (i.e. wiggliness, orange)"
  },
  {
    "objectID": "06_poly.html#orthogonal-polynomials",
    "href": "06_poly.html#orthogonal-polynomials",
    "title": "6: Polynomial Growth in MLM",
    "section": "Orthogonal Polynomials",
    "text": "Orthogonal Polynomials\nThe poly() function also enables us to compute “orthogonal polynomials”. This is the same information as the raw polynomials, re-expressed into a set of uncorrelated variables.\nRaw polynomials are correlated, which is what results makes their interpretation depend upon one another. For example, if we take the numbers 1,2,3,4,5, then these numbers are by definition correlated with their squares 1,4,9,16,25. As we increase from 1 to 5, we necessarily increase from 1 to 25.\nHowever, if we first center the set of numbers, so that 1,2,3,4,5 becomes -2,1,0,1,2, then their squares are 4,1,0,1,4 - they’re not correlated!\nOrthogonal polynomials essentially do this centering and scaling for \\(k\\) degrees of polynomial terms.\n\n\nSo while raw polynomials look like this:\n\nmatplot(poly(1:10, 3, raw=T), type=\"l\", lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nOrthogonal polynomials look like this:\n\nmatplot(poly(1:10, 3, raw=F), type=\"l\", lwd=2)\n\n\n\n\n\n\n\n\n\n\nThis orthogonality allows us to essentially capture express the linear trend, curvature, and ‘wiggliness’ of the trajectory independently from one another, rather than relative to one another.\nUltimately, models using raw polynomials and using orthogonal polynomials are identical, but the coefficients we get out represent different things.\nLet’s overwrite our poly variables with orthogonal polynomials, by setting raw = FALSE:\n\nsyndat &lt;- \n  syndat |&gt; \n    mutate(\n      poly1 = poly(age,degree = 3, raw=FALSE)[,1],\n      poly2 = poly(age,degree = 3, raw=FALSE)[,2],\n      poly3 = poly(age,degree = 3, raw=FALSE)[,3],\n    )\n\nAnd fit our model:\n\nOcubicmod &lt;- lm(syndens ~poly1+poly2+poly3,syndat)\nsummary(Ocubicmod)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.5917     0.1902  34.649  &lt; 2e-16 ***\npoly1        12.9161     1.6365   7.892 2.88e-11 ***\npoly2       -14.3156     1.6365  -8.748 7.68e-13 ***\npoly3         9.8212     1.6365   6.001 7.79e-08 ***\n---\nThe interpretation of the estimates themselves are not really very tangible anymore, because the scaling of the orthogonal polynomials has lost a clear link back to “age”.\nAs the polynomial terms are centered on the mean of age, the intercept is the estimated synaptic density at the mean age (the purple dot in Figure 8). The poly1, poly2 and poly3 coefficient represent the independent overall linear trend, centered curvature, and “wiggliness” of the relationship between synaptic density and age (as shown in the blue, green and orange lines in Figure 8 respectively).\n\n\n\n\n\nFigure 8: the independent rate of change (blue), curvature (green) and wiggliness (orange) of the y~x relationship"
  },
  {
    "objectID": "06_poly.html#raw-vs-orthognal",
    "href": "06_poly.html#raw-vs-orthognal",
    "title": "6: Polynomial Growth in MLM",
    "section": "Raw vs Orthognal",
    "text": "Raw vs Orthognal\nThe two models we have seen, one with raw polynomials, and one with orthogonal polynomials, are identical.\nFor proof, compare the two:\n\nanova(\n  lm(syndens ~ poly(age, 3, raw = TRUE), data = syndat),\n  lm(syndens ~ poly(age, 3, raw = FALSE), data = syndat)\n)\n\nAnalysis of Variance Table\n\nModel 1: syndens ~ poly(age, 3, raw = TRUE)\nModel 2: syndens ~ poly(age, 3, raw = FALSE)\n  Res.Df    RSS Df   Sum of Sq F Pr(&gt;F)\n1     70 187.47                        \n2     70 187.47  0 -1.1369e-13         \n\n\nSo why would we choose one vs the other?\nThe main reason is if we are interested in evaluating things relative to baseline, in which case raw polynomials allow us to do just that. If we are instead interested in evaluating the trends across the timecourse, then we would want orthogonal polynomials.\nConsider two examples:\n\n\nExample 1\nA student advisor who meets with students as they start university wants to know about how happiness evolves over the course of students’ year at univeristy, and wonders if this is different between introverted and extraverted individuals.\nIn this case, they would want raw polynomials, so that they can assess whether the two personality types differ when they first come to University, and how this is likely to evolve from that point.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\nA company has four stores across the UK, and they want to know if the stores have differed in how variable their earnings have been across the year.\nIn this case, looking at change relative to month 1 isn’t very useful. It would, for instance, tell us that the linear trend for store2’s earnings is upwards, whereas the linear trend for store 1 is flat. This makes store2 look better.\nIf we used orthogonal polynomials instead, we would see that the linear trend for store 2 is actually negative compared to store1.\n\n\n\n\n\n\n\n\n\n\n\n\nRaw? Orthogonal?\nFor non-linear relationships, a good plot is usually the most important thing!"
  },
  {
    "objectID": "07_ranef.html",
    "href": "07_ranef.html",
    "title": "7: Random Effect Structures",
    "section": "",
    "text": "This reading:\n\nextending the multilevel model to encompass more complex random effect structures\nmodel building and common issues"
  },
  {
    "objectID": "07_ranef.html#example-1-two-levels",
    "href": "07_ranef.html#example-1-two-levels",
    "title": "7: Random Effect Structures",
    "section": "Example 1: Two levels",
    "text": "Example 1: Two levels\nBelow is an example of a study that has a similar structure to those that we’ve seen thus far, in which we have just two levels (observations that are grouped in some way).\n\n\nStudy Design\nSuppose, for instance, that we conducted an experiment on a sample of 20 staff members from the Psychology department to investigate effects of CBD consumption on stress over the course of the working week. Participants were randomly allocated to one of two conditions: the control group continued as normal, and the CBD group were given one CBD drink every day. Over the course of the working week (5 days) participants stress levels were measured using a self-report questionnaire.\nWe can see our data here:\n\npsychstress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek1.csv\")\nhead(psychstress)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Holly N     Self-report     1 -0.417\n2 Psych Holly N     Self-report     2  0.924\n3 Psych Holly N     Self-report     3  0.634\n4 Psych Holly N     Self-report     4  1.21 \n5 Psych Holly N     Self-report     5  0.506\n6 Psych Tom   Y     Self-report     1 -0.557\n\n\n\n\nPlot\n\n\nCode\n# take the dataset, and make the x axis of our plot the 'day' variable, \n# and the y axis the 'stress' variable: \n# color everything by the CBD groups\nggplot(psychstress, aes(x = day, y = stress, col=CBD)) + \n  geom_point() + # add points to the plot\n  geom_line() + # add lines to the plot\n  facet_wrap(~pid) # split it by participant\n\n\n\n\n\n\n\n\n\n\n\nModel\nWe might fit a model that looks something like this:\n\n\nCode\nlibrary(lme4)\n# re-center 'day' so the intercept is day 1\npsychstress$day &lt;- psychstress$day-1 \n\n# fit a model of stress over time: stress~day\n# estimate differences between the groups in their stress change: day*CBD\n# people vary in their overall stress levels: 1|pid\n# people vary in their how stress changes over the week: day|pid\nm2level &lt;- lmer(stress ~ 1 + day * CBD + \n                  (1 + day | pid), data = psychstress)\n\n\nNote that there is a line in the model summary output just below the random effects that shows us the information about the groups, telling us that we have 100 observations that are grouped into 20 different participants’.\n\nsummary(m2level)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + day * CBD + (1 + day | pid)\n   Data: psychstress\n\nREML criterion at convergence: 127.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.17535 -0.65204 -0.02667  0.64622  1.81574 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n pid      (Intercept) 0.199441 0.44659      \n          day         0.004328 0.06579  0.02\n Residual             0.112462 0.33535      \nNumber of obs: 100, groups:  pid, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.13178    0.14329   0.920\nday          0.07567    0.03461   2.186\nCBDY        -0.08516    0.24221  -0.352\nday:CBDY    -0.19128    0.05851  -3.270\n\nCorrelation of Fixed Effects:\n         (Intr) day    CBDY  \nday      -0.339              \nCBDY     -0.592  0.201       \nday:CBDY  0.201 -0.592 -0.339"
  },
  {
    "objectID": "07_ranef.html#example-2-three-level-nested",
    "href": "07_ranef.html#example-2-three-level-nested",
    "title": "7: Random Effect Structures",
    "section": "Example 2: Three level Nested",
    "text": "Example 2: Three level Nested\nLet’s suppose that instead of simply sampling 20 staff members from the Psychology department, we instead went out and sampled lots of people from different departments across the University. The dataset below contains not just our 20 Psychology staff members, but also data from 220 other people from departments such as History, Philosophy, Art, etc..\n\nneststress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_nested.csv\")\nhead(neststress)\n\n# A tibble: 6 × 6\n  dept  pid      CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 CMVM  Ryan     Y     Self-report     1  0.933\n2 CMVM  Ryan     Y     Self-report     2  0.997\n3 CMVM  Ryan     Y     Self-report     3  0.408\n4 CMVM  Ryan     Y     Self-report     4  0.581\n5 CMVM  Ryan     Y     Self-report     5  0.442\n6 CMVM  Nicholas Y     Self-report     1  0.138\n\n\nIn this case, we have observations that are grouped by participants, and those participants can be grouped into the department in which they work. Three levels of nesting!\nYou can see in the Figure 6 below that there is variation between departments (i.e. people working in Art are a bit more relaxed, Political Science and CMVM is stressful, etc), and then within each of those, there is variation between participants (i.e. some people working in Art are more stressed than other people in Art).\n\n\nCode\nggplot(neststress, aes(x=day, y=stress,col=CBD))+\n  # plot points\n  geom_point()+\n  # split by departments\n  facet_wrap(~dept)+\n  # make a line for each participant\n  geom_line(aes(group=pid),alpha=.3)+ \n  # plot the mean and SE for each day.\n  stat_summary(geom=\"pointrange\",col=\"black\")\n\n\n\n\n\nFigure 6: A longitudinal study in which participants are nested within department\n\n\n\n\nTo account for these multiple sources of variation, we can fit a model that says both ( ... | dept) (“things vary by department”) and ( ... | dept:pid) (“things vary by participants within departments”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nneststress$day &lt;- neststress$day-1\n\nmnest &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nNote that we can have different random slopes for departments vs those for participants. Our model above includes all random slopes that are feasible given the study design.\n\n\n\n\n\n\nexplanations of each random slope\n\n\n\n\n\n\nparticipants can vary in their baseline stress levels.\n\n(1 | dept:pid)\n\nparticipants can vary in how stress changes over the week. e.g., some participants might get more stressed over the week, some might get less stressed\n\n(days | dept:pid)\n\n\nparticipants cannot vary in how CBD changes their stress level. because each participant is either CBD or control, “the effect of CBD on stress” doesn’t exist for a single participant (and so can’t very between participants)\n\n(CBD | dept:pid)\n\n\nparticipants cannot vary in how CBD affects their changes in stress over the week. For the same reason as above.\n\n( day*CBD | dept:pid)\n\ndepartments can vary in their baseline stress levels.\n\n(1 | dept)\n\n\ndepartments can vary in how stress changes over the week.\n\n(days | dept)\n\ndepartments can vary in how CBD changes stress levels. because each department contains some participants in the CBD group and some in the control group, “the effect of CBD on stress” does exist for a given department, and so could vary between departments. e.g. Philosophers taking CBD get really relaxed, but CBD doesn’t affect Mathematicians that much.\n\n(CBD | dept)\n\n\ndepartments can vary in how CBD affects changes in stress over the week\n\n( day*CBD | dept)\n\n\n\n\n\nNote that the above model is a singular fit, but it gives us a better place to start simplifying from. If we remove the day*CBD interaction in the by-department random effects, we get a model that converges:\n\nmnest2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nAnd plot our fitted values\n\n\nCode\nlibrary(broom.mixed)\naugment(mnest2) |&gt; \n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    # split by departments\n    facet_wrap(~dept) + \n    # make a line for each participant\n    geom_line(aes(group=pid),alpha=.3)+\n    # average fitted value for CBD vs control:  \n    stat_summary(geom=\"line\",aes(col=CBD),lwd=1)\n\n\n\n\n\nFigure 7: Plot of fitted values of the model. Individual lines for each participant, facetted by department. Thicker lines represent the department average fitted values split by CBD group\n\n\n\n\nAnd we can see in our summary that there is a lot of by-department variation - departments vary in their baseline stress levels with a standard deviation of 0.81, and within departments, participants vary in baseline stress scores with a standard deviation of 0.38.\n\nsummary(mnest2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n dept:pid (Intercept) 0.147661 0.38427             \n          day         0.012142 0.11019  -0.03      \n dept     (Intercept) 0.648410 0.80524             \n          day         0.001979 0.04449  -0.18      \n          CBDY        0.055388 0.23535   0.40 -0.22\n Residual             0.129765 0.36023             \nNumber of obs: 1200, groups:  dept:pid, 240; dept, 12\n...\nExamining ranef(mnest2) now gives us a list of dept:pid random effects, and then of dept random effects. We can plot them using dotplot.ranef.mer(), as seen below. From these, we can see for instance, that the effect of CBD is more negative for Theology, and Sociology and Maths have higher slopes of day. These map with the plot of fitted values we saw in Figure 7 - the department lines are going up more Math and Sociology than in other departments, and in Theology the blue CBD line is much lower relative to the red control line than in other departments.\n\ndotplot.ranef.mer(ranef(mnest2))$dept"
  },
  {
    "objectID": "07_ranef.html#example-3-crossed",
    "href": "07_ranef.html#example-3-crossed",
    "title": "7: Random Effect Structures",
    "section": "Example 3: Crossed",
    "text": "Example 3: Crossed\nForgetting about participants nested in departments, let’s return to our sample of 20 staff members from the Psychology department. In our initial study design, we had just one self report measure of stress each day for each person.\nHowever, we might just as easily have taken more measurements. i.e. on Day 1, we could have recorded Martin’s stress levels 10 times. Furthermore, we could have used 10 different measurements of stress, rather than just a self-report measure. We could measure his cortisol levels, blood pressure, heart rate variability, give him different questionnaires, ask an informant like his son to report his stress, and so on. And we could have done the same for everybody.\n\nstresscross &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_crossed.csv\")\nhead(stresscross)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure          day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Aja   N     Alpha-Amylase      1  0.269\n2 Psych Aja   N     Blood Pressure     1  0.855\n3 Psych Aja   N     Cortisol           1  0.278\n4 Psych Aja   N     EEQ                1  0.470\n5 Psych Aja   N     HRV                1 -0.404\n6 Psych Aja   N     Informant          1  0.774\n\n\nIn this case, we can group our participants in two different ways. For each participant we have 5 datapoints for each of 10 different measures of stress. So we have 5x10 = 50 observations for each participant. But if we group them by measure instead, then we have each measure 5 times for 20 participants, so 5x20 = 100 observations of each measure. And there is no hierarchy here - the “blood pressure” measure is the same measure for Martin as it is for Dan and Aja etc. It makes sense to think of by-measure variability as not being ‘within-participants’.\nThis means we can choose when plotting whether to split the plots by participants, with a different line for each measure (Figure 8), or split by measure with a different line for each participant (Figure 9)\n\n\nfacet = participant, line = measure\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  #make a line for each measure\n  geom_line(aes(group=measure))+\n  facet_wrap(~pid)\n\n\n\n\n\nFigure 8: crossed designs with participants and measures. we can facet by participant and plot a line for each measure\n\n\n\n\n\n\nfacet = measure, line = participant\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  # make a line for each ppt\n  geom_line(aes(group=pid))+\n  facet_wrap(~measure)\n\n\n\n\n\nFigure 9: crossed designs with participants and measures. we can facet by measure and plot a line for each participant\n\n\n\n\n\n\nWe can fit a model that therefore accounts for the by-participant variation (“things vary between participants”) and the by-measure variation (“things vary between measures”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nstresscross$day &lt;- stresscross$day-1\n\nmcross &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nNote that just as with the nested example above, we can have different random slopes for measures vs those for participants, depending upon what effects can vary given the study design.\nAs before, removing the interaction in the random effects achieves model convergence:\n\nmcross2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nAnd again we might plot our fitted values either of the ways we plotted our initial data in Figure 8 above, only with the .fitted values obtained from the augment() function:\n\n\nCode\naugment(mcross2) |&gt;\n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    geom_point()+\n    geom_line(aes(group=pid))+\n    facet_wrap(~measure)\n\n\n\n\n\n\n\n\n\nOur random effect variances show the estimated variance in different terms (the intercept, slopes of day, effect of CBD) between participants, and between measures.\nFrom the below it is possible to see, for instance, that there is considerable variability between how measures respond to CBD (they vary in the effect of CBD on stress with a standard deviation of 0.53)\n\nsummary(mcross2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n pid      (Intercept) 0.316578 0.56265             \n          day         0.014693 0.12121  -0.51      \n measure  (Intercept) 0.087111 0.29515             \n          day         0.008542 0.09242   0.88      \n          CBDY        0.283635 0.53257  -0.10  0.11\n Residual             0.088073 0.29677             \nNumber of obs: 1000, groups:  pid, 20; measure, 10\n...\nAgain, our dotplots of random effects help to also show this picture. We can see that the measures of “blood pressure”, “alpha-amylase”, “cortisol”, and “HRV” all have more effects of CBD that are more negative. We can see this in our plot of fitted values - these measures look like CBD vs control differnce is greater than in other measures.\n\ndotplot.ranef.mer(ranef(mcross2))$measure"
  },
  {
    "objectID": "08_modelbuilding.html",
    "href": "08_modelbuilding.html",
    "title": "8: Model Building",
    "section": "",
    "text": "This reading:\n\nthe ‘maximal’ model\nnon-convergence and overfitted models\nstrategies for simplifying models"
  },
  {
    "objectID": "08_modelbuilding.html#maximal-model",
    "href": "08_modelbuilding.html#maximal-model",
    "title": "8: Model Building",
    "section": "Maximal Model",
    "text": "Maximal Model\nTypically for many research designs, the following steps will keep you mostly on track to finding the maximal model.\nStart by thinking of the model structure in terms of these components:\n\nlmer(outcome ~ fixed effects + \n       (random effects | grouping structure), \n     data = ...)\n\n\nSpecify the outcome ~ fixed effects bit first.\n\nThe outcome variable should be clear: it is the variable we are wishing to explain/predict.\nThe fixed effects are the things we want to use to explain/predict variation in the outcome variable. These will often be the things that are of specific inferential interest along with potential confounders and other covariates. Just like the simple linear model.\n\nIf there is a grouping structure to your data, and those groups (preferably n&gt;7 or 8) are perceived as a random sample of a wider population (the specific groups aren’t interesting to you), then consider including random intercepts (and possibly random slopes of predictors) for those groups (1 + ... | grouping).\nIf there are multiple different grouping structures, is one nested within another? If so, we can specify this as (1 | higher_grouping ) + (1 |  lower_grouping:higher_grouping).\nIf the grouping structures are not nested, we can specify them as crossed: (1 | grouping1) + (1 | grouping2).\nIf any of the predictors in the fixed effects vary within the groups, it may be possible to also include them as random effects. For predictors that instead vary between groups, it rarely makes sense to include these as by-group random effects. For example, if we had a model with lmer(score ~ genetic_status + (1 + genetic_status | patient)) then we would be trying to model a process where “the effect of genetic_status on scores is different for each patient”. But if you consider an individual patient, their genetic status never changes. For patient \\(i\\), what is “the effect of genetic status on score”? It’s undefined. This is because genetic status only varies between patients.\n\nas a general rule, don’t specify random effects that are not also specified as fixed effects (an exception could be specifically for model comparison, to isolate the contribution of the fixed effect).\n\nSometimes, things can vary within one grouping, but not within another. E.g., for a design in which patients are nested within hospitals (1 | hospital) + (1 | patient:hospital), genetic_status varies between patients, but within hospitals. Therefore we could theoretically fit a random effect of (1 + genetic_status | hospital), but not one for (1 + genetic_status | patient:hospital)."
  },
  {
    "objectID": "08_modelbuilding.html#non-convergence",
    "href": "08_modelbuilding.html#non-convergence",
    "title": "8: Model Building",
    "section": "Non-Convergence",
    "text": "Non-Convergence\nOftentimes, models with more complex random effect structures will not converge because there are so many parameters, and not enough variability in the data, meaning that there are more places for the model estimation to go wrong. Remember that we fit these models with maximum likelihood estimation (MLE), a process that involves taking a guess at the model parameters that result in the greatest probability of the observed data, and step-by-step improving those guesses until we think we’re at the most likely set of parameters - until the model ‘converges’. Sometimes, however, MLE can sometimes get stuck, resulting in ‘non-convergence’.\nThere are many possible reasons for non-convergence, and it does not necessarily mean the fit is incorrect. However it is is cause for concern, and should be addressed before using the model, else you may end up reporting inferences which do not hold. There are lots of different things which we can try which might help our model to converge. A select few are detailed below:\n\n\n\n\n\n\nThings we can try\n\n\n\n\n\n\nmost likely solutions:\n\ndouble-check the model specification and the data\nConsider simplifying your model (more on this below)\n\n\n\nCenter and scale continuous predictor variables (e.g. with scale)\nChange the optimization method (for example, here we change it to bobyqa):\nlmer(..., control = lmerControl(optimizer=\"bobyqa\"))\nglmer(..., control = glmerControl(optimizer=\"bobyqa\"))\nUse allFit() to try the fit with all available optimizers. This will of course be slow, but is considered ‘the gold standard’; “if all optimizers converge to values that are practically equivalent, then we would consider the convergence warnings to be false positives.”\nallopts &lt;- allFit(model)\nsummary(allopts)\n\n\n\n\nFine-tune an optimizer. Using the optCtrl argument to [g]lmerControl (see ?convergence for details), we can have a lot of control over the optimizer. Recall that the optimizer is a method of iteratively assessing a set of parameters to maximise the probability of seeing the observed data2. We can change things such as the number of steps the algorithm keeps trying for, and the thresholds at which the algorithm stops (Figure 1).\n\n\n\n\n\n\n\nFigure 1: An optimizer will stop after a certain number of iterations, or when it meets a tolerance threshold"
  },
  {
    "objectID": "08_modelbuilding.html#singular-fits",
    "href": "08_modelbuilding.html#singular-fits",
    "title": "8: Model Building",
    "section": "Singular Fits",
    "text": "Singular Fits\nAs well as convergence warnings, you may have noticed that some of our models over the last few weeks have been giving a warning message:\n\nboundary (singular) fit: see ?isSingular\n\nUp to now, we’ve been largely ignoring these messages, but we should really have been addressing them in some way. ‘Singular fit’ warnings indicate that our model is likely to be ‘overfitted’ - that is, the random effects structure which we have specified is too complex to be supported by the data.\nFor simple random effect structures (i.e. a random intercept + a random slope), we can often see this issue reflected in the variance components of the random effect, when variances get estimated at (or very close to) zero, and/or when correlations get estimated at (or very close to) 1 or -1 (Figure 2). With more complex structures it is not always so easily visible, but we can do a double check for this issue using the handy isSingular(model) function - if it returns TRUE then it indicates our model might be overfitted.\n\n\n\n\n\nFigure 2: In simple random effect structures we can often easily see issues of overfitting as they are reflected in variances being estimated as 0 and/or correlations being estimated as perfect correlations of 1 or -1\n\n\n\n\nWhat do we do in these cases? Simplify, simplify, simplify!\n\n\n\n\n\n\nScales can matter!\n\n\n\n\n\n\nThe scale of our predictors can sometimes play a part here. If we were fitting a model of shoe_size ~ height, then the estimated coefficient is going to depend on how we measure height. If we measure it in millimeters, then we’ll probably have a very small coefficient (people’s shoe size will only change by a tiny amount for every 1mm height they gain), but if we measure height in kilometers, then we’ll have a very big coefficient (“grow an extra kilometer in height, and your shoe size will increase 10000 sizes”!!).\nIn the multilevel model, we’re estimating the variances in these relationships across a set of groups (or ‘clusters’). If the coefficient is in millimeters, then the variance is in millimeters too, and so the number will be quite small. If it’s in kilometers, the coefficient is in units 100,000 times bigger, and so is the variance.\nScaling predictors doesn’t change the relationship being studied, but it does change the numeric values we are asking our relationship to be presented in. As the estimation of multilevel models can get into difficulty when variances are too close to zero, you may occasionally receive messages such as those below.\nPay attention to them, and check your variables. If some are on very different scales, then consider trying to rescale them to something that is still meaningful for you.\n\nWarning messages: 1: Some predictor variables are on very different scales: consider rescaling\n\n\nWarning messages: 1: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, :Model is nearly unidentifiable: large eigenvalue ratio - Rescale variables?"
  },
  {
    "objectID": "08_modelbuilding.html#footnotes",
    "href": "08_modelbuilding.html#footnotes",
    "title": "8: Model Building",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthis doesn’t mean simply including every predictor in the fixed effects also in the random effects part. ‘possible’ refers to ‘possible given the study design’↩︎\ninstead of maximising the likelihood, more often (for practical reasons) our algorithms try to minimise \\(-2 \\times\\) the log-likelihood↩︎"
  },
  {
    "objectID": "09_assump.html",
    "href": "09_assump.html",
    "title": "9: MLM Assumptions",
    "section": "",
    "text": "This reading:\n\nMultilevel model assumptions: random effects can be thought of as another level of residual!\nInfluence in multilevel models: influential observations and influential groups."
  },
  {
    "objectID": "09_assump.html#level-1-residuals",
    "href": "09_assump.html#level-1-residuals",
    "title": "9: MLM Assumptions",
    "section": "Level 1 residuals",
    "text": "Level 1 residuals\nWe can get the level 1 (observation-level) residuals the same way we used to do for lm() - by just using resid() or residuals(). Additionally, there are a few useful techniques for plotting these which we have listed below:\n\n\nresid vs fitted\nWe can plot the residuals vs fitted model (just like we used to for lm()), and assess the extend to which the assumption holds that the residuals are zero mean. (we want the blue smoothed line to be fairly close to zero across the plot)\n\n# \"p\" below is for points and \"smooth\" for the smoothed line\nplot(jsmod, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresiduals vs fitted plots will trend upwards\n\n\n\n\n\nAs a result of partial pooling, the residuals vs fitted plots will often tend to go upwards.\nIt’s quite confusing to get your head around, but it is because the groups that are more outlying (which tend to have the lowest and highest fitted values, so they’re on the left and right of the resid vs fitted plot) are the ones that get ‘shrunk’ more towards the center. So the fitted values will get pull towards the middle of the plot, but the residuals will get bigger (more negative for groups on below average, and more positive for groups above average). The result is that the entire plot tilts!\nYou can see an animation of this in Figure 2. The left hand plot shows the model fitted values for each group, with the residuals indicated by dashed lines. The right hand plots shows the residuals vs fitted plot. The animation moves between the ‘no pooling’ and ‘partial pooling’ approach, and as it does so, the plot on the right tilts!\n\n\n\n\n\nFigure 2: The effect of partial pooling on residuals vs fitted plots. The extent to which this tilting happens depends on the amount of shrinkage, so will be more evident when there are few observations in the groups\n\n\n\n\n\n\n\n\n\nscale-location\nAgain, like we can for lm(), we can also look at a scale-location plot. This is where the square-root of the absolute value of the residuals is plotted against the fitted values, and allows us to more easily assess the assumption of constant variance.\n(we want the blue smoothed line to be close to horizontal across the plot)\n\nplot(jsmod,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\nfacetted plots\nWe can also plot these “resid v fitted” and “scale-location” plots for each cluster, to check that our residual mean and variance is not related to the clusters:\n\nplot(jsmod,\n         form = resid(.) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\nplot(jsmod,\n         form = sqrt(abs(resid(.))) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\n\nresidual normality\nWe can also examine the normality the level 1 residuals, using things such as histograms and QQplots:\n(we want the datapoints to follow close to the diagonal line)\n\nqqnorm(resid(jsmod)); qqline(resid(jsmod))\n\n\n\n\n\n\n\n\n\nhist(resid(jsmod))"
  },
  {
    "objectID": "09_assump.html#level-2-residuals",
    "href": "09_assump.html#level-2-residuals",
    "title": "9: MLM Assumptions",
    "section": "Level 2+ residuals",
    "text": "Level 2+ residuals\nThe second level of residuals in the multilevel model are actually just our random effects! We’ve seen them already whenever we use ranef()!\nTo get out these we often need to do a bit of indexing. ranef(model) will give us a list with an item for each grouping. In each item we have a set of columns, one for each thing which is varying by that grouping.\nBelow, we see that ranef(jsmod) gives us something with one entry, $dept, which contains 2 columns (the random intercepts and random slopes of payscale):\n\nranef(jsmod)\n\n$dept\n                                        (Intercept)    payscale\nAccounting                              -0.03045458 -0.19259376\nArchitecture and Landscape Architecture  0.29419381 -0.35855884\nArt                                     -0.29094345  0.15293285\nBusiness Studies                        -0.27858102  0.18008149\n...                                      ...         ... \nSo we can extract the random intercepts using ranef(jsmod)$dept[,1].\nAgain, we want normality of the random effects, so we can make more histograms or qqplots, for both the random intercepts and the random slopes:\ne.g., for the random intercepts:\n\nqqnorm(ranef(jsmod)$dept[,1]);qqline(ranef(jsmod)$dept[,1])\n\n\n\n\n\n\n\n\nand for the random slopes:\n\nqqnorm(ranef(jsmod)$dept[,2]);qqline(ranef(jsmod)$dept[,2])"
  },
  {
    "objectID": "09_assump.html#model-simulations",
    "href": "09_assump.html#model-simulations",
    "title": "9: MLM Assumptions",
    "section": "model simulations",
    "text": "model simulations\nSometimes, a good global assessment of your model comes from how good a representation of the observed data it is. We can look at this in a cool way by simulating from our model a new set of values for the outcome. If we do this a few times over, and plot each ‘draw’ (i.e. set of simulated values), we can look at how well it maps to the observed set of values:\nOne quick way to do this is with the check_predictions() function from the performance package:\n\nlibrary(performance)\ncheck_predictions(jsmod, iterations = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptional: doing it yourself\n\n\n\n\n\nDoing this ourselves gives us a lot more scope to query differences between our observed vs model-predicted data.\nThe simulate() function will simulate response variable values for us.\nThe re.form = NULL bit is saying to include the random effects when making simulations (i.e. use the information about the specific clusters we have in our data). If we said re.form = NA it would base simulations on a randomly generated set of clusters with the associated intercept and slope variances estimated by our model.\n\nmodsim &lt;- simulate(jsmod, nsim = 200, re.form=NULL)\n\nTo get this plotted, we’ll have to do a bit of reworking, because it gives us a separate column for each draw. So if we pivot them longer we can make a density plot for each draw, and then add on top of that our observed scores:\n\n# take the simulations\nmodsim |&gt; \n  # pivot \"everything()\" (useful function to capture all columns),\n  # put column names into \"sim\", and the values into \"value\"\n  pivot_longer(everything(), names_to=\"sim\",values_to=\"value\") |&gt;\n  # plot them! \n  ggplot(aes(x=value))+\n  # plot a different line for each sim. \n  # to make the alpha transparency work, i need to use\n  # geom_line(stat=\"density\") rather than \n  # geom_density() (for some reason alpha applies to fill here)\n  geom_line(aes(group=sim), stat=\"density\", alpha=.1,\n            col=\"darkorange\") +\n  # finally, add the observed scores!  \n  geom_density(data = jsuni, aes(x=jobsat), lwd=1)\n\n\n\n\n\n\n\n\nHowever, we can also go further! We can pick a statistic, let’s use the IQR, and see how different our observed IQR is from the IQRs of a series of simulated draws.\nHere are 1000 simulations. This time I don’t care about simulating for these specific clusters, I just want to compare to random draws of clusters:\n\nsims &lt;- simulate(jsmod, nsim=1000, re.form=NA)\n\nThe apply() function (see also lapply, sapply ,vapply, tapply) is a really nice way to take an object, and apply a function to it. The number 2 here is to say “do it on each column”. If we had 1 it would be saying “do it on each row”.\nThis gives us the IQR of each simulation:\n\nsimsIQR &lt;- apply(sims, 2, IQR)\n\nWe can then ask what proportion of our simulated draws have an IQR smaller than our observed IQR? If the answer is very big or very small it indicates our model does not very represent this part of reality very well.\n\nmean(IQR(jsuni$jobsat)&gt;simsIQR)\n\n[1] 0.451"
  },
  {
    "objectID": "10_centering.html",
    "href": "10_centering.html",
    "title": "10: Centering",
    "section": "",
    "text": "This reading:\nFORTHCOMING"
  },
  {
    "objectID": "11_writing.html",
    "href": "11_writing.html",
    "title": "11: Reporting on analyses with MLM",
    "section": "",
    "text": "This reading:\n\nA (non-exhaustive) checklist of things to think about/include when writing up analyses with multilevel models"
  },
  {
    "objectID": "11_writing.html#the-sample-data",
    "href": "11_writing.html#the-sample-data",
    "title": "11: Reporting on analyses with MLM",
    "section": "The sample data",
    "text": "The sample data\nDescriptives of hierarchical data are sometimes a bit more difficult than when we don’t have any ‘levels’. Typically, what we are wanting to do is provide our readers with a picture of the characteristics of our sample. “Our sample” now refers to multiple levels, so we want to describe each of these. More often than not, one of these levels will be a bit more interesting to us as a population we are hoping to generalise to. In psychology we are usually interested in “people”, so if we have data that is multiple trials per participant, we would probably want to focus on describing the participants (the clusters) as the individual trials are something we exert control over as the experimenter. If each datapoint was a child and they were nested in schools, we would probably want to describe both the children and the schools that are in our sample.\nThe aim here is to provide a picture of our sample so that a reader can get a sense of how ‘transportable’ the findings are to different contexts. For instance, if participants in our study are all university students, then we want to be careful about thinking that the findings will apply in other populations (see e.g. “most people aren’t WEIRD”).\n\nA checklist\n\n\nwhat is the hierarchical data structure (how many levels, what is each level?)\nDescribe any data cleaning outlier/data removal prior to calculating descriptive statistics (these tend to be the impossible values - i.e. observations that you would never want in your data anyway)\nsample sizes: how many at each level?\n\nhow many lower-level within each higher level unit? (if this varies, provide an average, and possibly a min and a max)\n\nscales of measured variables\ndescriptive statistics of relevant variables that characterise your sample.\n\nthese should be computed at the level at which they were measured. For instance, if you have observations grouped by participant, mean(data$age) would give the average age of your observations (which isn’t meaningful, and would differ from the average age of your participants if you have a different number of observations for each participant).\n\nHow much of the variability in the outcome variable is attributable to the clustering? (i.e. ICC)"
  },
  {
    "objectID": "11_writing.html#the-methods",
    "href": "11_writing.html#the-methods",
    "title": "11: Reporting on analyses with MLM",
    "section": "The methods",
    "text": "The methods\nWhen writing up any statistical analysis, one important thing to keep in mind is transparency in the decisions and actions taken in the analysis process. The aim is to avoid a reader wondering “how did they end up with these results?”. Ideally, another researcher would be able to reproduce your analysis based on your explanation of what you have done.\nWith multilevel models, there’s a lot of choices that we make - the scaling and centering of variables, models being fitted with ML vs REML, the method used to conduct inference, and so on. In addition, in the event that we arrived at our final model after a series of non-converging models that were then simplified, we would ideally explain this process.\n\nA checklist\n\n\nDescribe any transformations to the data that are made prior to conducting the analysis (e.g., you’ll often re-center a time variable)\nDescribe the process that led to your final model(s)\n\nClearly explain the structure of your initial model (e.g. this might be the ‘maximal model’), and if this failed to converge, explain what random effects were removed and in what order? if possible, explain why.\n\nState the software packages and versions used to fit models, along with the estimation method (ML/REML) and optimiser used.\n\nWhat is the structure of your final model(s)?\n\nYou don’t need to write a complicated mathematical equation for your model. Describing it in words is fine provided you’re clear. e.g. “the outcome variable Y was modelled using mixed effects regression with afixed effects including a main effect of A and B as well as their interaction. The random effects include a random intercept by participant”\nLinear/binomial/poisson/… - if not linear, what link function (e.g., logit, log) was used?\nSpecify all fixed effects.\nSpecify all random effects according to the sampling units (e.g. schools/children etc) with which they interact. Be careful to make sure it’s clear what slopes are for which groupings!.\n\n\nIt’s often useful to state clearly the relevant test/comparison/parameter estimate of interest, and link this explicitly to the research questions/hypotheses.\n\nAny model comparisons should be clearly stated so that the reader understands the structure of both models being compared.\nSpecify the methods used to conduct inference (e.g. LRT, bootstrap), and if relevant, explain why (e.g. Kenward Rogers might be used due to a small number of level 2 units)."
  },
  {
    "objectID": "11_writing.html#the-results",
    "href": "11_writing.html#the-results",
    "title": "11: Reporting on analyses with MLM",
    "section": "The results",
    "text": "The results\nWriting up results will vary depending on the strategies employed. The important part is to highlight the relevant test/comparison that addresses the research aims, and explain what the result means with respect to the question at hand.\nAdditionally, be sure to take some time to understand what the estimate actually means (\\(p&lt;.05\\) is just a small part of the story). With models like these we are almost always just looking at outcome “differences” between levels of a categorical predictor or “change” across some continuous predictor. Does the estimated difference/change, and its direction, make sense to you? What does it mean practically? Asking yourself questions like this is also a good way of sense checking your analysis (i.e. a strong counter-intuitive finding could mean you have a variable coded back to front!).\nFor reporting parameter estimates, ideally we would include both the estimate and the precision (i.e. the standard error or a confidence interval). When reporting statistical tests, make sure to include the test statistic (\\(t\\), \\(F\\), \\(\\chi^2\\), etc.), the relevant degrees of freedom, and the p-value.\n\nA checklist\n\n\nresults of model comparisons and what they mean in the context of the research question\n\nparameter estimates and precision for relevant fixed effects.\n\nvariance components\n\nhow does the effect of interest vary between groups?\n\nis it related to other group level variance (i.e. the random effect correlations if modelled)\n\nif relevant - sensitivity to influential observations and clusters."
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\n\n\n\n\n\nSolution\n\n\n\nsolution\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multi-level/Mixed Effects Models",
    "section": "",
    "text": "These readings and walkthroughs show how we can extend the linear model to analyses of “hierarchical data”, in which observations are clustered in higher-level groups (e.g. trials within participants, or students within schools). We see how these methods lend themselves well to longitudinal data, and we present one of the more traditional approaches to studying non-linear change over time. The assumptions underlying these models are discussed, along with certain considerations that are important to bear in mind especially for observational data.\nReadings and walkthroughs are presented with accompanying R code."
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin flip. For a fair coin, the chance of getting a heads/tails for any given flip is 0.5.\nWe can simulate the number of “heads” in a single fair coin flip with the following code (because it is a single flip, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\n\nWe can simulate the number of “heads” in 8 fair coin flips with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin flips? Answer: 4! Doing another 8 flips:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin flips, but to do 1000 sets of 8 flips:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  3  42 106 206 287 221  94  33   8"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nSo what is the probability of observing \\(k\\) heads in \\(n\\) flips of a fair coin?\nAs coin flips are independent, we can calculate probability using the product rule (\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent).\nSo the probability of observing 2 heads in 2 flips is \\(0.5 \\cdot 0.5 = 0.25\\)\nWe can get to this probability using dbinom():\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 flips, those two heads could occur in various ways:\n\n\n\n\n\n\n  \n    \n      Ways to get 2 heads in 8 flips\n    \n  \n  \n    TTTTHTHT\n    THHTTTTT\n    THTTHTTT\n    TTTTTHHT\n    TTTHTTHT\n    HTTTTHTT\n    HTTTTTTH\n    TTTHTTTH\n    ...\n  \n  \n  \n\n\n\n\nAs it happens, there are 28 different ways this could happen.2\nThe probability of getting 2 heads in 8 flips of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin flips (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given flip)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin flips is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nSo how does likelihood differ?\nFor likelihood, we are interested in hypotheses about or models of our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?). Do we think it is biased to land on heads 60% of the time? or 30% of the time? All of these are different ‘models’.\nTo consider these hypotheses, we need to observe some data - we need to have a given number of flips, and the resulting number of heads.\nWhereas when discussing probability, we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given flip, for the likelihood we are fixing the number of heads observed, and can make statements about different possible parameters that might govern the coin’s behaviour.\nFor example, let’s suppose we did observe 2 heads in 8 flips, what is the probability of seeing this data given various parameters?\nHere, our parameter (the probability that we think the coin lands on heads) can take any real number between from 0 to 1, but let’s do it for a selection:\n\npossible_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, possible_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin flips, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). The idea that the coin is fair (0.5 probability) is more likely. The most likely parameter is 0.25 (because \\(\\frac{2}{8}=0.25\\)).\nYou can visualise this below:\n\n\n\n\n\nFigure 1: Likelihood curve (probability of observed data across possible parameters)"
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nE.g. above we were talking about \\(P(\\text{2 heads in 8 flips}\\vert \\text{fair coin})\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\).\nGiven that we know that if we have a specific value for \\(\\theta\\), then \\(P(d \\vert \\theta)\\) will give us the probability of observing \\(d\\), we can ask “what value of \\(\\theta\\) will maximise the probability of observing \\(d\\)?”.\nThis will sometimes get written as \\(\\mathcal{L}(\\theta \\vert d)\\) as the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#maximum-likelihood-estimation",
    "href": "lvp.html#maximum-likelihood-estimation",
    "title": "Likelihood vs Probability",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nFigure 1 shows the probability of our observed data (2 heads in 8 coin flips) for various “models” of the coin’s behaviour. In this simple case, the candidate “models” of the coin’s behaviour are simply different values for “the probability of getting heads”.\nHowever, the idea of “probability of data given some model” can scale up to more complex statistical models like regression models. The important difference is that such models consist of more than just one parameter. For instance, a simple regression model of lm(y ~ 1 + x, data) involves three things: the intercept, the slope of x, and the standard deviation of the error.\nMaximum likelihood estimation is the process of optimising the set of parameters that result in the greatest probability of the observed data. In our coin-flip example in Figure 1, this is essentially asking “where is the top of the curve?”, but as soon as we add more parameters we have more dimensions, and the curve becomes a surface."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. There are other ways to do statistics (not covered in this course) - e.g., in Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎\nIf you really want to see them all, try running combn(8, 2) in your console.↩︎"
  }
]