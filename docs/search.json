[
  {
    "objectID": "00_datasetsRAUDENBUSHBRYK.html",
    "href": "00_datasetsRAUDENBUSHBRYK.html",
    "title": "Practice Datasets",
    "section": "",
    "text": "Below are various datasets from these readings and from lectures and exercises across our courses. For each one, there is a quick explanation of the study design which also details the research aims of the project.\nPick one of the datasets to test yourself with fitting, checking, interpreting, and reporting on multilevel models.\n\n\n\n\n\n\n\nschoolmot.csv - motivation and grades in school children #cross-sectional\n\n\n\n\n\nThis dataset contains information on 900 children from 30 different schools across Scotland. The data was collected as part of a study looking at whether education-related motivation is associated with school grades. This is expected to be different for state vs privately funded schools.\nAll children completed an ‘education motivation’ questionnaire, and their end-of-year grade average has been recorded.\nData are available at https://uoepsy.github.io/data/schoolmot.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nmotiv\nChild's Education Motivation Score (range 0 - 10)\n\n\nfunding\nFunding ('state' or 'private')\n\n\nschoolid\nName of School that the child attends\n\n\ngrade\nChild's end-of-year grade average (0-100)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n\nmod1 &lt;- lmer(grade ~ motiv * funding + \n               (1 + motiv | schoolid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_lifesatscot.csv - #cross-sectional\n\n\n\n\n\nThese data come from 112 people across 12 different Scottish dwellings (cities and towns). Information is captured on their ages and a measure of life satisfaction. The researchers are interested in if there is an association between age and life-satisfaction.\nData are available at https://uoepsy.github.io/data/lmm_lifesatscot.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nage\nAge (years)\n\n\nlifesat\nLife Satisfaction score\n\n\ndwelling\nDwelling (town/city in Scotland)\n\n\nsize\nSize of Dwelling (&gt; or &lt;100k people)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_lifesatscot.csv\")\n\nmod &lt;- lmer(lifesat ~ 1 + age + (1 + age | dwelling), df)\n\n# if you want to see a cross-level interaction (not relevant for RQ):\ndf$age &lt;- df$age/10 # makes fitting easier\nmod2 &lt;- lmer(lifesat ~ 1 + age * size + (1 + age | dwelling), df)\n\n\n\n\n\n\n\n\n\n\nlmm_jsup.csv - Workplace pride#cross-sectional\n\n\n\n\n\nA questionnaire was sent to all UK civil service departments, and the lmm_jsup.csv dataset contains all responses that were received. Some of these departments work as hybrid or ‘virtual’ departments, with a mix of remote and office-based employees. Others are fully office-based.\nThe questionnaire included items asking about how much the respondent believe in the department and how it engages with the community, what it produces, how it operates and how treats its people. A composite measure of ‘workplace-pride’ was constructed for each employee. Employees in the civil service are categorised into 3 different roles: A, B and C. The roles tend to increase in responsibility, with role C being more managerial, and role A having less responsibility. We also have data on the length of time each employee has been in the department (sometimes new employees come straight in at role C, but many of them start in role A and work up over time).\nWe’re interested in whether the different roles are associated with differences in workplace-pride.\nData are available at https://uoepsy.github.io/data/lmm_jsup.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndepartment_name\nName of government department\n\n\ndept\nDepartment Acronym\n\n\nvirtual\nWhether the department functions as hybrid department with various employees working remotely (1), or as a fully in-person office (0)\n\n\nrole\nEmployee role (A, B or C)\n\n\nseniority\nEmployees seniority point. These map to roles, such that role A is 0-4, role B is 5-9, role C is 10-14. Higher numbers indicate more seniority\n\n\nemployment_length\nLength of employment in the department (years)\n\n\nwp\nComposite Measure of 'Workplace Pride'\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\n# either Roles or Seniority would work here:\nmod &lt;- lmer(wp ~ employment_length + seniority + (1 + seniority | dept), df)\n\n# doesn't converge:\nmod &lt;- lmer(wp ~ employment_length + role + (1 + role | dept), df)\nmod &lt;- lmer(wp ~ employment_length + role + (1 | dept), df)\n\n\n\n\n\n\n\n\n\n\nwellbeingwork3.rda - work patterns and mental wellbeing #longitudinal\n\n\n\n\n\nThe “Wellbeing in Work” dataset contains information on employee wellbeing, assessed at baseline (start of study), 12 months post, 24 months post, and 36 months post. over the course of 36 months. Participants were randomly assigned to one of three employment conditions:\n\ncontrol: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.\n\nunlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description.\nfourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nThe researchers have two main questions: Overall, did the participants’ wellbeing stay the same or did it change? Did the employment condition groups differ in the how wellbeing changed over the assessment period?\nData are available (in .rda format) at https://uoepsy.github.io/data/wellbeingwork3.rda\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nID\nParticipant ID\n\n\nTimePoint\nTimepoint (0 = baseline, 1 = 12 months, 2 = 24 months, 3 = 36 months)\n\n\nCondition\nEmployment Condition ('control' = 5 day week, 28 days of leave. 'unlimited_leave' = 5 days a week, unlimited leave. 'fourday_week' = 4 day week, 28 days of leave)\n\n\nWellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\n\nmod1 &lt;- lmer(Wellbeing~TimePoint+(1+TimePoint|ID), \n             data = wellbeingwork3)\nmod2 &lt;- lmer(Wellbeing~TimePoint+Condition+(1+TimePoint|ID), \n             data = wellbeingwork3)\nmod3 &lt;- lmer(Wellbeing~TimePoint*Condition+(1+TimePoint|ID), \n             data = wellbeingwork3)\nanova(mod1,mod2,mod3)\n\nsummary(mod3)\n\n\n\n\n\n\n\n\n\n\ntoy2.csv - Toys!#cross-sectional\n\n\n\n\n\nThis example builds on one from the USMR course, where the lectures explored linear regression with a “toy dataset” looking at how hours of practice influences the reading age of different toy characters (see USMR Week 7 Lecture). Here, we broaden our scope to the investigation of how practice affects reading age for all toys (not just Martin’s Playmobil characters).\nData are available at https://uoepsy.github.io/data/toy2.csv containing information on 129 different toy characters that come from a selection of different families/types of toy.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ntoy_type\nType of Toy\n\n\nyear\nYear Released\n\n\ntoy\nCharacter\n\n\nhrs_week\nHours of practice per week\n\n\nR_AGE\nReading Age\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/toy2.csv\")\n\nmod &lt;- lmer(R_AGE ~ 1 + hrs_week + (1 + hrs_week | toy_type), df)\n\n\n\n\n\n\n\n\n\n\nLAAwellbeing.csv - mental wellbeing across Scotland#cross-sectional\n\n\n\n\n\nResearchers want to study the relationship between time spent outdoors and mental wellbeing, across all of Scotland. They contact all the Local Authority Areas (LAAs) and ask them to collect data for them, with participants completing the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being, and being asked to estimate the average number of hours they spend outdoors each week. Twenty of the Local Authority Areas provided data.\nData are available at https://uoepsy.github.io/data/LAAwellbeing.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier\n\n\nname\nParticipant Name\n\n\nlaa\nLocal Authority Area\n\n\noutdoor_time\nNumber of hours spent outdoors per week\n\n\nwellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n\n\ndensity\nPopulation density of local authority area (number of people per square km)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n\nmod1 &lt;- lmer(wellbeing ~ density + outdoor_time + \n       (1 + outdoor_time | laa), \n       data = df) \n\nsummary(mod1)\n\neaster egg: check for influential people!\n\n\n\n\n\n\n\n\n\nstressint.csv - CBT and stress levels#longitudinal\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in changes in stress across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier\n\n\nstress\nStress (range 0 to 100)\n\n\ntime\nTime (pre/post/during)\n\n\ngroup\nWhether participant is in the CBT group or control group\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressint.csv\")\ndf$time &lt;- factor(df$time, levels=c(\"Pre\",\"During\",\"Post\"))\n\nTemptation is to fit the below, but it won’t work, because each pid has only 1 obs for each time-point, so we’re overfitting (the whole Error: number of observations (=150) &lt;= number of random effects (=150) for term message).\n\nmod1 &lt;- lmer(stress ~ time*group + \n               (1 + time|ppt), \n             data = df)\n\n\nmod1 &lt;- lmer(stress ~ time*group + \n               (1 |ppt), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\ndrivingmusicwithin.csv - the influence of music on driving speeds#repeated-measures\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npid\nParticipant Identifier\n\n\nspeed\nAvg Speed Driven on Route (mph)\n\n\nmusic\nMusic listened to while driving (classical music / rap music / spoken word)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/drivingmusicwithin.csv\")\n\nTemptation is to fit the below, but it won’t work, because each pid has only 1 obs for each music, so we’re overfitting\n\nmod1 &lt;- lmer(speed ~ music + \n               (1 + music | pid), \n             data = df)\n\n\nmod1 &lt;- lmer(speed ~ music + \n               (1 | pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\ndapr3_mannequin.csv - the role of mannequins in clothing purchases#repeated-measures\n\n\n\n\n\nDoes clothing seem more attractive to shoppers when it is viewed on a model, and is this dependent on item price? 30 participants were presented with a set of pictures of items of clothing, and rated each item how likely they were to buy it. Each participant saw 20 items, ranging in price from £5 to £100. 15 participants saw these items worn by a model, while the other 15 saw the items against a white background.\nData are available at https://uoepsy.github.io/data/dapr3_mannequin.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npurch_rating\nPurchase Rating (sliding scale 0 to 100, with higher ratings indicating greater perceived likelihood of purchase)\n\n\nprice\nPrice presented with item (range £5 to £100)\n\n\nppt\nParticipant Identifier\n\n\ncondition\nWhether items are seen on a model or on a white background\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/dapr3_mannequin.csv\")\n\n#scale price to help convergence\n#change 1 in price is now change of £10\ndf$price &lt;- df$price/10\n\nmod1 &lt;- lmer(purch_rating ~ price*condition + \n       (1+price|ppt), \n       data = df)\n  \nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: purch_rating ~ price * condition + (1 + price | ppt)\n   Data: df\n\nREML criterion at convergence: 4754.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7237 -0.6389  0.0491  0.6836  3.2354 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ppt      (Intercept)  59.361   7.7046       \n          price         0.713   0.8444  -0.78\n Residual             148.039  12.1671       \nNumber of obs: 600, groups:  ppt, 30\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)           41.2807     2.4672  16.732\nprice                  2.4767     0.3270   7.575\nconditionmodel        -1.8533     3.4891  -0.531\nprice:conditionmodel   1.1600     0.4624   2.509\n\nCorrelation of Fixed Effects:\n            (Intr) price  cndtnm\nprice       -0.804              \nconditinmdl -0.707  0.568       \nprc:cndtnmd  0.568 -0.707 -0.804\n\n\n\n\n\n\n\n\n\n\n\ncrqeds.csv - routine and emotion dysregulation in children#cross-sectional\n\n\n\n\n\nAre children with more day-to-day routine better at regulating their emotions? A study of 200 children from 20 schools (9 private schools and 11 state schools) completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ).\nData are available at https://uoepsy.github.io/data/crqeds.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nschoolid\nSchool Identifier\n\n\nEDS\nEmotion Dysregulation Score (range 1-6, higher values indicate more *dys*regulation of emotions)\n\n\nCRQ\nChildhood Routine Questionnaire Score (range 0-7, higher values indicate more day-to-day routine)\n\n\nschooltype\nSchool type (private / state)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/crqeds.csv\")\n\nmod1 &lt;- lmer(EDS ~ schooltype + CRQ + \n       (1 + CRQ | schoolid), \n       data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_ef_sdmt.csv - Audio interference in executive functioning#repeated-measures\n\n\n\n\n\nThis data is from a simulated study that aims to investigate the following research question:\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n30 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room. Unfortunately, lots of the tests were not administered correctly, and so not every participant has the full 15 trials worth of data.\nData are available at https://uoepsy.github.io/data/lmm_ef_sdmt.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\naudio\nAudio heard during the test ('no_audio', 'white_noise','music')\n\n\nheadphones\nWhether the participant listened via speakers (S) in the room or via noise cancelling headphones (H)\n\n\nSDMT\nSymbol Digit Modalities Test (SDMT) score\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 11. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_ef_sdmt.csv\")\n\nmod1 &lt;- lmer(SDMT ~ audio * headphones + \n               (1 + audio | PID), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_nssjobsat.csv - #cross-sectional\n\n\n\n\n\nLet’s suppose we are studying employee job satisfaction at the university, and we want to estimate the association between pay-scale and job satisfaction, controlling for the NSS rating of departments.\nWe have 399 employees from 25 different departments, and we got them to fill in a job satisfaction questionnaire, and got information on what their payscale was. We have also taken information from the national student survey on the level of student satisfaction for each department.\nEach datapoint here represents an individual employee, and these employees are grouped into departments.\nData are available at https://uoepsy.github.io/data/msmr_nssjobsat.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nNSSrating\nNational Student Satisfaction Rating for the Department\n\n\ndept\nDepartment name\n\n\npayscale\nPay scale of employee\n\n\njobsat\nJob satisfaction of employee\n\n\njobsat_binary\nBinary question of whether the employee considered themselves to be satisfied with their work (1) or not (0)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 12. \n\ndf&lt;-read_csv(\"https://uoepsy.github.io/data/msmr_nssjobsat.csv\")\n\nmod &lt;- lmer(jobsat ~ 1 + NSSrating + payscale + (1 + payscale | dept), df)\n\n\n\n\n\n\n\n\n\n\nlmm_apespecies.csv & lmm_apeage.csv - dominance in adolescence of great apes#longitudinal\n\n\n\n\n\nWe have data from a large sample of great apes who have been studied between the ages of 1 to 10 years old (i.e. during adolescence). Our data includes 4 species of great apes: Chimpanzees, Bonobos, Gorillas and Orangutans. Each ape has been assessed on a primate dominance scale at various ages. Data collection was not very rigorous, so apes do not have consistent assessment schedules (i.e., one may have been assessed at ages 1, 3 and 6, whereas another at ages 2 and 8).\nThe researchers are interested in examining how the adolescent development of dominance in great apes differs between species.\nData on the dominance scores of the apes are available at https://uoepsy.github.io/data/lmm_apeage.csv and the information about which species each ape is are in https://uoepsy.github.io/data/lmm_apespecies.csv.\n\n\n\n\n\n\nTable 1: Data Dictionary: lmm_apespecies.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nape\nApe Name\n\n\nspecies\nSpecies (Bonobo, Chimpanzee, Gorilla, Orangutan)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Data Dictionary: lmm_apeage.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nape\nApe Name\n\n\nage\nAge at assessment (years)\n\n\ndominance\nDominance (Z-scored)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 13. \n\ndfape1 &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_apespecies.csv\")\ndfape2 &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_apeage.csv\")\n\ndf &lt;- full_join(dfape1, dfape2)\n\nsome cleaning is needed:\n\ndf &lt;- df |&gt; \n  mutate(\n    # fix species typos\n    species = case_when(\n      species %in% c(\"chimp\",\"chimpanzee\") ~ \"chimp\",\n      species %in% c(\"gorilla\",\"gorrila\") ~ \"gorilla\",\n      TRUE ~ species\n    )\n  ) |&gt;\n    filter(\n      # get rid of ages -99\n      age &gt; 0, \n      # keep when dominance is between -5 and 5 \n      # (5 here is a slightly arbitrary choice, but you can see from\n      # our checks that this will only exclude the two extreme datapoints\n      # that are 21.2 and 19.4\n      (dominance &lt; 5 & dominance &gt; -5) \n    )\n\nmodel comparison answers “do species differ in growth of dominance?”\nfor “how do specific species differ?” we look at fixed effects.\n\nmod &lt;- lmer(dominance ~ age * species + (1 + age | ape), df)\nmod.rstr &lt;- lmer(dominance ~ age + species + (1 + age | ape), df)\n\nanova(mod.rstr, mod)\n\n\n\n\n\n\n\n\n\n\nlmm_mindfuldecline.csv - mindfulness and cognitive decline#longitudinal\n\n\n\n\n\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.\nData are available at https://uoepsy.github.io/data/lmm_mindfuldecline.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nsitename\nSite Identifier\n\n\nppt\nParticipant Identifier\n\n\ncondition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n\n\nvisit\nStudy Visit Number (1 - 10)\n\n\nage\nAge (in years) at study visit\n\n\nACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n\n\nimp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 14. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_mindfuldecline.csv\")\n\nif we don’t recenter age, then the intercept variability (1|ppt) is estimated way back 60 years before we collected any data. The slopes will basically perfectly predicted these intercept differences, so the model will be singular.\nre-centering will help.\n\n#recenter age\ndf$ageC &lt;- df$age-60\n\nmod1 &lt;- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_mindfuldeclineFULL.csv - Multi-center Mindful Cognitive Aging#longitudinal#more-complex-groupings\n\n\n\n\n\nA large study involving 14 different research centers is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. Each site recruits between 15 and 30 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). For each center, roughly half of the participants engaged with daily mindfulness sessions, while the remaining participants did not.\nData are available at https://uoepsy.github.io/data/lmm_mindfuldeclineFULL.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nsitename\nSite Identifier\n\n\nppt\nParticipant Identifier\n\n\ncondition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n\n\nvisit\nVisit number (1 - 10)\n\n\nage\nAge (years) at visit\n\n\nACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n\n\nimp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 15. as above but it’s nested\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_mindfuldeclineFULL.csv\")\ndf$ageC &lt;- df$age-60\n\n# maximal model won't converge\nmmod &lt;- lmer(ACE ~ 1 + ageC * condition + \n              ( 1 + ageC * condition | sitename) +\n               (1 + ageC | sitename:ppt), \n             data = df)\n\n# probably simplify to \nmod &lt;- lmer(ACE ~ 1 + ageC * condition + \n              ( 1 + ageC | sitename) +\n               (1 + ageC | sitename:ppt), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nstressweek1.csv - CBD drinks and stress levels: Version 1#longitudinal\n\n\n\n\n\nSuppose that we conducted an experiment on a sample of 20 staff members from the Psychology department to investigate effects of CBD consumption on stress over the course of the working week. Participants were randomly allocated to one of two conditions: the control group continued as normal, and the CBD group were given one CBD drink every day. Over the course of the working week (5 days) participants stress levels were measured using a self-report questionnaire.\nData are available at https://uoepsy.github.io/data/stressweek1.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndept\nDepartment\n\n\npid\nParticipant Name\n\n\nCBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n\n\nmeasure\nMeasure used to assess stress levels\n\n\nday\nDay of the working week (1 to 5)\n\n\nstress\nStress Level (standardised)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 16. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek1.csv\")\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n               (1 + day | pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nstressweek_nested.csv - CBD drinks and stress levels: Version 2#longitudinal#more-complex-groupings\n\n\n\n\n\nAs for Version 1 of this study (see above), but instead of a sample of 20 participants from the psychology staff, we have 240 people from various departments such as History, Philosophy, Art, etc..\nData are available at https://uoepsy.github.io/data/stressweek_nested.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndept\nDepartment\n\n\npid\nParticipant Name\n\n\nCBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n\n\nmeasure\nMeasure used to assess stress levels\n\n\nday\nDay of the working week (1 to 5)\n\n\nstress\nStress Level (standardised)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 17. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_nested.csv\")\n\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\n# removed day*CBD|dept to obtain convergence\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n               (1 + day + CBD | dept) +\n               (1 + day | dept:pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nstressweek_crossed.csv - CBD drinks and stress levels: Version 3#longitudinal#more-complex-groupings\n\n\n\n\n\nAs for Version 1 of this study (see above), with 20 staff members from the Psychology department, but instead of taking a measurement only on a self-report scale, we took 10 different measures every time point (cortisol levels, blood pressure, heart rate variability, various questionnaires etc).\nData are available at https://uoepsy.github.io/data/stressweek_crossed.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndept\nDepartment\n\n\npid\nParticipant Name\n\n\nCBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n\n\nmeasure\nMeasure used to assess stress levels\n\n\nday\nDay of the working week (1 to 5)\n\n\nstress\nStress Level (standardised)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 18. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_crossed.csv\")\n\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\n# removed day*CBD|dept to obtain convergence\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | measure) +\n                (1 + day | pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\ncogdecline.csv - domain differences in cognitive aging#longitudinal#more-complex-groupings\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 48, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nData are available at https://uoepsy.github.io/data/cogdecline.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ncID\nCenter ID\n\n\npptID\nParticipant Identifier\n\n\neduc\nEducational attainment (years of education)\n\n\nage\nAge at visit (years)\n\n\nprocessing_speed\nScore on Processing Speed domain task\n\n\nspatial_visualisation\nScore on Spatial Visualisation domain task\n\n\nmemory\nScore on Memory domain task\n\n\nreasoning\nScore on Reasoning domain task\n\n\nvocabulary\nScore on Vocabulary domain task\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 19. we need to reshape this data! we don’t have a single variable “score” here, because it’s actually split across 5 columns (one for each domain). So we reshape those columns to have a variable called “score” and a variable indicating which domain it is a score of (we’ll call it “domain”):\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/cogdecline.csv\")\n\n# reshape\ndf &lt;- df |&gt; pivot_longer(processing_speed:vocabulary,\n                   names_to = \"domain\",\n                   values_to = \"score\")\n\n# recenter age and educ\ndf$age &lt;- (df$age - 48)/5\ndf$educ &lt;- df$educ - min(df$educ)\n\nthis won’t converge:\n\nmod1 &lt;- lmer(score ~ educ + age * domain + \n               (1 + educ + age * domain | cID) + \n               (1 + age * domain | cID:pptID),\n             data = df)\n\nthis will - it’s using the trick of putting the domain on the RHS here, to still have different slopes of score~age for each domain:\n\nmod1 &lt;- lmer(score ~ educ + age * domain + \n               (1 | cID) + \n               (1 + age | cID:pptID) +\n               (1 + age | cID:pptID:domain),\n             data = df,\n             control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_gadeduc.csv - Psychoeducation treatment effects#longitudinal#more-complex-groupings\n\n\n\n\n\nThis is synthetic data from a randomised controlled trial to evaluate the efficacy of a psychoeducational treatment on anxiety disorders, in which 30 therapists randomly assigned patients (each therapist saw between 2 and 28 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nData are available at https://uoepsy.github.io/data/lmm_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 20. this one needs lots of reshaping and sorting out first, but it can actually be done with minimal code using things like separate()\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\ndf &lt;- df |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nmod &lt;- lmer(GAD~ visit*group+ \n              (1+visit*group|therapist)+\n              (1+visit|therapist:patient),\n            df)\n\n\n\n\n\n\n\n\n\n\nabs_intervention.csv - evaluating an intervention to reduce adolescent aggressive behaviours#longitudinal#more-complex-groupings\n\n\n\n\n\nIn 2010 A US state’s commissioner for education was faced with growing community concern about rising levels of adolescent antisocial behaviours.\nAfter a series of focus groups, the commissioner approved the trialing of an intervention in which yearly Parent Management Training (PMT) group sessions were offered to the parents of a cohort of students entering 10 different high schools. Every year, the parents were asked to fill out an informant-based version of the Aggressive Behaviour Scale (ABS), measuring verbal and physical abuse, socially inappropriate behavior, and resisting care. Where possible, the same parents were followed up throughout the child’s progression through high school. Alongside this, parents from a cohort of students entering 10 further high schools in the state were recruited to also complete the same informant-based ABS, but were not offered the PMT group sessions.\nThe commissioner has two main questions: Does the presentation of aggressive behaviours increase as children enter the secondary school system? Is there any evidence for the effectiveness of Parent Management Training (PMT) group sessions in curbing the rise of aggressive behaviors during a child’s transition into the secondary school system?\nData are available at https://uoepsy.github.io/data/abs_intervention.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nschoolid\nSchool Name\n\n\nppt\nParticipant Identifier\n\n\nage\nAge (years)\n\n\ninterv\nWhether or not parents attended Parent Management Training (PMT) group sessions (0 = No, 1 = Yes)\n\n\nABS\nAggressive Behaviours Scale. Measures verbal and physical abuse, socially inappropriate behavior, and resisting care. Scores range from 0 to 100, with higher scores indicating more aggressive behaviours.\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 21. Check here: https://uoepsy.github.io/dapr3/2425/lectures/05recap.html#/the-research-process-model-specification\n\n\n\n\n\n\n\n\n\nlmm_laughs.csv - the visual aspect of humour#repeated-measures#more-complex-groupings\n\n\n\n\n\nThese data are simulated to imitate an experiment that investigates the effect of visual non-verbal communication (i.e. gestures, facial expressions) on joke appreciation. 90 Participants took part in the experiment, in which they each rated how funny they found a set of 30 jokes. For each participant, the order of these 30 jokes was randomly set for each run of the experiment. For each participant, the set of jokes was randomly split into two halves, with the first half being presented in audio-only, and the second half being presented in audio and video. This meant that each participant saw 15 jokes with video and 15 without, and each joke would be presented in with video roughly half of the times it was seen.\nThe researchers want to investigate whether the delivery (audio/audiovideo) of jokes is associated with differences in humour-ratings.\nData are available at https://uoepsy.github.io/data/lmm_laughs.csv\n\n\n\n\nTable 3: Data Dictionary: lmm_laughs.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identification Number\n\n\njoke_label\nJoke presented\n\n\njoke_id\nJoke Identification Number\n\n\ndelivery\nExperimental manipulation: whether joke was presented in audio-only ('audio') or in audiovideo ('video')\n\n\nrating\nHumour rating chosen on a slider from 0 to 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 22. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_laughs.csv\")\n\nmod &lt;- lmer(rating ~ delivery + \n              (1 + delivery | joke_id) +\n              (1 + delivery | ppt),\n            data = df)\n\nworth looking at random effects for different jokes to see which ones are funniest etc.\nThe jokes are all from some study 10 years ago that aimed to find “the worlds funniest joke”\n\n\n\n\n\n\n\n\n\nNGV.csv - Video game aggression and the dark triad#repeated-measures#more-complex-groupings\n\n\n\n\n\nThese data are from an experiment designed to investigate how the realism of video games is associated with more/less unnecessarily aggressive gameplay, and whether this differs depending upon a) the playing mode (playing on a screen vs VR headset), and b) individual differences in the ‘dark triad’ personality traits.\nThe experiment involved playing 10 levels of a game in which the objective was to escape a maze. Various obstacles and other characters were present throughout the maze, and players could interact with these by side-stepping or jumping over them, or by pushing or shooting at them. All of these actions took the same amount of effort to complete (pressing a button), and each one achieved the same end (moving beyond the obstacle and being able to continue through the maze).\nEach participant completed all 10 levels twice, once in which all characters were presented as cartoons, and once in which all characters were presented as realistic humans and animals. The layout of the level was identical in both, the only difference being the depiction of objects and characters. For each participant, these 20 levels (\\(2 \\times 10\\) mazes) were presented in a random order. Half of the participants played via a screen, and the other half played via a VR headset. For each level played, we have a record of “needless game violence” (NGV) which was calculated via the number of aggressive (pushing/shooting) actions taken (+0.5 for every action that missed an object, +1 for every action aimed at an inanimate object, and +2 for every action aimed at an animate character).\nPrior to the experiment, each participant completed the Short Dark Triad 3 (SD-3), which measures the three traits of machiavellianism, narcissism, and psychopathy.\nData are available at https://uoepsy.github.io/data/NGV.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant number\n\n\nage\nParticipant age (years)\n\n\nlevel\nMaze level (1 to 20)\n\n\ncharacter\nWhether the objects and characters in the level were presented as 'cartoon' or as 'realistic'\n\n\nmode\nWhether the participant played via a screen or with a VR headset\n\n\nP\nPsycopathy Trait from SD-3 (score 1-5)\n\n\nN\nNarcissism Trait from SD-3 (score 1-5)\n\n\nM\nMachiavellianism Trait from SD-3 (score 1-5)\n\n\nNGV\nNeedless Game Violence metric\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 23. (you could also try to fit the interactions in the random effects here but i’m not going to even try!)\n\nm0 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + character + mode + P + M + N | level), data = ngv)\n\nafter some simplification, I end up at the model below. You might end up at a slightly different random effect structure, and that is completely okay! The important thing is to be transparent in your decisions.\n\nm1 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + mode | level), data = ngv)\n\n\n\n\n\n\n\n\n\n\nerm_belief.csv - erm.. I don’t believe you#repeated-measures#more-complex-groupings\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random. Participants rated each statement on how believable it is on a scale of 0 to 100.\nData are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier\n\n\ntrial_n\nTrial number\n\n\nsentence\nStatement identifier\n\n\ncondition\nCondition (fluent v disfluent)\n\n\nbelief\nbelief rating (0-100)\n\n\nstatement\nStatement\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 24. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/erm_belief.csv\")\n# make trial_n numeric\ndf$trial_n &lt;- as.numeric(gsub(\"trial_\",\"\",df$trial_n))\n# relevel condition:\ndf$condition &lt;- factor(df$condition, levels=c(\"fluent\",\"disfluent\"))\n\n# having (trial_n | ppt) doesn't converge, so simplify to:\nmod1 &lt;- lmer(belief ~ trial_n + condition + \n               (1 | ppt) + \n               (1 + condition | statement), \n             data = df)\n\nsummary(mod1)\n\neaster egg: plot your ranefs\n\ndotplot.ranef.mer(ranef(mod1))$statement\n\n\n\n\n\n\n\n\n\n\nlmm_alcgad.csv - relative levels of anxiety and alcohol use#repeated-measures\n\n\n\n\n\nA research study is investigating how anxiety is associated with drinking habits. Data was collected from 50 participants from 5 centers. Researchers administered the generalised anxiety disorder (GAD-7) questionnaire to measure levels of anxiety over the past week, and collected information on the units of alcohol participants had consumed within the week. Each participant was observed on 10 different occasions.\nThe researchers are also interested in testing an intervention (given to half of the participants) - they want to know if this changes the association between anxiety and alcohol consumption.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nalcunits\nNumber of units of alcohol consumed over the past week\n\n\ngad\nScore on the Generalised Anxiety Disorder scale (scores 0-3 on 7 questions are totalled for an overall score)\n\n\nintervention\nWhether the participant is part of the intervention group (1) or not (0)\n\n\ncenter\nCenter ID\n\n\nppt\nParticipant ID\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 25. We actually have participants nested in center here, but there are only 5 centers..\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_alcgad.csv\")\n\nfirst though, we’ll calculate ppt means and ppt deviations-from-means\n\ndf &lt;- df |&gt; \n  group_by(ppt) |&gt;\n  mutate(\n    gadm = mean(gad),\n    gaddev = gad - mean(gad)\n  ) |&gt; ungroup()\n\nthis works, but bear in mind we do only have 5 centers..\n\nmod &lt;- lmer(alcunits ~ gadm + gaddev + \n       (1 + gaddev | center) +\n       (1 + gaddev | center:ppt),\n     df)\n\nthis would be defensible too:\n\nmod &lt;- lmer(alcunits ~ center + gadm + gaddev + \n       (1 + gaddev | ppt),\n     df)\n\nWe also have the intervention question, which to meet convergence might need to do something like:\n\nmod &lt;- lmer(alcunits ~ center + (gadm + gaddev)*intervention + \n       (1 | ppt),\n     df)\n\n\n\n\n\n\n\n\n\n\ndapr3_tgu.csv - Physiotherapy and physical functioning #repeated-measures\n\n\n\n\n\nA researcher is interested in the efficacy of physiotherapy in helping people to regain normal physical functioning. They are curious whether doing more physiotherapy leads to better outcomes, or if it is possibly that the patients who tend to do more of their exercises tend to have better outcomes. 20 in-patients from 2 different hospitals (1 private, 1 govt funded) were monitored over the course of their recovery following knee-surgery. Every day, the time each patient spent doing their physiotherapy exercises was recorded. At the end of each day, participants completed the “Time get up and go” task, a measure of physical functioning.\nData are available at https://uoepsy.github.io/data/dapr3_tgu.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ntgu\nTime Get up and Go Task - measure of physical functioning. Scored in minutes, with lower scores indicating better physical functioning\n\n\nphys\nMinutes of physiotherapy exercises completed that day\n\n\nhospital\nHospital ID\n\n\npatient\nPatient ID\n\n\nprioritylevel\nPriority level of patients' surgery (rank 1-4, with 1 being most urgent surgey, and 4 being least urgent)\n\n\nprivate\n0 = government funded hospital, 1 = private hospital\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 26. patients are nested in hospitals, but there are only 2 hospitals!\njust shove it in as fixed predictor\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/dapr3_tgu.csv\")\n\ndf &lt;- df |&gt; group_by(patient) |&gt;\n  mutate(\n    physm = mean(phys),\n    physdev = phys - mean(phys)\n  ) |&gt; ungroup()\n\nmod &lt;- lmer(tgu ~ hospital + physm + physdev + \n              (1 + physdev | patient),\n            data = df)\n\n\n\n\n\n\n\n\n\n\nlmm_bflpe.csv - big fish little pond #cross-sectional\n\n\n\n\n\nThese data are simulated based on the “Big-fish-little-pond” effect in educational literature.\nWe are interested in better understanding the relationship between school children’s grades and their academic self-concept (their self-perception of ability in specific and general academic disciplines).\nWe have data from 20 classes of children, capturing information on their grades at school (range 0 to 10), and a measure of academic self-concept:\nData are available at https://uoepsy.github.io/data/lmm_bflpe.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ngrade\nChildren's School Grade\n\n\nclass\nClass Identifier\n\n\nself_concept\nChildren's Self-Concept Score\n\n\nchild\nChild Identifier\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 27. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_bflpe.csv\")\n\ndf &lt;- df |&gt; group_by(class) |&gt;\n  mutate(\n    gradem = mean(grade),\n    gradedev = grade - mean(grade)\n  )\n\nmod &lt;- lmer(self_concept ~ gradem + gradedev + \n              (1 + gradedev | class), \n            data = df)\n\n\n\n\n\n\n\n\n\n\nhangry.csv - hunger + anger = hanger#cross-sectional\n\n\n\n\n\nThis study is interested in evaluating whether peoples’ hunger levels are associated with their levels of irritability (i.e., “the hangry hypothesis”), and if this differs between people on a diet vs those who aren’t. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nData are available at https://uoepsy.github.io/data/hangry.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant Identifier\n\n\nfivetwo\nWhether the participant follows the five-two diet\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 28. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/hangry.csv\")\n\ndf &lt;- df |&gt; group_by(ppt) |&gt;\n  mutate(\n    hungerm = mean(q_hunger),\n    hungerdev = q_hunger - mean(q_hunger)\n  )\n\nmod &lt;- lmer(q_irritability ~ (hungerm + hungerdev) * fivetwo +\n              (1 + hungerdev | ppt), \n            data = df,\n            control=lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\n\n\n\n\n\npvt_bilingual.csv - Vocabulary development in monolingual and bilingual children#longitudinal#non-linear#more-complex-groupings\n\n\n\n\n\n488 children from 30 schools were included in the study. Children were assessed on a yearly basis for 7 years throughout primary school on a measure of vocabulary administered in English, the Picture Vocab Test (PVT). 295 were monolingual English speakers, and 193 were bilingual (english + another language).\nPrevious research conducted on monolingual children has suggested that that scores on the PVT increase steadily up until the age of approximately 7 or 8 at which point they begin to plateau. The aim of the present study is to investigate differences in the development of vocabulary between monolingual and bilingual children.\nData are available at https://uoepsy.github.io/data/pvt_bilingual.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nchild\nChild's name\n\n\nschool\nSchool Identifier\n\n\nisBilingual\nBinary variable indicating whether the child is monolingual (0) or bilingual (1)\n\n\nage\nAge (years)\n\n\nPVT\nScore on the Picture Vocab Test (PVT). Scores range 0 to 60\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 29. Let’s read in the data:\n\npvt &lt;- read_csv(\"https://uoepsy.github.io/data/pvt_bilingual.csv\")\n\nWe have 30 distinct schools:\n\nn_distinct(pvt$school)\n\n[1] 30\n\n\nAnd 418 distinct children. Is that right?\n\nn_distinct(pvt$child)\n\n[1] 418\n\n\nGiven that the pvt$child variable is just the first name of the child, it’s entirely likely that there will be, for instance more than one “Martin”.\nThis says that there are 487!\n\npvt |&gt; count(school, child) |&gt; nrow()\n\n[1] 487\n\n\nBut wait… we could still have issues. What if there were 2 “Martin”s at the same school??\n\npvt |&gt; \n  # count the school-children groups\n  count(school, child) |&gt; \n  # arrange the output so that the highest \n  # values of the 'n' column are at the top\n  arrange(desc(n))\n\n# A tibble: 487 × 3\n   school    child        n\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;\n 1 School 14 James       14\n 2 School 14 Michelle    14\n 3 School 15 Michael     14\n 4 School 21 Daniel      14\n 5 School 3  Jackson     14\n 6 School 5  Raven       14\n 7 School 9  Kaamil      14\n 8 School 1  Allyssa      7\n 9 School 1  Armon        7\n10 School 1  Dakota       7\n# ℹ 477 more rows\n\n\nAha! There are 7 cases where schools have two children of the same name. Remember that each child was measured at 7 timepoints. We shouldn’t have people with 14!\nIf we actually look at the data, we’ll see that it is very neatly organised, with each child’s data together. This means that we could feasibly make an educated guess that, e.g., the “Jackson” from “School 3” in rows 155-161 is different from the “Jackson” from “School 3” at rows 190-196.\nBecause of the ordering of our data, we can do something like this:\n\npvt &lt;- \n  pvt |&gt;\n  # group by the school and child\n  group_by(school, child) |&gt;\n  mutate(\n    # make a new variable which counts from 1 to \n    # the number of rows for each school-child\n    n_obs = 1:n()\n  ) |&gt;\n  # ungroup the data\n  ungroup() |&gt;\n  mutate(\n    # change it so that if the n_obs is &gt;7, the \n    # child becomes \"[name] 2\", to indicate they're the second\n    # child with that name\n    child = ifelse(n_obs&gt;7, paste0(child,\" 2\"), child)\n  )\n\nNow we have 494!\n\npvt |&gt; count(school, child) |&gt; nrow()\n\n[1] 494\n\n\nAnd nobody has anything other than 7 observations!\n\npvt |&gt; count(school, child) |&gt;\n  filter(n != 7)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: school &lt;chr&gt;, child &lt;chr&gt;, n &lt;int&gt;\n\n\nPhew!\nOkay, let’s just fit an intercept-only model:\n\npvt_null &lt;- lmer(PVT ~ 1 +  (1 | school/child), data = pvt)\nsummary(pvt_null)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: PVT ~ 1 + (1 | school/child)\n   Data: pvt\n\nREML criterion at convergence: 23844.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7768 -0.5984  0.0068  0.5744  4.1806 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n child:school (Intercept) 33.48    5.786   \n school       (Intercept) 19.76    4.445   \n Residual                 43.59    6.602   \nNumber of obs: 3458, groups:  child:school, 494; school, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  27.5263     0.8667   31.76\n\n\nAs we can see from summary(bnt_null), the random intercept variances are 33.48 for child-level, 19.76 for school-level, and the residual variance is 43.59.\nSo child level differences account for \\(\\frac{33.48}{33.48 + 19.76 + 43.59} = 0.35\\) of the variance in PVT scores, and child & school differences together account for \\(\\frac{33.48 + 19.76}{33.48 + 19.76 + 43.59} = 0.55\\) of the variance.\nHere’s an initial plot too:\n\nggplot(pvt, aes(x=age,y=PVT,col=factor(isBilingual)))+\n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")\n\n\n\n\n\n\n\n\nI feel like either raw or orthogonal polynomials would be fine here - there’s nothing explicit from the study background about stuff “at baseline”. There’s the stuff about the plateau at 7 or 8, but we can get that from the model plots. Orthogonal will allow us to compare the trajectories overall (their linear trend, the ‘curviness’ and ‘wiggliness’).\nAn additional benefit of orthogonal polynomials is that we are less likely to get singular fits when we include polynomial terms in our random effects. Remember, raw polynomials are correlated, so often the by-participant variances in raw poly terms are highly correlated.\nI’ve gone for 3 degrees of polynomials here because the plot above shows a bit of an S-shape for the bilinguals.\n\npvt &lt;- pvt |&gt; mutate(\n  poly1 = poly(age, 3)[,1],\n  poly2 = poly(age, 3)[,2],\n  poly3 = poly(age, 3)[,3],\n  isBilingual = factor(isBilingual)\n)\n\nThese models do not converge.\nI’ve tried to preserve the by-child random effects of time, because while I think Schools probably do vary, School’s all teach the same curriculum, whereas there’s a lot of varied things that can influence a child’s vocabulary, both in and out of school\n\nmod1 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + (poly1 + poly2 + poly3)*isBilingual | school) + \n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod2 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * (poly1 + poly2) + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod3 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod4 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod5 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# relative to the variance in time slopes, there's v little by-school variance in bilingual differences in vocab\n\nmod6 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly2 +  poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# looks like curvature doesn't vary between schools much as linear and wiggliness \n\nthis one does!\n\nmod7 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\n\nlibrary(broom.mixed)\naugment(mod7) |&gt; \n  mutate(\n    poly1 = round(poly1, 3) # because of rounding errors that make plot weird\n  ) |&gt;\n  ggplot(aes(x=poly1,col=isBilingual))+\n  stat_summary(geom=\"pointrange\",aes(y=PVT))+\n  stat_summary(geom=\"line\", aes(y=.fitted))\n\n\n\n\n\n\n\n\nrefitted with lmerTest:\n\nmod7 &lt;- lmerTest::lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nsummary(mod7)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + (1 + poly1 +  \n    poly3 | school) + (1 + (poly1 + poly2 + poly3) | school:child)\n   Data: pvt\n\nREML criterion at convergence: 23076.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.4457 -0.5499 -0.0004  0.5261  4.4194 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr             \n school:child (Intercept)   34.86   5.904                    \n              poly1       1842.60  42.926    0.01            \n              poly2       3945.86  62.816   -0.08  0.23      \n              poly3        684.38  26.161    0.19  0.61  0.87\n school       (Intercept)   19.97   4.468                    \n              poly1        928.32  30.468   -0.40            \n              poly3        335.59  18.319   -0.52 -0.12      \n Residual                   31.93   5.651                    \nNumber of obs: 3458, groups:  school:child, 494; school, 30\n\nFixed effects:\n                    Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)          27.9464     0.8987   31.1128  31.098  &lt; 2e-16 ***\npoly1               157.0421     9.5750   39.5964  16.401  &lt; 2e-16 ***\npoly2               -48.9606     8.0944  517.6541  -6.049 2.80e-09 ***\npoly3                -1.2793     8.1739   61.5750  -0.157  0.87614    \nisBilingual1         -1.2113     0.5863  465.7927  -2.066  0.03939 *  \npoly1:isBilingual1   16.6104    12.3121  501.9133   1.349  0.17791    \npoly2:isBilingual1   56.2609    12.9500  517.6541   4.344 1.68e-05 ***\npoly3:isBilingual1  -34.3286    11.8629 1217.8366  -2.894  0.00387 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) poly1  poly2  poly3  isBln1 pl1:B1 pl2:B1\npoly1       -0.215                                          \npoly2       -0.013  0.026                                   \npoly3       -0.184 -0.003  0.072                            \nisBilingul1 -0.250 -0.001  0.020 -0.020                     \nply1:sBlng1  0.000 -0.500 -0.020 -0.022 -0.001              \nply2:sBlng1  0.008 -0.016 -0.625 -0.045 -0.033  0.033       \nply3:sBlng1 -0.009 -0.020 -0.050 -0.566  0.034  0.038  0.079\n\n\n\n\n\n\n\n\n\n\nterm\nest\np\ninterpretation\n\n\n\n\n(Intercept)\n27.95\n*\naverage vocab score at the mean age (for monolingual)\n\n\npoly1\n157.04\n*\nvocab increases over time (for monolingual children)\n\n\npoly2\n-48.96\n*\nthe increase in vocab becomes more gradual (for monolingual children)\n\n\npoly3\n-1.28\n\nno significant wiggliness to vocab trajectory of the average monolingual child\n\n\nisBilingual1\n-1.21\n*\naverage vocab score at mean age is lower for bilingual vs monolingual children\n\n\npoly1:isBilingual1\n16.61\n\nno significant difference in linear trend of vocab for bilingual vs monolingual\n\n\npoly2:isBilingual1\n56.26\n*\ncurvature for vocab trajectory of bilingual children significantly differs from that of monolinguals\n\n\npoly3:isBilingual1\n-34.33\n*\nwiggliness for vocab trajectory of bilingual children significantly differs from that of monolinguals\n\n\n\n\n\n\n\nFrom the random effects, we get even more information!\nSchool’s vary in the average child vocab score at mean age with an SD of 4.5. More school level variation in linear trends than in curvature. Schools with higher vocab scores at the mean age tend to have lower linear increase, and also a more negative curvature.\nWithin schools, children vary in the vocab scores at mean age with an SD of 5.9. Lots of child-level variation in curvature and linear increases, slightly less variation in wiggliness.\n\nVarCorr(mod7)\n\n Groups       Name        Std.Dev. Corr                \n school:child (Intercept)  5.9043                      \n              poly1       42.9256   0.005              \n              poly2       62.8161  -0.078  0.227       \n              poly3       26.1606   0.191  0.609  0.871\n school       (Intercept)  4.4683                      \n              poly1       30.4683  -0.403              \n              poly3       18.3190  -0.524 -0.116       \n Residual                  5.6510                      \n\n\n\n\n\n\n\n\n\n\n\nAz.rda - trajectories of memory, simple, and complex daily functioning tasks in patients with Alzheimer’s#longitudinal#non-linear\n\n\n\n\n\nThese data are available at https://uoepsy.github.io/data/Az.rda. You can load the dataset using:\n\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n\nand you will find the Az object in your environment.\nThe Az object contains information on 30 Participants with probable Alzheimer’s Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100.\nWe’re interested in whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nSubject\nUnique Subject Identifier\n\n\nTime\nTime point of the study (1 to 10)\n\n\nTask\nTask type (Memory, cADL, sADL)\n\n\nPerformance\nScore on test (range 0 to 100)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 30. \n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nmidlifeape.csv - mid-life happiness slump in great apes#longitudinal#non-linear\n\n\n\n\n\nPrevious research has evidenced a notable dip in happiness for middle-aged humans. Interestingly, this phenomenon has even been observed in other primates, such as chimpanzees.\nThe present study is interested in examining whether the ‘middle-age slump’ happens to a similar extent for Orangutans as it does for Chimpanzees.\n200 apes (117 Chimps and 83 Orangutans) were included in the study. All apes were studied from early adulthood (10-12 years old for most great apes), and researchers administered the Happiness in Primates (HiP) scale to each participant every 3 years, up until the age of 40.\nData are available at https://uoepsy.github.io/data/midlife_ape.csv.\nThe dataset has already been cleaned, and the researchers have confirmed that it includes 117 Chimps and 83 Orangutans, and every ape has complete data (i.e. 10 rows for each ape).\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\napeID\nApe's Name (all names are chosen to be unique)\n\n\nage\nAge (in years) at assessment\n\n\nspecies\nSpecies (chimp v orangutan)\n\n\nHiP\nHappiness in Primate Scale (range 1 to 18)\n\n\ntimepoint\nStudy visit (1 to 10)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 31. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/midlife_ape.csv\")\n\n# add polynomials\ndf &lt;- df |&gt; \n  mutate(\n    poly1 = poly(timepoint,2,raw=F)[,1],\n    poly2 = poly(timepoint,2,raw=F)[,2]\n  )\n\nmod1 = lmer(HiP ~ 1 + (poly1 + poly2) * species +\n            (1 + poly1 + poly2 | apeID), \n            data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nmemorytap.csv - Memory Recall & Finger Tapping#repeated-measures#binomial-outcome\n\n\n\n\n\nResearchers are interested in investigating whether, after accounting for effects of sentence length, rhythmic tapping of fingers aids memory recall. They recruited 40 participants. Each participant was tasked with studying and then recalling 10 randomly generated sentences between 1 and 14 words long. For 5 of these sentences, participants were asked to tap their fingers along with speaking the sentence in both the study period and in the recall period. For the remaining 5 sentences, participants were asked to sit still.\nData are available at https://uoepsy.github.io/data/memorytap.csv, and contains information on the length (in words) of each sentence, the condition (static vs tapping) under which it was studied and recalled, and whether the participant was correct in recalling it.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier (n=40)\n\n\nslength\nNumber of words in sentence\n\n\ncondition\nCondition under which sentence is studied and recalled ('static' = sitting still, 'tap' = tapping fingers along to sentence)\n\n\ncorrect\nWhether or not the sentence was correctly recalled\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 32. \n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nnwl.RData - novel word learning#longitudinal#binomial-outcome\n\n\n\n\n\nData are available at\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\n\nIn the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test. Data were also collect from healthy controls.  Our broader research aim today is to compare the two lesion location groups (those with anterior vs. posterior lesions) with respect to their accuracy of responses over the course of the study.\n Figure 1 shows the differences between lesion location groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)\n\n\n\n\n\n\n\n\nFigure 1: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ngroup\nWhether participant is a stroke patient ('patient') or a healthy control ('control')\n\n\nlesion_location\nLocation of brain lesion: anterior vs posterior\n\n\nblock\nExperimental block (1-9). Blocks 1-7 were learning blocks, immediately followed by a test in block 8. Block 9 was a follow-up test at a later point\n\n\nPropCorrect\nProportion of 30 responses in a given block that the participant got correct\n\n\nNumCorrect\nNumber of responses (out of 30) in a given block that the participant got correct\n\n\nNumError\nNumber of responses (out of 30) in a given block that the participant got incorrect\n\n\nID\nParticipant Identifier\n\n\nPhase\nExperimental phase, corresponding to experimental block(s): 'Learning', 'Immediate','Follow-up'\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 33. \n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\ntestenhancedlearning.RData#longitudinal#more-complex-groupings#binomial-outcome\n\n\n\n\n\nAn experiment was run to conceptually replicate “test-enhanced learning” (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (StudyStudy), the other group studied the material once then did a test (StudyTest). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (Test_word).\nThe critical (replication) prediction is that the StudyStudy group perform better on the immediate test, but the StudyTest group will retain the material better and thus perform better on the 1-week follow-up test.\nWe have two options for how we measure “test performance”:\n\nThe time taken to correctly recall a given word.\n\nWhether or not a given word was correctly recalled\n\nThe following code loads the data into your R environment by creating a variable called tel:\n\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nSubject_ID\nUnique Participant Identifier\n\n\nGroup\nGroup denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest)\n\n\nDelay\nTime of recall test ('min' = Immediate, 'week' = One week later)\n\n\nTest_word\nWord being recalled (175 different test words)\n\n\nCorrect\nWhether or not the word was correctly recalled\n\n\nRtime\nTime to recall word (milliseconds)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 34. \n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nmsmr_trolley.csv - trolley problems#repeated-measures#binomial-outcome\n\n\n\n\n\nThe “Trolley Problem” is a thought experiment in moral philosophy that asks you to decide whether or not to pull a lever to divert a trolley. Pulling the lever changes the trolley direction from hitting 5 people to a track on which it will hit one person.\n\n\n\n\n\n\n\n\n\nPrevious research has found that the “framing” of the problem will influence the decisions people make:\n\n\n\n\n\n\n\n\npositive frame\nneutral frame\nnegative frame\n\n\n\n\n5 people will be saved if you pull the lever; one person on another track will be saved if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n5 people will be saved if you pull the lever, but another person will die. One people will be saved if you do not pull the lever, but 5 people will die. All your actions are legal and understandable. Will you pull the lever?\nOne person will die if you pull the lever. 5 people will die if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n\n\n\n\n\n\n\nWe conducted a study to investigate whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved).\n120 participants were recruited, and each gave answers to 12 versions of the thought experiment. For each participant, four versions followed each of the positive/neutral/negative framings described above, and for each framing, 2 would save 5 people and 2 would save 15 people.\nData are available at https://uoepsy.github.io/data/msmr_trolley.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nframe\nframing of the thought experiment (positive/neutral/negative\n\n\nlives\nlives at stake in the thought experiment (5 or 15)\n\n\nlever\nWhether or not the participant chose to pull the lever (1 = yes, 0 = no)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 35. \n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nmonkey social status and problem solving ability#repeated-measures#binomial-outcome\n\n\n\n\n\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a “dominant” or “subordinate” status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either “easy” or “difficult”, and the researchers recorded whether or not the monkey solved each problem.\nWe’re interested in how the social status of monkeys is associated with the ability to solve problems.\nData are available at https://uoepsy.github.io/data/msmr_monkeystatus.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nstatus\nSocial Status of monkey (adolescent, subordinate adult, or dominant adult)\n\n\ndifficulty\nProblem difficulty ('easy' vs 'difficult')\n\n\nmonkeyID\nMonkey Name\n\n\nsolved\nWhether or not the problem was successfully solved by the monkey\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 36. \n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\n\n# relevel difficulty\ndf$difficulty &lt;- factor(df$difficulty, \n                        levels=c(\"easy\",\"difficult\"))\n\nmod1 &lt;- glmer(solved ~ difficulty + status +\n                (1 + difficulty | monkeyID), \n              family=binomial,\n              data = df)\n\nsummary(mod1)"
  },
  {
    "objectID": "01_clustered.html",
    "href": "01_clustered.html",
    "title": "1: Group-Structured Data",
    "section": "",
    "text": "This reading:\n\nA refresher on the linear regression model\n\nAn introduction to group-structured (or ‘clustered’) data\nWorking with group-structured data (sample sizes, ICC, visualisations)",
    "crumbs": [
      "1: Group-Structured Data"
    ]
  },
  {
    "objectID": "01_clustered.html#clusters-clusters-everywhere",
    "href": "01_clustered.html#clusters-clusters-everywhere",
    "title": "1: Group-Structured Data",
    "section": "Clusters clusters everywhere",
    "text": "Clusters clusters everywhere\nThe idea of observing “children in schools” is just one such example of clustering that we might come across. This same hierarchical data structure can be found in other settings, such as patients within medical practices, employees within departments, people within towns etc. These sort of groups are higher level observations that we might sample (i.e. I randomly sample 20 schools, and then from each school randomly sample 30 children). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a Repeated Measures study we have individual experimental trials clustered within participants. Longitudinal studies exhibit the same data structure but have time-ordered observations clustered within people.\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters4. Table 1 shows just a few examples of different levels of clustering that may arise from different types of study.\n\n\n\n\n\nTable 1: Various different study designs will give rise to clustered data.\n\n\n\n\n\n\n\n\n\n\nCross Sectional\nRepeated Measures\nLongitudinal\n\n\n\n\nLevel n\n...\n...\n...\n\n\n...\n...\n...\n...\n\n\nLevel 3\nSchool\n...\nFamilies\n\n\nLevel 2\nClassroom\nParticipants\nPeople\n\n\nLevel 1 (Observations)\nChildren\nExperimental Stimuli\nTime\n\n\n\n\n\n\n\n\n\n\nThe common thread throughout all these designs is the hierarchy. At the lowest level of our hierarchy is the individual observed thing. For some designs, individual people might be the lowest observation level, for others, people might be the clusters (i.e. we have multiple data points per person).",
    "crumbs": [
      "1: Group-Structured Data"
    ]
  },
  {
    "objectID": "01_clustered.html#what-are-these-groupsclusters",
    "href": "01_clustered.html#what-are-these-groupsclusters",
    "title": "1: Group-Structured Data",
    "section": "What are these ‘groups’/‘clusters’?",
    "text": "What are these ‘groups’/‘clusters’?\nAt the fundamental level, we are using the term ‘cluster’ here to refer to a grouping of observations. In fact, we use the terms “clusters” and “groups” interchangeably in this context, so it’s worth taking a bit of time to try and understand the kind of groupings that we’re talking about (and how we think about them).\n\n\n“Clusters” are just “groups”.\n\nWhen we talk about clustered data, the groups we are discussing are typically those that can be thought of as a random sample of higher level units.\n\nOften the specific group-differences are not of interest.\n\n\nContrast the idea of ‘clusters’ with how we think about other sorts of groupings. In a study that looks at “how does heart rate differ between people taking placebo vs aspirin vs beta-blockers?” (Figure 7 LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we’ll use the same drugs (they’re not just a random sample of drugs - the x-axis of our LH plot in Figure 7 will be the same).\nIf we are interested in “what is the average grade at GCSE?”, and we have children grouped into different schools (Figure 7 RH plot), we are probably not interested in all the specific differences between grades in Broughton High School vs Gryffe High School etc. If we were to run our study again, we don’t collect data from the same set of schools. We can view these schools as ‘clusters’ - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).\n\n\n\n\n\n\n\n\nFigure 7: Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\n\n\n\n\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the type of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as “mixed effects modelling”), which is a regression modelling technique that allows us to explore questions such as these (and many more).5\n\n\n\n\n\n\noptional “univariate”and “multivariate”\n\n\n\n\n\nIn “univariate” statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.\nIn “multivariate” statistics, there are more sources of variation. For the “children in schools” example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to elicit different responses from one another.\n\n\n\n\n\n\n\n\n\noptional: “Panel data”\n\n\n\n\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as ‘panel data’. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:\n\n\n\n\n\n\n\n\nFigure 8: Panels of data\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Panels of panels of data",
    "crumbs": [
      "1: Group-Structured Data"
    ]
  },
  {
    "objectID": "01_clustered.html#determining-sample-sizes",
    "href": "01_clustered.html#determining-sample-sizes",
    "title": "1: Group-Structured Data",
    "section": "Determining Sample Sizes",
    "text": "Determining Sample Sizes\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different levels.\nIn the description of the SchoolMot data above we are told the relevant numbers:\n\n\n\n\n\n\n\n\n\nUnit\nSample Size\n\n\n\n\nLevel 2\nSchool\n30\n\n\nLevel 1 (Observations)\nChildren\n900\n\n\n\n\n\n\n\nWe can check this in our data:\n\nschoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n\n[1] 900\n\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n\n[1] 30\n\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from Calderglen High School and only 10 from Broughton High School, or do we have the same number in each?\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the summary() function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):\n\nschoolmot |&gt;\n  count(schoolid) |&gt;\n  summary()\n\n   schoolid               n     \n Length:30          Min.   :30  \n Class :character   1st Qu.:30  \n Mode  :character   Median :30  \n                    Mean   :30  \n                    3rd Qu.:30  \n                    Max.   :30",
    "crumbs": [
      "1: Group-Structured Data"
    ]
  },
  {
    "objectID": "01_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "href": "01_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "title": "1: Group-Structured Data",
    "section": "ICC - Quantifying clustering in an outcome variable",
    "text": "ICC - Quantifying clustering in an outcome variable\nThe IntraClass Correlation Coefficient (ICC) is a measure of how much variation in a variable is attributable to the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol \\(\\rho\\):7\n\\[\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n\\]\nThis is illustrated in the Figure 10 below, in which our continuous outcome variable (children’s grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the “between-group variance” as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the “within-group variance” as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\n\nCode\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n\n\n\n\n\n\n\n\nFigure 10: Variance in grades between schools. Data from https://uoepsy.github.io/data/schoolmot.csv\n\n\n\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can extract it from a fitted model.\nIn the school motivation data (visualised above), it’s estimated that 22% of the variance in grades is due to school-related differences:\n\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n\n[1] 0.2191859\n\n\n\n\n\n\n\n\noptional: calculating ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC:\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a couple of new columns. The first is the overall mean of \\(y\\), and the second is the mean of \\(y\\) for each group. Note that we calculate this by first using group_by to make the subsequent operation (the mutate) be applied to each group. To ensure that the grouping does not persist after this, we’ve passed it to ungroup at the end.\n\nschoolmot &lt;- \n  schoolmot |&gt; \n  mutate(\n    grand_mean = mean(grade)\n  ) |&gt;\n  group_by(schoolid) |&gt;\n  mutate(\n    group_mean = mean(grade)\n  ) |&gt;\n  ungroup()\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nschoolmot &lt;- schoolmot |&gt; \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n\nFinally, we divide them by the degrees of freedom. Our degrees of freedom for our between group variance \\(30 \\text{ groups} - 1 \\text{ grand mean}=29\\)\nOur degrees of freedom for our within group variance is \\(900 \\text{ observations} - 30 \\text{ groups}=870\\)\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n\nAnd calculate the ICC!!!\nThe 29 here is the \\(k-1\\) in the formula above, where \\(k\\) is the number of observations within each group.\n\n# ICC\n(msb-mse) /(msb + (29*mse))\n\n[1] 0.2191859\n\n\n\n\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\nWe can think of it as the average correlation between all possible pairs of observations from the same group. Suppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (Figure 11). The ICC is the correlation between such pairs.\n\n\n\n\n\n\n\n\nFigure 11: ICC is the correlation of randomly drawn pairs from the same group\n\n\n\n\n\n\n\n\n\n\n\noptional: a little simulation\n\n\n\n\n\nWe can actually do the “randomly drawn pair of observations from the same group” via simulation.\nThe code below creates a function for us to use. Can you figure out how it works?\n\nget_random_pair &lt;- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n\nTry it out, by running it several times.\n\nget_random_pair()\n\n[1] 28.35 50.84\n\n\nNow let’s make our computer do it loads and loads of times:\n\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims &lt;- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims &lt;- t(sims)\ncor(sims[,1], sims[,2])\n\n[1] 0.2097805\n\n\n\n\n\n\n\n\n\n\n\noptional: correlations from group-structured data\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n# A tibble: 7 × 3\n  cluster observation y    \n* &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 group_1 1           4    \n2 group_1 2           2    \n3 group_2 1           4    \n4 group_2 2           2    \n5 group_3 1           7    \n6 group_3 2           5    \n7 ...     ...         ...  \n\n\nThe ICC for this data is 0.18.\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, they’re just random draws from a group.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen any of these:\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\nWhy ICC?\nThe ICC tells us the proportion of the total variability in an outcome variable that is attributable to the differences between groups/clusters. It ranges from 0 to 1.\nThis helps us to assess the appropriateness of using a multilevel approach. If the ICC is high, it suggests that a large amount of the variance is at the cluster level (justifying the use of multilevel modeling to account for this structure).\nThere are no cut-offs - the interpretation of ICC values is inherently field-specific, as what constitutes a high or low ICC depends on the nature of the outcome variable, and the hierarchical structure within a particular research context.",
    "crumbs": [
      "1: Group-Structured Data"
    ]
  },
  {
    "objectID": "01_clustered.html#visualisations",
    "href": "01_clustered.html#visualisations",
    "title": "1: Group-Structured Data",
    "section": "Visualisations",
    "text": "Visualisations\nWhen we’re visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. For instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens within a given school (see e.g. Figure 5 from earlier).\nSome useful ggplot tools here are:\n\nfacet_wrap() - make a separate little plot for each level of a grouping variable\nthe group aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n\n\nfacets\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point() +\n  facet_wrap(~schoolid)\n\n\n\n\n\n\n\n\n\n\ngroup\n\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)",
    "crumbs": [
      "1: Group-Structured Data"
    ]
  },
  {
    "objectID": "01_clustered.html#footnotes",
    "href": "01_clustered.html#footnotes",
    "title": "1: Group-Structured Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nWith the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we can actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).↩︎\nor “mean squares residual”↩︎\nIt’s “turtles all the way down”↩︎\nWhile multilevel models are great for multilevel questions, sometimes we may be interested in only things that occur at “level 1” (the lowest observation level). While not the focus of this course, there are also many other methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply “account for the nuisance clustering”.↩︎\nNote, this is not true for a set of analytical methods called “cluster analysis”, which attempts to identify clusters that haven’t been measured/observed (or may not even ‘exist’ in any real sense of the word).↩︎\nalthough this symbol get used for lots of other correlation-y things too!↩︎\nanother way to think of this is that we could figure out the the estimated grade ~ funding difference exactly from the set of grade ~ schoolid coefficients.↩︎",
    "crumbs": [
      "1: Group-Structured Data"
    ]
  },
  {
    "objectID": "09_assump.html",
    "href": "09_assump.html",
    "title": "9: MLM Assumptions",
    "section": "",
    "text": "This reading:\n\nMultilevel model assumptions\n\nRandom effects can be thought of as another level of residual\n\nCase diagnostics in multilevel models\n\ninfluential observations and influential groups\n\n\nOptional Extra: Group confounding and the random effects assumption",
    "crumbs": [
      "9: MLM Assumptions"
    ]
  },
  {
    "objectID": "09_assump.html#level-1-residuals",
    "href": "09_assump.html#level-1-residuals",
    "title": "9: MLM Assumptions",
    "section": "Level 1 residuals",
    "text": "Level 1 residuals\nWe can get the level 1 (observation-level) residuals the same way we used to do for lm() - by just using resid() or residuals(). Additionally, there are a few useful techniques for plotting these which we have listed below:\n\n\nresid vs fitted\nWe can plot the residuals vs fitted model (just like we used to for lm()), and assess the extend to which the assumption holds that the residuals are zero mean. (we want the blue smoothed line to be fairly close to zero across the plot)\n\n# \"p\" below is for points and \"smooth\" for the smoothed line\nplot(jsmod, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresiduals vs fitted plots will trend upwards\n\n\n\n\n\nAs a result of partial pooling, the residuals vs fitted plots will often tend to go upwards.\nIt’s quite confusing to get your head around, but it is because the groups that are more outlying (which tend to have the lowest and highest fitted values, so they’re on the left and right of the resid vs fitted plot) are the ones that get ‘shrunk’ more towards the center. So the fitted values will get pull towards the middle of the plot, but the residuals will get bigger (more negative for groups on below average, and more positive for groups above average). The result is that the entire plot tilts!\nYou can see an animation of this in Figure 2. The left hand plot shows the model fitted values for each group, with the residuals indicated by dashed lines. The right hand plots shows the residuals vs fitted plot. The animation moves between the ‘no pooling’ and ‘partial pooling’ approach, and as it does so, the plot on the right tilts!\n\n\n\n\n\n\n\n\nFigure 2: The effect of partial pooling on residuals vs fitted plots. The extent to which this tilting happens depends on the amount of shrinkage, so will be more evident when there are few observations in the groups\n\n\n\n\n\n\n\n\n\n\nscale-location\nAgain, like we can for lm(), we can also look at a scale-location plot. This is where the square-root of the absolute value of the residuals is plotted against the fitted values, and allows us to more easily assess the assumption of constant variance.\n(we want the blue smoothed line to be close to horizontal across the plot)\n\nplot(jsmod,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\nfacetted plots\nWe can also plot these “resid v fitted” and “scale-location” plots for each cluster, to check that our residual mean and variance is not related to the clusters:\n\nplot(jsmod,\n         form = resid(.) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\nplot(jsmod,\n         form = sqrt(abs(resid(.))) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\n\nresidual normality\nWe can also examine the normality the level 1 residuals, using things such as histograms and QQplots:\n(we want the datapoints to follow close to the diagonal line)\n\nqqnorm(resid(jsmod)); qqline(resid(jsmod))\n\n\n\n\n\n\n\n\n\nhist(resid(jsmod))",
    "crumbs": [
      "9: MLM Assumptions"
    ]
  },
  {
    "objectID": "09_assump.html#level-2-residuals",
    "href": "09_assump.html#level-2-residuals",
    "title": "9: MLM Assumptions",
    "section": "Level 2+ residuals",
    "text": "Level 2+ residuals\nThe second level of residuals in the multilevel model are actually just our random effects! We’ve seen them already whenever we use ranef()!\nTo get out these we often need to do a bit of indexing. ranef(model) will give us a list with an item for each grouping. In each item we have a set of columns, one for each thing which is varying by that grouping.\nBelow, we see that ranef(jsmod) gives us something with one entry, $dept, which contains 2 columns (the random intercepts and random slopes of payscale):\n\nranef(jsmod)\n\n$dept\n                                        (Intercept)    payscale\nAccounting                              -0.03045458 -0.19259376\nArchitecture and Landscape Architecture  0.29419381 -0.35855884\nArt                                     -0.29094345  0.15293285\nBusiness Studies                        -0.27858102  0.18008149\n...                                      ...         ... \nSo we can extract the random intercepts using ranef(jsmod)$dept[,1].\nAgain, we want normality of the random effects, so we can make more histograms or qqplots, for both the random intercepts and the random slopes:\ne.g., for the random intercepts:\n\nqqnorm(ranef(jsmod)$dept[,1]);qqline(ranef(jsmod)$dept[,1])\n\n\n\n\n\n\n\n\nand for the random slopes:\n\nqqnorm(ranef(jsmod)$dept[,2]);qqline(ranef(jsmod)$dept[,2])",
    "crumbs": [
      "9: MLM Assumptions"
    ]
  },
  {
    "objectID": "09_assump.html#model-simulations",
    "href": "09_assump.html#model-simulations",
    "title": "9: MLM Assumptions",
    "section": "model simulations",
    "text": "model simulations\nSometimes, a good global assessment of your model comes from how good a representation of the observed data it is. We can look at this in a cool way by simulating from our model a new set of values for the outcome. If we do this a few times over, and plot each ‘draw’ (i.e. set of simulated values), we can look at how well it maps to the observed set of values:\nOne quick way to do this is with the check_predictions() function from the performance package:\n\nlibrary(performance)\ncheck_predictions(jsmod, iterations = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptional: doing it yourself\n\n\n\n\n\nDoing this ourselves gives us a lot more scope to query differences between our observed vs model-predicted data.\nThe simulate() function will simulate response variable values for us.\nThe re.form = NULL bit is saying to include the random effects when making simulations (i.e. use the information about the specific clusters we have in our data). If we said re.form = NA it would base simulations on a randomly generated set of clusters with the associated intercept and slope variances estimated by our model.\n\nmodsim &lt;- simulate(jsmod, nsim = 200, re.form=NULL)\n\nTo get this plotted, we’ll have to do a bit of reworking, because it gives us a separate column for each draw. So if we pivot them longer we can make a density plot for each draw, and then add on top of that our observed scores:\n\n# take the simulations\nmodsim |&gt; \n  # pivot \"everything()\" (useful function to capture all columns),\n  # put column names into \"sim\", and the values into \"value\"\n  pivot_longer(everything(), names_to=\"sim\",values_to=\"value\") |&gt;\n  # plot them! \n  ggplot(aes(x=value))+\n  # plot a different line for each sim. \n  # to make the alpha transparency work, i need to use\n  # geom_line(stat=\"density\") rather than \n  # geom_density() (for some reason alpha applies to fill here)\n  geom_line(aes(group=sim), stat=\"density\", alpha=.1,\n            col=\"darkorange\") +\n  # finally, add the observed scores!  \n  geom_density(data = jsuni, aes(x=jobsat), lwd=1)\n\n\n\n\n\n\n\n\nHowever, we can also go further! We can pick a statistic, let’s use the IQR, and see how different our observed IQR is from the IQRs of a series of simulated draws.\nHere are 1000 simulations. This time I don’t care about simulating for these specific clusters, I just want to compare to random draws of clusters:\n\nsims &lt;- simulate(jsmod, nsim=1000, re.form=NA)\n\nThe apply() function (see also lapply, sapply ,vapply, tapply) is a really nice way to take an object, and apply a function to it. The number 2 here is to say “do it on each column”. If we had 1 it would be saying “do it on each row”.\nThis gives us the IQR of each simulation:\n\nsimsIQR &lt;- apply(sims, 2, IQR)\n\nWe can then ask what proportion of our simulated draws have an IQR smaller than our observed IQR? If the answer is very big or very small it indicates our model does not very represent this part of reality very well.\n\nmean(IQR(jsuni$jobsat)&gt;simsIQR)\n\n[1] 0.451",
    "crumbs": [
      "9: MLM Assumptions"
    ]
  },
  {
    "objectID": "09_assump.html#footnotes",
    "href": "09_assump.html#footnotes",
    "title": "9: MLM Assumptions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“endogeneity” is a term used to refer to variables that are influenced by other variables in a system. In the multilevel model, the predictors are assumed to be ‘exogenous’ (i.e. not influenced by anything)↩︎",
    "crumbs": [
      "9: MLM Assumptions"
    ]
  },
  {
    "objectID": "02_lmm.html",
    "href": "02_lmm.html",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "",
    "text": "This reading:\n\nIntroducing the multilevel (or “mixed effects”) regression model\nPartially pooling information across groups\nFitting multilevel models in R\nModel estimation and convergence\n\n\n\n\n\n\n\ndifferent names for the same thing\n\n\n\n\n\nThe methods we’re going to start to look at are known by lots of different names (see Figure 1). The core idea is that model parameters vary at more than one level..\n\n\n\n\n\n\n\n\nFigure 1: size weighted by hits on google scholar search (sept 2020)",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "02_lmm.html#random-intercepts",
    "href": "02_lmm.html#random-intercepts",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "random intercepts",
    "text": "random intercepts\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.1 Then, we can take a coefficient \\(b_?\\) and allow it to be different for each cluster \\(i\\) by adding the suffix \\(b_{?i}\\). Below, we have done this for our intercept \\(b_0\\), which has become \\(b_{0i}\\).\nHowever, we also need to define these differences in some way, and the multilevel model does this by expressing each cluster’s intercept as a deviation (\\(\\zeta_{0i}\\) for cluster \\(i\\), below) from a fixed number (\\(\\gamma_{00}\\), below). Because these differences are to do with the clusters (and not the individual observations within them), we often write these as a “level 2 equation”:\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nmixed-effects notation\n\n\n\n\n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:\n\\[\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n\\]\nThis notation typically corresponds with the “mixed effects” terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:\n\\[\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n\\]\n\n\n\nReturning to our school children’s grade example, we can fit a model with “random intercepts for schools”, which would account for some schools having higher grades, some having lower grades, etc.\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n\\] If we consider one of our schools (e.g. “Beeslack Community High School”) we can see that our model predicts that this school has higher grades than most other schools (Figure 3). We can see how this is modelled as a deviation \\(\\zeta_{0\\text{B}}\\) (B for Beeslack) from some fixed value \\(\\gamma_{00}\\).\n\n\n\n\n\n\n\n\nFigure 3: Fitted values from a multilevel model with random intercepts for schools\n\n\n\n\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of lm(grade ~ motiv + schoolid)), which would also estimate a difference for each cluster?\n\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we can get these out). We are estimating a distribution of deviations.\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the variance of a normal distribution of clusters.\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n\n\n\na fixed intercept \\(\\gamma_{00}\\)\n\nthe variance with which schools deviate from the fixed intercept \\(\\sigma^2_0\\)\n\na fixed slope for motiv \\(b_1\\)\n\nand we also need the residual variance too \\(\\sigma^2_\\varepsilon\\)\n\n\n\n\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nRemember, \\(\\sim N(m,s)\\) is a way of writing “are normally distributed with a mean of \\(m\\) and a standard deviation of \\(s\\)”. So the \\(\\zeta_{0i} \\sim N(0,\\sigma_0)\\) bit is saying that the school deviations from the fixed intercept are modelled as a normal distribution, with a mean of 0, and a standard deviation of \\(\\sigma_0\\) (which gets estimated by our model).\nThis can be seen in Figure 4 - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.\n\n\n\n\n\n\n\n\nFigure 4: grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance of school-level deviations).",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "02_lmm.html#random-slopes",
    "href": "02_lmm.html#random-slopes",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "random slopes",
    "text": "random slopes\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). For instance, we can allow the slope of \\(x\\) on \\(y\\) to be different for each cluster, by specifying in our model that \\(b_{1i}\\) is a distribution of cluster deviations \\(\\zeta_{1i}\\) around the fixed slope \\(\\gamma_{10}\\).\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\nWhen we have random intercepts and random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying “random intercepts and slopes are normally distributed with mean of 0 and standard deviations of \\(\\sigma_0\\) and \\(\\sigma_1\\) respectively, and with a correlation of \\(\\rho_{01}\\)”. We’ll see more on this in future weeks, so don’t worry too much right now.\nIn Figure 5, we can see now that both the intercept and the slope of grades across motivation are varying by-school.\n\n\n\n\n\n\n\n\nFigure 5: predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\n\n\n\n\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations \\(\\zeta_{1i}\\) around a fixed estimate \\(\\gamma_{10}\\).\nSo each group (school) now has, as visualised in Figure 6:\n\na deviation from the fixed intercept\na deviation from the fixed slope\n\n\n\n\n\n\n\n\n\nFigure 6: random intercepts and random slopes\n\n\n\n\n\nWhile it’s possible to show the distribution of intercepts on the left hand side of our grade ~ motiv plot, it’s hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in Figure 7. We can see, for instance, that “Hutcheson’s Grammar School” has a higher intercept, but a lower slope.\n\n\n\n\n\n\n\n\nFigure 7: grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance of school-level deviations)\n\n\n\n\n\n\n\n\n\n\n\noptional: joint distribution of intercept and slopes\n\n\n\n\n\nWhen we have random intercepts and slopes in our model, we don’t just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. This comes back to the part of the equation we mentioned briefly above, where we used:\n\n\\(\\sigma_0\\) to represent the standard deviation of intercept deviations\n\\(\\sigma_1\\) to represent the standard deviation of slope deviations\n\\(\\rho_{01}\\) to represent the correlation between intercept deviations and slope deviations\n\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\] For a visual intuition about this, see Figure 8, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).\n\n\n\n\n\n\n\n\nFigure 8: Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "02_lmm.html#extracting-model-parameters",
    "href": "02_lmm.html#extracting-model-parameters",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Extracting model parameters",
    "text": "Extracting model parameters\nAlongside summary(), there are some useful functions in R that allow us to extract the parameters estimated by the model:\n\nfixed effects\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\nfixef(smod3)\n\n       (Intercept)              motiv       fundingstate motiv:fundingstate \n         40.314262           2.629406         -17.253102           2.848504 \n\n\n\n\nrandom effect variances\nThe random effect variances (sometimes referred to as the “variance components”) represent the estimated spread with which clusters vary around the fixed effects.\n\nVarCorr(smod3)\n\n Groups   Name        Std.Dev. Corr  \n schoolid (Intercept) 10.2530        \n          motiv        1.6108  -0.481\n Residual             11.7911",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "02_lmm.html#making-model-predictions",
    "href": "02_lmm.html#making-model-predictions",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Making model predictions",
    "text": "Making model predictions\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models\n\nrandom effects\nOften referred to as the “random effects”, the deviations for each cluster from the fixed effects can be obtained using ranef().\nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that “Anderson High School” has an estimated intercept that is 9.39 higher than average, and an estimate slope of motivation that is 0.61 lower than average.\n\nranef(smod3)\n\n$schoolid\n                                       (Intercept)       motiv\nAnderson High School                    9.386006   -0.60628851\nArdnamurchan High School               -3.413536    0.15035243\nBalwearie High School                  -13.599480   1.12438549\nBeeslack Community High School          9.658259   -0.14963519\n...                                     ...         ...\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.\n\ndotplot.ranef.mer(ranef(smod3))\n\n$schoolid\n\n\n\n\n\n\n\n\n\n\n\ncluster coefficients\nRather than looking at deviations from fixed effects, we can calculate the intercept and slopes for each cluster.\nWe can get these out using coef()\n\ncoef(smod3)\n\n$schoolid\n                               (Intercept) motiv       fundingstate  motiv:fundingstate\nAnderson High School           49.70028    2.02311498  -17.25313     2.848511\nArdnamurchan High School       36.90073    2.77975592  -17.25313     2.848511\nBalwearie High School          26.71479    3.75378898  -17.25313     2.848511\nBeeslack Community High School 49.97253    2.47976830  -17.25313     2.848511\n...                            ...         ...         ...           ...\n\n\nfixef() + ranef() = coef()\n\n\nFor example, if we are estimating that “Anderson High School” has an intercept that is 9.39 higher than average, and the average (our fixed effect intercept) is 40.31, then we know that this has an intercept of 40.31 + 9.39 = 49.70.\nHowever, note that we also have slopes for things that aren’t present in the output of ranef() - we have the slopes for the fixed predictors for each cluster too, but because these are fixed they are simply the same number of each cluster.\nBecause “Anderson High School” is a state-funded school, the intercept is not just “9.39 above our fixed effect intercept” - it is estimated to be 17.25 lower than that because it is state-funded. So our estimated intercept for this school is 40.31 + 9.39 - 17.25 = 32.45.\nIn fact, we can essentially write out a regression equation for each individual cluster. For instance, “Anderson High School” is a state-funded school, so the expected grade for a child \\(i\\) from this school would be: \\[\n\\begin{align}\n\\widehat{grade_i} &= 49.70 + (2.02 \\cdot motiv_i) - (17.25 \\cdot 1) + (2.85 \\cdot motiv_i \\cdot 1) \\\\\n&= 32.45 + (4.87 \\cdot motiv_i) \\\\\n\\end{align}\n\\] And for Beeslack Community High School, which is a private funded school, the expected grade would be: \\[\n\\begin{align}\n\\widehat{grade_i} &= 49.97 + (2.48 \\cdot motiv_i) - (17.25 \\cdot 0) + (2.85 \\cdot motiv_i \\cdot 0) \\\\\n&= 49.97 + (2.48 \\cdot motiv_i) \\\\\n\\end{align}\n\\]",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "02_lmm.html#visualising-models",
    "href": "02_lmm.html#visualising-models",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Visualising models",
    "text": "Visualising models\nIn the vast majority of analyses using multilevel models, the bit that people are most interested in is the fixed effects - these represent the estimated effects averaged across the clusters in our sample.\nIt’s tempting, then, to think that we could take the model-predicted values for each observation (predict()/fitted()/augment() etc.) and average them up to get our estimated effects. If every cluster had the same amount of data at every level of the predictor then this would work but we would have more issues in estimating the uncertainty.\nInstead, it is common to use helpful packages that can provide us with a small dataframe of estimated effects and uncertainty, that we can then plot. One such package is the effects package, which has the effect() function, that can be used as so:\n\nlibrary(effects)\neffect(term = \"motiv*funding\", mod = smod3) |&gt;\n  as.data.frame()\n\n   motiv funding      fit       se    lower    upper\n1    0.4 private 41.36602 4.376137 32.77735 49.95470\n2    3.0 private 48.20248 3.072389 42.17256 54.23240\n3    5.0 private 53.46129 2.975875 47.62079 59.30179\n4    7.0 private 58.72010 3.776260 51.30876 66.13145\n5    9.0 private 63.97891 5.064830 54.03860 73.91922\n6    0.4   state 25.25232 3.186986 18.99750 31.50715\n7    3.0   state 39.49489 2.314470 34.95248 44.03730\n8    5.0   state 50.45071 2.257971 46.01918 54.88224\n9    7.0   state 61.40653 2.797539 55.91604 66.89702\n10   9.0   state 72.36235 3.679694 65.14053 79.58417\n\n\nWe can then use this to plot:\n\neffect(term = \"motiv*funding\", mod = smod3) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x = motiv, col = funding, fill = funding)) +\n  geom_line(aes(y = fit)) + \n  geom_ribbon(aes(y = fit, ymin = lower, ymax = upper), alpha = .3)\n\n\n\n\n\n\n\n\nHowever, as with plotting all estimated effects, it is very useful to also display the underlying data, to make clear the variability we would actually expect. The plot above shows us what we would expect for children from the schools on average. It doesn’t clearly depict how schools and children vary, and a reader could easily take away the idea that we expect most children to fall within those bands.\nOne easy way is to adjust plot is to simply show the raw data behind our estimated effects, which requires giving the ggplot() code data from two different places:\n\nefplot &lt;- effect(term = \"motiv*funding\", mod = smod3) |&gt;\n  as.data.frame()\n\nggplot(data = schoolmot, \n       aes(x = motiv, col = funding, fill = funding)) +\n  geom_point(aes(y = grade), alpha = .3) + \n  geom_line(data = efplot, aes(y = fit)) + \n  geom_ribbon(data = efplot, \n              aes(y = fit, ymin = lower, ymax = upper), alpha = .3)",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "02_lmm.html#convergence-warnings-singular-fits",
    "href": "02_lmm.html#convergence-warnings-singular-fits",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "convergence warnings & singular fits",
    "text": "convergence warnings & singular fits\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different ‘optimisers’.\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nTechnical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn’t possible.\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:\n\nA typical convergence warning:\n\n\nwarning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)\n\nA singular fit:\n\n\nboundary (singular) fit: see ?isSingular\n\n\n\nDo not trust the results of a model that does not converge\n\nThere are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them), but for the time being, if lmer() gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "02_lmm.html#footnotes",
    "href": "02_lmm.html#footnotes",
    "title": "2: The Multi-level/Mixed Effect Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome books use “cluster \\(j\\) &gt;&gt; observation \\(i\\)”, others use “cluster \\(i\\) &gt;&gt; observation \\(j\\)”. We use the latter here↩︎\nthis exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added↩︎\nremember, variance = standard deviation squared↩︎\nit’s a bit like n-1 being in the denominator of the formula for standard deviation↩︎",
    "crumbs": [
      "2: The Multi-level/Mixed Effect Model"
    ]
  },
  {
    "objectID": "00_lm_assumpt.html",
    "href": "00_lm_assumpt.html",
    "title": "LM Troubleshooting",
    "section": "",
    "text": "In the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below.",
    "crumbs": [
      "Additional Docs",
      "LM Troubleshooting"
    ]
  },
  {
    "objectID": "00_lm_assumpt.html#tests-of-the-coefficients",
    "href": "00_lm_assumpt.html#tests-of-the-coefficients",
    "title": "LM Troubleshooting",
    "section": "Tests of the coefficients",
    "text": "Tests of the coefficients\n\nlibrary(lmtest)\nlibrary(sandwich)\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC0\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.0383561  0.8635215 -0.0444  0.96466  \nx            0.4924743  0.2631998  1.8711  0.06438 .\nx2b          1.2305743  0.7625359  1.6138  0.10985  \nx2c         -0.0010129  0.9210642 -0.0011  0.99912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Additional Docs",
      "LM Troubleshooting"
    ]
  },
  {
    "objectID": "00_lm_assumpt.html#model-comparisons",
    "href": "00_lm_assumpt.html#model-comparisons",
    "title": "LM Troubleshooting",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nmod_res &lt;- lm(y ~ 1 + x, data = troubledf2)\nmod_unres &lt;- lm(y ~ 1 + x + x2, data = troubledf2)\nwaldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = \"HC0\"))\n\nWald test\n\nModel 1: y ~ 1 + x\nModel 2: y ~ 1 + x + x2\n  Res.Df Df      F Pr(&gt;F)\n1     98                 \n2     96  2 1.8704 0.1596",
    "crumbs": [
      "Additional Docs",
      "LM Troubleshooting"
    ]
  },
  {
    "objectID": "00_lm_assumpt.html#boostrapped-coefficients",
    "href": "00_lm_assumpt.html#boostrapped-coefficients",
    "title": "LM Troubleshooting",
    "section": "Boostrapped Coefficients",
    "text": "Boostrapped Coefficients\nWe can get out some bootstrapped confidence intervals for our coefficients using the car package:\n\nlibrary(car)\n# bootstrap our model coefficients\nboot_mod &lt;- Boot(mod)\n# compute confidence intervals\nConfint(boot_mod)\n\nBootstrap bca confidence intervals\n\n              Estimate        2.5 %    97.5 %\n(Intercept)  1.5156272  0.269082523 3.0150279\nx            0.3769504  0.005839124 0.7201455\nx2b          0.2497345 -0.718176725 1.3009887\nx2c         -0.1305828 -1.015342466 0.6681926\nx2d          1.1534433  0.031319608 2.4027965",
    "crumbs": [
      "Additional Docs",
      "LM Troubleshooting"
    ]
  },
  {
    "objectID": "00_lm_assumpt.html#bootstrapped-anova",
    "href": "00_lm_assumpt.html#bootstrapped-anova",
    "title": "LM Troubleshooting",
    "section": "Bootstrapped ANOVA",
    "text": "Bootstrapped ANOVA\nIf we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the incremental addition of each predictor, we can get bootstrapped p-values from the ANOVA.boot function in the lmboot package.\nOur original ANOVA:\n\nanova( lm(y~x+x2, data = df) )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nx          1  20.64 20.6427  5.4098 0.02215 *\nx2         3  25.60  8.5331  2.2363 0.08902 .\nResiduals 95 362.50  3.8158                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd our bootstrapped p-values:\n\nlibrary(lmboot)\nmy_anova &lt;- ANOVA.boot(y~x+x2, data = df, \n                       B = 1000)\n# these are our bootstrapped p-values:\nmy_anova$`p-values`\n\n[1] 0.023 0.100\n\n#let's put them alongside our original ANOVA table:\ncbind(\n  anova( lm(y~x+x2, data = df) ),\n  p_bootstrap = c(my_anova$`p-values`,NA)\n)\n\n          Df    Sum Sq   Mean Sq  F value     Pr(&gt;F) p_bootstrap\nx          1  20.64273 20.642727 5.409835 0.02215056       0.023\nx2         3  25.59936  8.533122 2.236273 0.08902175       0.100\nResiduals 95 362.49886  3.815777       NA         NA          NA",
    "crumbs": [
      "Additional Docs",
      "LM Troubleshooting"
    ]
  },
  {
    "objectID": "00_lm_assumpt.html#other-things",
    "href": "00_lm_assumpt.html#other-things",
    "title": "LM Troubleshooting",
    "section": "Other things",
    "text": "Other things\nWe can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.\nFor example, to bootstrap the \\(R^2\\) for the model lm(y~x+x2), we would create a little function called rsq:\n\nrsq &lt;- function(data, indices){\n  # this is the bootstrap resample\n  bdata &lt;- data[indices,]\n  # this is the model, fitted to the resample\n  fit &lt;- lm(y ~ x + x2, data = bdata)\n  # this returns the R squared\n  return(summary(fit)$r.square)\n}\n\nWe then use the boot package, giving 1) our original data and 2) our custom function to the boot() function, and compute some confidence intervals:\n\nlibrary(boot)\nbootrsq_results &lt;- boot(data = df, statistic = rsq, R = 1000)\nboot.ci(bootrsq_results, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootrsq_results, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.0174,  0.2196 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable",
    "crumbs": [
      "Additional Docs",
      "LM Troubleshooting"
    ]
  },
  {
    "objectID": "00_lm_assumpt.html#footnotes",
    "href": "00_lm_assumpt.html#footnotes",
    "title": "LM Troubleshooting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nThis method finds an appropriate value for \\(\\lambda\\) such that the transformation \\((sign(x) |x|^{\\lambda}-1)/\\lambda\\) results in a close to normal distribution.↩︎\nThis is a special formulation of something called a ‘Sandwich’ estimator!↩︎\nor \\(sign( rank(|y|) )\\)↩︎",
    "crumbs": [
      "Additional Docs",
      "LM Troubleshooting"
    ]
  },
  {
    "objectID": "05_long.html",
    "href": "05_long.html",
    "title": "5: Example: Longitudinal MLM",
    "section": "",
    "text": "This reading:\n\nWalkthrough Example: “change over time” - fitting multilevel models to longitudinal data.\n\nThe application of multilevel models to longitudinal data is simply taking the same sort of models we have already learned about and applying them to a different context in which “time” is a predictor.",
    "crumbs": [
      "5: Example: Longitudinal MLM"
    ]
  },
  {
    "objectID": "05_long.html#example",
    "href": "05_long.html#example",
    "title": "5: Example: Longitudinal MLM",
    "section": "Example",
    "text": "Example\n\nData: lmm_mindfuldecline.csv\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.\nThe data are available at: https://uoepsy.github.io/data/lmm_mindfuldecline.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nsitename\nName of the site where the study was conducted\n\n\nppt\nParticipant Identifier\n\n\ncondition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n\n\nvisit\nStudy Visit Number (1 - 10)\n\n\nage\nAge (in years) at study visit\n\n\nACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n\n\nimp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n\n\n\n\n\n\n\n\n\nexploring the data\n\nlibrary(tidyverse)\nlibrary(lme4)\nmmd &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_mindfuldecline.csv\")\nhead(mmd)\n\n# A tibble: 6 × 7\n  sitename ppt   condition visit   age   ACE imp  \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 Sncbk    PPT_1 control       1    60  84.5 unimp\n2 Sncbk    PPT_1 control       2    62  85.6 imp  \n3 Sncbk    PPT_1 control       3    64  84.5 imp  \n4 Sncbk    PPT_1 control       4    66  83.1 imp  \n5 Sncbk    PPT_1 control       5    68  82.3 imp  \n6 Sncbk    PPT_1 control       6    70  83.3 imp  \n\n\nHow many participants in each condition? We know from the description there should be 10 in each, but lets check!\n\nmmd |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    n_ppt = n_distinct(ppt)\n  )\n\n# A tibble: 2 × 2\n  condition   n_ppt\n  &lt;chr&gt;       &lt;int&gt;\n1 control        10\n2 mindfulness    10\n\n\nHow many observations does each participant have? With only 20 participants, we could go straight to plotting as a way of getting lots of information all at once. From the plot below, we can see that on the whole participants’ cognitive scores tend to decrease. Most participants have data at every time point, but 4 or 5 people are missing a few. The control participants look (to me) like they have a slightly steeper decline than the mindfulness group:\n\nggplot(mmd, aes(x = age, y = ACE, col = condition)) + \n  geom_point() +\n  geom_line(aes(group=ppt), alpha=.4)+\n  facet_wrap(~ppt)\n\n\n\n\n\n\n\n\n\n\nmodelling change over time\nInitially, we’ll just model how cognition changes over time across our entire sample (i.e. ignoring the condition the participants are in). Note that both the variables study_visit and age represent exactly the same information (time), so we have a choice of which one to use.\n\n\n\n\n\n\nWhy the age variable (currently) causes problems\n\n\n\n\n\nAs it is, the age variable we have starts at 60 and goes up to 78 or so.\nIf we try and use this in a model, we get an error!\n\nmod1 &lt;- lmer(ACE ~ 1 + age + \n               (1 + age | ppt), \n             data = mmd)\n\nModel failed to converge with max|grad| = 0.366837 (tol = 0.002, component 1)\nThis is because of the fact that intercepts and slopes are inherently dependent upon one another. Remember that the intercept is “when all predictors are zero”. So in this case it is the estimate cognition of new-born babies. But all our data comes from people who are 65+ years old!\nThis means that trying to fit (1 + age | ppt) will try to estimate the variability in people’s change in cognition over time, and the variability in cognition at age zero. As we can see in Figure 1, because the intercept is so far away from the data, the angle of each persons’ slope has a huge influence over where their intercept is. The more upwards a persons’ slope is, the lower down their intercept is.\n\n\n\n\n\n\n\n\nFigure 1: lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the ‘intercept’ is estimated back at age 0\n\n\n\n\n\nThis results in issues for estimating our model, because the intercepts and slopes are perfectly correlated! The estimation process has hit a boundary (a perfect correlation):\n\nVarCorr(mod1)\n\n Groups   Name        Std.Dev. Corr  \n ppt      (Intercept) 7.51567        \n          age         0.12696  -0.999\n Residual             0.51536        \n\n\nSo what we can do is either center age on 60 (so that the random intercept is the estimated variability in cognition at aged 60, i.e. the start of the study), or use the study_visit variable.\nEither will do, we just need to remember the units they are measured in!\n\n\n\nLet’s center age on 60:\n\nmmd$ageC &lt;- mmd$age-60\n\nAnd fit our model:\n\nmod1 &lt;- lmer(ACE ~ 1 + ageC + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom our fixed effects, we can see that scores on the ACE tend to decrease by about 0.18 for every 1 year older people get (as a very rough rule of thumb, \\(t\\) statistics that are \\(&gt;|2\\text{-ish}|\\) are probably going to be significant when assessed properly).\n\nsummary(mod1)\n\n...\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 85.22558    0.10198 835.735\nageC        -0.17938    0.03666  -4.893\nWe’re now ready to add in group differences in their trajectories of cognition:\n\nmod2 &lt;- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom this model, we can see that for the control group the estimated score on the ACE at age 60 is 85 (that’s the (Intercept)). For these participants, scores are estimated to decrease by -0.27 points every year (that’s the slope of ageC). For the participants in the mindfulness condition, they do not score significantly differently from the control group at age 60 (the condition [mindfulness] coefficient). For the mindfulness group, there is a reduction in the decline of cognition compared to the control group, such that this group decline 0.17 less than the control group every year.\n(note, there are always lots of ways to frame interactions. A “reduction in decline” feels most appropriate to me here)\nGiven that we have a fairly small number of clusters here (20 participants), Kenward Rogers is a good method of inference as it allows us to use REML (meaning unbiased estimates of the random effect variances) and it includes a small sample adjustment to our standard errors.\n\nlibrary(parameters)\nmodel_parameters(mod2, ci_method=\"kr\", ci_random=FALSE)\n\n# Fixed Effects\n\nParameter                      | Coefficient |   SE |         95% CI |      t |    df |      p\n----------------------------------------------------------------------------------------------\n(Intercept)                    |       85.20 | 0.15 | [84.89, 85.52] | 568.00 | 17.75 | &lt; .001\nageC                           |       -0.27 | 0.04 | [-0.36, -0.17] |  -5.93 | 17.95 | &lt; .001\ncondition [mindfulness]        |        0.05 | 0.21 | [-0.39,  0.49] |   0.23 | 17.49 | 0.821 \nageC × condition [mindfulness] |        0.17 | 0.06 | [ 0.04,  0.31] |   2.73 | 17.99 | 0.014 \n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: ppt)       |        0.35\nSD (ageC: ppt)            |        0.14\nCor (Intercept~ageC: ppt) |        0.26\nSD (Residual)             |        0.49\n\n\nFrom those parameters and our interpretation above, we are able to start putting a picture together - two groups that start at the same point, one goes less steeply down over time than the other.\nAnd that’s exactly what we see when we visualise those fixed effects:\n\n\nCode\nlibrary(effects)\neffect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=ageC+60,y=fit,\n             ymin=lower,ymax=upper,\n             col=condition, fill = condition))+\n  geom_line(lwd=1)+\n  geom_ribbon(alpha=.2, col=NA) +\n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nSometimes it is more helpful for a reader if we add in the actual observed trajectories to these plots. To do so, we need to combine two data sources - the fixed effects estimation from effect(), and the data itself:\n\n\nCode\nploteff &lt;- effect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame()\n\nmmd |&gt;\n  ggplot(aes(x=ageC+60,col=condition,fill=condition))+\n  geom_line(aes(y=ACE,group=ppt), alpha=.4) +\n  geom_line(data = ploteff, aes(y=fit), lwd=1)+\n  geom_ribbon(data = ploteff, aes(y=fit,ymin=lower,ymax=upper),\n              alpha=.2, col=NA) + \n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nThis plot gives us more a lot more context. To a lay reader, our initial plot potentially could be interpreted as if we would expect every person’s cognitive trajectories to fall in the blue and red bands. But those bands are representing the uncertainty in the fixed effects - i.e. the uncertainty in the average persons’ trajectory. When we add in the observed trajectories, we see the variability in people’s trajectories (one person even goes up over time!).\nOur model represents this variability in the random effects part. While the estimated average slope is -0.27 for the control group (and -0.27+0.17=-0.09 for the mindfulness group), people are estimated to vary in their slopes with a standard deviation of 0.14 (remember we can extract this info using VarCorr(), or just look in the output of summary(model)).\n\nVarCorr(mod2)\n\n Groups   Name        Std.Dev. Corr \n ppt      (Intercept) 0.34615       \n          ageC        0.13866  0.260\n Residual             0.49450       \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Two normal distributions with mean of -0.27 (purple) and -.09 (green) and a standard deviation of 0.14\n\n\n\n\n\nIf you think about what this means - it means that some participants we would expect to actually increase in their slopes. If we have a normal distribution with a mean of -0.3 or -0.09 and a standard distribution of 0.14, then we would expect some values to to positive (see e.g., Figure 2).",
    "crumbs": [
      "5: Example: Longitudinal MLM"
    ]
  },
  {
    "objectID": "05_long.html#footnotes",
    "href": "05_long.html#footnotes",
    "title": "5: Example: Longitudinal MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nassuming that it is people we are studying!↩︎",
    "crumbs": [
      "5: Example: Longitudinal MLM"
    ]
  },
  {
    "objectID": "03_inference.html",
    "href": "03_inference.html",
    "title": "3: Inference for MLM",
    "section": "",
    "text": "This reading:\n\nBriefly: why getting p-values from lmer() is not as easy as it was for lm()\nSummaries of the main approaches that are typically used for conducting inference (i.e. getting confidence intervals or p-values, model comparisons) for multilevel models.\n\nDon’t feel like you have to remember all of these, just be aware that they exist, and refer back to this page whenever you need to.",
    "crumbs": [
      "3: Inference for MLM"
    ]
  },
  {
    "objectID": "03_inference.html#summary",
    "href": "03_inference.html#summary",
    "title": "3: Inference for MLM",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood based\nparametric bootstrap\n\n\n\n\ntests/CIs of individual parameters\nTests of individual parameters can be done by refitting with lmerTest::lmer(...) for the Satterthwaite (S) method, or using parameters::model_parameters(model, ci_method=\"kr\") for Kenward Rogers (KR).\nProfile likelihood CIs for individual parameters can be obtained via confint(m, method=\"profile\"), but this can be computationally demanding.\nParametric Bootstrapped CIs for individual parameters can be obtained via confint(m, method=\"boot\")\n\n\nmodel comparisons(different fixed effects, same random effects)\nComparisons of models that differ only in their fixed effects can be done via \\(F\\) tests in the pbkrtest package:SATmodcomp(m2, m1) for S and KRmodcomp(m2, m1) for KR.\nComparisons of models that differ only in their fixed effects can be done via LRT using anova(m1, m2)\nComparisons of models that differ only in their fixed effects can be done via a bootstrapped LRT using PBmodcomp(m2, m1) from the pbkrtest package.\n\n\n\nFor KR, models must be fitted with REML=TRUE (a good option for small samples). For S, models can be fitted with either.\nFor likelihood based methods for fixed effects, models must be fitted with REML=FALSE.Likelihood based methods are asymptotic (i.e. hold when \\(n \\rightarrow \\infty\\)). Best avoided with smaller sample sizes (i.e. a small number of groups)\nTime consuming, but considered best available method (can be problematic with unstable models)\n\n\n\n\n\n\n\n\n\noptional: testing random effects?\n\n\n\n\n\nTests of random effects are difficult because the null hypothesis (the random effect variance is zero) lies on a boundary (you can’t have a negative variance). Comparisons of models that differ only in their random effects can be done by comparing ratio of likelihoods when fitted with REML=TRUE (this has to be done manually), but these tests should be treated with caution.\nWe can obtain confidence intervals for our random effect variances using both the profile likelihood and the parametric boostrap methods discussed above.\nAs random effects are typically part of the experimental design, there is often little need to test their significance. In most cases, the maximal random effect structure can be conceptualised without reference to the data or any tests, and the inclusion/exclusion of specific random effects is more a matter of what simplifications are required for the model to converge. Inclusion/exclusion of parameters based on significance testing is rarely, if ever a sensible approach.",
    "crumbs": [
      "3: Inference for MLM"
    ]
  },
  {
    "objectID": "03_inference.html#footnotes",
    "href": "03_inference.html#footnotes",
    "title": "3: Inference for MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(n\\) observations minus \\(k\\) parameters (slope of x) minus 1 intercept↩︎",
    "crumbs": [
      "3: Inference for MLM"
    ]
  },
  {
    "objectID": "11_writing.html",
    "href": "11_writing.html",
    "title": "11: Reporting on analyses with MLM",
    "section": "",
    "text": "This reading:\n\nA (non-exhaustive) checklist of things to think about/include when writing up analyses with multilevel models",
    "crumbs": [
      "11: Reporting on analyses with MLM"
    ]
  },
  {
    "objectID": "11_writing.html#the-sample-data",
    "href": "11_writing.html#the-sample-data",
    "title": "11: Reporting on analyses with MLM",
    "section": "The sample data",
    "text": "The sample data\nDescriptives of hierarchical data are sometimes a bit more difficult than when we don’t have any ‘levels’. Typically, what we are wanting to do is provide our readers with a picture of the characteristics of our sample. “Our sample” now refers to multiple levels, so we want to describe each of these. More often than not, one of these levels will be a bit more interesting to us as a population we are hoping to generalise to. In psychology we are usually interested in “people”, so if we have data that is multiple trials per participant, we would probably want to focus on describing the participants (the clusters) as the individual trials are something we exert control over as the experimenter. If each datapoint was a child and they were nested in schools, we would probably want to describe both the children and the schools that are in our sample.\nThe aim here is to provide a picture of our sample so that a reader can get a sense of how ‘transportable’ the findings are to different contexts. For instance, if participants in our study are all university students, then we want to be careful about thinking that the findings will apply in other populations (see e.g. “most people aren’t WEIRD”).\n\nA checklist\n\n\nwhat is the hierarchical data structure (how many levels, what is each level?)\nDescribe any data cleaning outlier/data removal prior to calculating descriptive statistics (these tend to be the impossible values - i.e. observations that you would never want in your data anyway)\nsample sizes: how many at each level?\n\nhow many lower-level within each higher level unit? (if this varies, provide an average, and possibly a min and a max)\n\nscales of measured variables\ndescriptive statistics of relevant variables that characterise your sample.\n\nthese should be computed at the level at which they were measured. For instance, if you have observations grouped by participant, mean(data$age) would give the average age of your observations (which isn’t meaningful, and would differ from the average age of your participants if you have a different number of observations for each participant).\n\nHow much of the variability in the outcome variable is attributable to the clustering? (i.e. ICC)",
    "crumbs": [
      "11: Reporting on analyses with MLM"
    ]
  },
  {
    "objectID": "11_writing.html#the-methods",
    "href": "11_writing.html#the-methods",
    "title": "11: Reporting on analyses with MLM",
    "section": "The methods",
    "text": "The methods\nWhen writing up any statistical analysis, one important thing to keep in mind is transparency in the decisions and actions taken in the analysis process. The aim is to avoid a reader wondering “how did they end up with these results?”. Ideally, another researcher would be able to reproduce your analysis based on your explanation of what you have done.\nWith multilevel models, there’s a lot of choices that we make - the scaling and centering of variables, models being fitted with ML vs REML, the method used to conduct inference, and so on. In addition, in the event that we arrived at our final model after a series of non-converging models that were then simplified, we would ideally explain this process.\n\nA checklist\n\n\nDescribe any transformations to the data that are made prior to conducting the analysis (e.g., you’ll often re-center a time variable)\nDescribe the process that led to your final model(s)\n\nClearly explain the structure of your initial model (e.g. this might be the ‘maximal model’), and if this failed to converge, explain what random effects were removed and in what order? if possible, explain why.\n\nState the software packages and versions used to fit models, along with the estimation method (ML/REML) and optimiser used.\n\nWhat is the structure of your final model(s)?\n\nYou don’t need to write a complicated mathematical equation for your model. Describing it in words is fine provided you’re clear. e.g. “the outcome variable Y was modelled using mixed effects regression with afixed effects including a main effect of A and B as well as their interaction. The random effects include a random intercept by participant”\nLinear/binomial/poisson/… - if not linear, what link function (e.g., logit, log) was used?\nSpecify all fixed effects.\nSpecify all random effects according to the sampling units (e.g. schools/children etc) with which they interact. Be careful to make sure it’s clear what slopes are for which groupings!.\n\n\nIt’s often useful to state clearly the relevant test/comparison/parameter estimate of interest, and link this explicitly to the research questions/hypotheses.\n\nAny model comparisons should be clearly stated so that the reader understands the structure of both models being compared.\nSpecify the methods used to conduct inference (e.g. LRT, bootstrap), and if relevant, explain why (e.g. Kenward Rogers might be used due to a small number of level 2 units).",
    "crumbs": [
      "11: Reporting on analyses with MLM"
    ]
  },
  {
    "objectID": "11_writing.html#the-results",
    "href": "11_writing.html#the-results",
    "title": "11: Reporting on analyses with MLM",
    "section": "The results",
    "text": "The results\nWriting up results will vary depending on the strategies employed. The important part is to highlight the relevant test/comparison that addresses the research aims, and explain what the result means with respect to the question at hand.\nAdditionally, be sure to take some time to understand what the estimate actually means (\\(p&lt;.05\\) is just a small part of the story). With models like these we are almost always just looking at outcome “differences” between levels of a categorical predictor or “change” across some continuous predictor. Does the estimated difference/change, and its direction, make sense to you? What does it mean practically? Asking yourself questions like this is also a good way of sense checking your analysis (i.e. a strong counter-intuitive finding could mean you have a variable coded back to front!).\nFor reporting parameter estimates, ideally we would include both the estimate and the precision (i.e. the standard error or a confidence interval). When reporting statistical tests, make sure to include the test statistic (\\(t\\), \\(F\\), \\(\\chi^2\\), etc.), the relevant degrees of freedom, and the p-value.\n\nA checklist\n\n\nresults of model comparisons and what they mean in the context of the research question\n\nparameter estimates and precision for relevant fixed effects.\n\nvariance components\n\nhow does the effect of interest vary between groups?\n\nis it related to other group level variance (i.e. the random effect correlations if modelled)\n\nif relevant - sensitivity to influential observations and clusters.",
    "crumbs": [
      "11: Reporting on analyses with MLM"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multi-level/Mixed Effects Models",
    "section": "",
    "text": "These readings and walkthroughs show how we can extend the linear model to analyses of “hierarchical data”, in which observations are clustered in higher-level groups (e.g. trials within participants, or students within schools). We see how these methods lend themselves well to longitudinal data, and we present one of the more traditional approaches to studying non-linear change over time. The assumptions underlying these models are discussed, along with certain considerations that are important to bear in mind especially for observational data.\nReadings and walkthroughs are presented with accompanying R code."
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\n\n\n\n\n\nSolution\n\n\n\nSolution 1. solution\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood).",
    "crumbs": [
      "Additional Docs",
      "Likelihood vs Probability"
    ]
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin flip. For a fair coin, the chance of getting a heads/tails for any given flip is 0.5.\nWe can simulate the number of “heads” in a single fair coin flip with the following code (because it is a single flip, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\n\nWe can simulate the number of “heads” in 8 fair coin flips with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin flips? Answer: 4! Doing another 8 flips:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 6\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin flips, but to do 1000 sets of 8 flips:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  2  18 110 229 273 213 121  28   6",
    "crumbs": [
      "Additional Docs",
      "Likelihood vs Probability"
    ]
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nSo what is the probability of observing \\(k\\) heads in \\(n\\) flips of a fair coin?\nAs coin flips are independent, we can calculate probability using the product rule (\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent).\nSo the probability of observing 2 heads in 2 flips is \\(0.5 \\cdot 0.5 = 0.25\\)\nWe can get to this probability using dbinom():\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 flips, those two heads could occur in various ways:\n\n\n\n\n\n\n\n\nWays to get 2 heads in 8 flips\n\n\n\n\nTHTTTHTT\n\n\nHTTTTHTT\n\n\nTTTTHTTH\n\n\nTTTHTTTH\n\n\nHTTTTTTH\n\n\nHTTTHTTT\n\n\nHHTTTTTT\n\n\nTTHTTTTH\n\n\nTTTHTTHT\n\n\n...\n\n\n\n\n\n\n\nAs it happens, there are 28 different ways this could happen.2\nThe probability of getting 2 heads in 8 flips of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin flips (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given flip)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin flips is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0",
    "crumbs": [
      "Additional Docs",
      "Likelihood vs Probability"
    ]
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nSo how does likelihood differ?\nFor likelihood, we are interested in hypotheses about or models of our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?). Do we think it is biased to land on heads 60% of the time? or 30% of the time? All of these are different ‘models’.\nTo consider these hypotheses, we need to observe some data - we need to have a given number of flips, and the resulting number of heads.\nWhereas when discussing probability, we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given flip, for the likelihood we are fixing the number of heads observed, and can make statements about different possible parameters that might govern the coin’s behaviour.\nFor example, let’s suppose we did observe 2 heads in 8 flips, what is the probability of seeing this data given various parameters?\nHere, our parameter (the probability that we think the coin lands on heads) can take any real number between from 0 to 1, but let’s do it for a selection:\n\npossible_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, possible_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin flips, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). The idea that the coin is fair (0.5 probability) is more likely. The most likely parameter is 0.25 (because \\(\\frac{2}{8}=0.25\\)).\nYou can visualise this below:\n\n\n\n\n\n\n\n\nFigure 1: Likelihood curve (probability of observed data across possible parameters)",
    "crumbs": [
      "Additional Docs",
      "Likelihood vs Probability"
    ]
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nE.g. above we were talking about \\(P(\\text{2 heads in 8 flips}\\vert \\text{fair coin})\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\).\nGiven that we know that if we have a specific value for \\(\\theta\\), then \\(P(d \\vert \\theta)\\) will give us the probability of observing \\(d\\), we can ask “what value of \\(\\theta\\) will maximise the probability of observing \\(d\\)?”.\nThis will sometimes get written as \\(\\mathcal{L}(\\theta \\vert d)\\) as the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\).",
    "crumbs": [
      "Additional Docs",
      "Likelihood vs Probability"
    ]
  },
  {
    "objectID": "lvp.html#maximum-likelihood-estimation",
    "href": "lvp.html#maximum-likelihood-estimation",
    "title": "Likelihood vs Probability",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nFigure 1 shows the probability of our observed data (2 heads in 8 coin flips) for various “models” of the coin’s behaviour. In this simple case, the candidate “models” of the coin’s behaviour are simply different values for “the probability of getting heads”.\nHowever, the idea of “probability of data given some model” can scale up to more complex statistical models like regression models. The important difference is that such models consist of more than just one parameter. For instance, a simple regression model of lm(y ~ 1 + x, data) involves three things: the intercept, the slope of x, and the standard deviation of the error.\nMaximum likelihood estimation is the process of optimising the set of parameters that result in the greatest probability of the observed data. In our coin-flip example in Figure 1, this is essentially asking “where is the top of the curve?”, but as soon as we add more parameters we have more dimensions, and the curve becomes a surface.",
    "crumbs": [
      "Additional Docs",
      "Likelihood vs Probability"
    ]
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. There are other ways to do statistics (not covered in this course) - e.g., in Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎\nIf you really want to see them all, try running combn(8, 2) in your console.↩︎",
    "crumbs": [
      "Additional Docs",
      "Likelihood vs Probability"
    ]
  },
  {
    "objectID": "04_log.html",
    "href": "04_log.html",
    "title": "4: Example: Logistic MLM",
    "section": "",
    "text": "This reading:\n\nWalkthrough Example: Logistic multilevel models\n\nlm() is to glm() as lmer() is to glmer()",
    "crumbs": [
      "4: Example: Logistic MLM"
    ]
  },
  {
    "objectID": "04_log.html#example",
    "href": "04_log.html#example",
    "title": "4: Example: Logistic MLM",
    "section": "Example",
    "text": "Example\n\nData: msmr_monkeystatus.csv\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a “dominant” or “subordinate” status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either “easy” or “difficult”, and the researchers recorded whether or not the monkey solved each problem.\nWe’re interested in how the social status of monkeys is associated with the ability to solve problems.\nThe data is available at https://uoepsy.github.io/data/msmr_monkeystatus.csv.\n\n\ngetting to know my monkeys\nWe know from the study background that we have a series group of monkeys who have each attempted to solve some problems. If we look at our data, we can see that it is already in long format, in that each row represents the lowest unit of observation (a single problem attempted). We also have the variable monkeyID which indicates what monkey each problem has been attempted by. We can see the status of each monkey, and the difficulty of each task, along with whether it was solved:\n\nlibrary(tidyverse)\nlibrary(lme4)\nmstat &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\nhead(mstat)\n\n# A tibble: 6 × 4\n  status      difficulty monkeyID solved\n  &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;\n1 subordinate easy       Seunghoo      1\n2 subordinate easy       Seunghoo      0\n3 subordinate difficult  Seunghoo      0\n4 subordinate easy       Seunghoo      1\n5 subordinate difficult  Seunghoo      0\n6 subordinate easy       Seunghoo      1\n\n\nWe can do some quick exploring to see how many monkeys we have (50), and how many problems each one attempted (min = 3, max = 11:\n\nmstat |&gt; \n  count(monkeyID) |&gt; # count the monkeys!  \n  summary()\n\n   monkeyID               n        \n Length:50          Min.   : 3.00  \n Class :character   1st Qu.: 6.25  \n Mode  :character   Median : 8.00  \n                    Mean   : 7.94  \n                    3rd Qu.:10.00  \n                    Max.   :11.00  \n\n\nLet’s also see how many monkeys of different statuses we have in our sample:\n\nmstat |&gt; \n  group_by(status) |&gt; # group statuses\n  summarise(\n    # count the distinct monkeys\n    nmonkey = n_distinct(monkeyID)\n  ) \n\n# A tibble: 3 × 2\n  status      nmonkey\n  &lt;chr&gt;         &lt;int&gt;\n1 adolescent       16\n2 dominant         23\n3 subordinate      11\n\n\nIt’s often worth plotting as much as you can to get to a sense of what we’re working with. Here are the counts of easy/difficult problems that each monkey attempted. We can see that Richard only did difficult problems, and Nadheera only did easy ones, but most of the monkeys did both types of problem.\n\n# which monkeys did what type of problems? \nmstat |&gt; count(status, monkeyID, difficulty) |&gt;\n  ggplot(aes(x=difficulty,y=n, fill=status))+\n  geom_col()+\n  facet_wrap(~monkeyID) +\n  scale_x_discrete(labels=abbreviate) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhen working with binary outcomes, it’s often useful to calculate and plot proportions. In this case, the proportions of problems solved for each status of monkey. At first glance it looks like “subordinate” monkeys solve more problems, and adolescents solve fewer (makes sense - they’re still learning!).\n\n# a quick look at proportions of problems solved:\nggplot(mstat, aes(x=difficulty, y=solved,\n                       col=status))+\n  stat_summary(geom=\"pointrange\",size=1)+\n  facet_wrap(~status)\n\n\n\n\n\n\n\n\n\n\nmodels of monkeys\nNow we come to fitting our model.\nRecall that we are interested in how the ability to solve problems differs between monkeys of different statuses. It’s very likely that difficulty of a problem is going to influence that it is solved, so we’ll control for difficulty.\nglmer(solved ~ difficulty + status + \n      ...\n      data = mstat, family = binomial)\nWe know that we have multiple datapoints for each monkey, and it also makes sense that there will be monkey-to-monkey variability in the ability to solve problems (e.g. Brianna may be more likely to solve problems than Jonathan).\nglmer(solved ~ difficulty + status + \n      (1 + ... | monkeyID),\n      data = mstat, family = binomial)\nFinally, it also makes sense that effects of problem-difficulty might vary by monkey (e.g., if Brianna is just really good at solving problems, problem-difficulty might not make much difference. Whereas if Jonathan is struggling with the easy problems, he’s likely to really really struggle with the difficult ones!).\nFirst, we’ll relevel the difficulty variable so that the reference level is “easy”:\n\nmstat &lt;- mstat |&gt; mutate(\n  difficulty = fct_relevel(factor(difficulty), \"easy\")\n)\n\nand fit our model:\n\nmmod &lt;- glmer(solved ~ difficulty + status + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nsummary(mmod)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n   Data: mstat\n\n     AIC      BIC   logLik deviance df.resid \n   503.7    531.6   -244.8    489.7      390 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9358 -0.6325 -0.3975  0.6748  2.5160 \n\nRandom effects:\n Groups   Name                Variance Std.Dev. Corr \n monkeyID (Intercept)         1.551    1.246         \n          difficultydifficult 1.371    1.171    -0.44\nNumber of obs: 397, groups:  monkeyID, 50\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)          -0.3945     0.3867  -1.020  0.30767   \ndifficultydifficult  -0.8586     0.3053  -2.812  0.00492 **\nstatusdominant        0.6682     0.4714   1.417  0.15637   \nstatussubordinate     1.4596     0.5692   2.564  0.01033 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) dffclt sttsdm\ndffcltydffc -0.333              \nstatusdmnnt -0.721 -0.031       \nstatssbrdnt -0.594 -0.033  0.497\n\n\n\n\ntest and visualisations of monkey status\nTo examine if monkey status has an effect, we can compare with the model without status:\n\n\nCode\nmmod0 &lt;- glmer(solved ~ difficulty + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nanova(mmod0, mmod)\n\n\nData: mstat\nModels:\nmmod0: solved ~ difficulty + (1 + difficulty | monkeyID)\nmmod: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmmod0    5 506.13 526.05 -248.07   496.13                       \nmmod     7 503.70 531.58 -244.85   489.70 6.4367  2    0.04002 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd we can see that the status of monkeys is associated with differences in the probability of successful problem solving (\\(\\chi^2(2)\\) = 6.44, p &lt; 0.05).\nAnd if we want to visualise the relevant effect, we can (as we did with glm()) plot on the predicted probability scale, which is much easier to interpret:\n\n\nCode\nlibrary(effects)\neffect(term=c(\"status\",\"difficulty\"), mod=mmod) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=difficulty, y=fit))+\n  geom_pointrange(aes(ymin=lower,ymax=upper, col=status),\n                  size=1, lwd=1,\n                  position=position_dodge(width=.3)) +\n  labs(x = \"problem difficulty\", y = \"predicted probability\")\n\n\n\n\n\n\n\n\n\n\n\ninterpretation\nAnd just with the single level logistic models, our fixed effects can be converted to odds ratios (OR), by exponentiation:\n\ncbind(\n  fixef(mmod), # the fixed effects\n  confint(mmod, method=\"Wald\", parm=\"beta_\") # Wald CIs for fixed effects\n) |&gt;\n  exp()\n\n                                  2.5 %     97.5 %\n(Intercept)         0.6740221 0.3158658  1.4382872\ndifficultydifficult 0.4237565 0.2329242  0.7709359\nstatusdominant      1.9506650 0.7743099  4.9141746\nstatussubordinate   4.3042614 1.4106434 13.1334867\n\n\n\n\n\n\n\n\n\n\nterm\nest\nOR\nOR interpretation\n\n\n\n\n(Intercept)\n-0.39\n0.67\nestimated odds of an adolescent monkey solving an easy problem\n\n\ndifficultydifficult\n-0.86\n0.42\nodds of successful problem solving are more than halved (0.42 times the odds) when the average monkey moves from an easy to a difficult problem\n\n\nstatusdominant\n0.67\n1.95\nodds of success would be almost doubled (1.95 times the odds) if the average monkey were to change from adolescent to dominant status (NB this is non-significant)\n\n\nstatussubordinate\n1.46\n4.30\nodds of success would quadruple (4.3 times the odds) if the average monkey were to change from adolescent to subordinate status\n\n\n\n\n\n\n\n\n\n\n\nSide note\nContrast this with what we would get from a linear multilevel model. If we were instead modelling a “problem score” with lmer(), rather than “solved yes/no” with glmer(), our coefficients would be interpreted as the estimated difference in scores between adolescent and subordinate monkeys.\nNote that estimating differences between groups is not quite the same idea as estimating the effect “if a particular (the average) monkey changed from adolescent to subordinate”. In the linear world, these two things are the same, but our odds ratios give us only the latter.",
    "crumbs": [
      "4: Example: Logistic MLM"
    ]
  },
  {
    "objectID": "04_log.html#footnotes",
    "href": "04_log.html#footnotes",
    "title": "4: Example: Logistic MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that binary outcomes are just a special case of the binomial↩︎",
    "crumbs": [
      "4: Example: Logistic MLM"
    ]
  },
  {
    "objectID": "08_modelbuilding.html",
    "href": "08_modelbuilding.html",
    "title": "8: Model Building",
    "section": "",
    "text": "This reading:\n\nModel building and convergence issues\n\nThe ‘maximal’ model\nNon-convergence and overfitted models\nStrategies for simplifying models",
    "crumbs": [
      "8: Model Building"
    ]
  },
  {
    "objectID": "08_modelbuilding.html#maximal-model",
    "href": "08_modelbuilding.html#maximal-model",
    "title": "8: Model Building",
    "section": "Maximal Model",
    "text": "Maximal Model\nTypically for many research designs, the following steps will keep you mostly on track to finding the maximal model.\nStart by thinking of the model structure in terms of these components:\n\nlmer(outcome ~ fixed effects + \n       (random effects | grouping structure), \n     data = ...)\n\n\nSpecify the outcome ~ fixed effects bit first.\n\nThe outcome variable should be clear: it is the variable we are wishing to explain/predict.\nThe fixed effects are the things we want to use to explain/predict variation in the outcome variable. These will often be the things that are of specific inferential interest along with potential confounders and other covariates. Just like the simple linear model.\n\nIf there is a grouping structure to your data, and those groups (preferably n&gt;7 or 8) are perceived as a random sample of a wider population (the specific groups aren’t interesting to you), then consider including random intercepts (and possibly random slopes of predictors) for those groups (1 + ... | grouping).\nIf there are multiple different grouping structures, is one nested within another? If so, we can specify this as (1 | higher_grouping ) + (1 |  lower_grouping:higher_grouping).2\nIf the grouping structures are not nested, we can specify them as crossed: (1 | grouping1) + (1 | grouping2).\nIf any of the predictors in the fixed effects vary within the groups, it may be possible to also include them as random effects. For predictors that instead vary between groups, it rarely makes sense to include these as by-group random effects. For example, if we had a model with lmer(score ~ genetic_status + (1 + genetic_status | patient)) then we would be trying to model a process where “the effect of genetic_status on scores is different for each patient”. But if you consider an individual patient, their genetic status never changes. For patient \\(i\\), what is “the effect of genetic status on score”? It’s undefined. This is because genetic status only varies between patients.\n\nas a general rule, don’t specify random effects that are not also specified as fixed effects (an exception could be specifically for model comparison, to isolate the contribution of the fixed effect).\n\nSometimes, things can vary within one grouping, but not within another. E.g., for a design in which patients are nested within hospitals (1 | hospital) + (1 | patient:hospital), genetic_status varies between patients, but within hospitals. Therefore we could theoretically fit a random effect of (1 + genetic_status | hospital), but not one for (1 + genetic_status | patient:hospital).",
    "crumbs": [
      "8: Model Building"
    ]
  },
  {
    "objectID": "08_modelbuilding.html#non-convergence",
    "href": "08_modelbuilding.html#non-convergence",
    "title": "8: Model Building",
    "section": "Non-Convergence",
    "text": "Non-Convergence\nOftentimes, models with more complex random effect structures will not converge because there are so many parameters, and not enough variability in the data, meaning that there are more places for the model estimation to go wrong. Remember that we fit these models with maximum likelihood estimation (MLE), a process that involves taking a guess at the model parameters that result in the greatest probability of the observed data, and step-by-step improving those guesses until we think we’re at the most likely set of parameters - until the model ‘converges’. Sometimes, however, MLE can sometimes get stuck, resulting in ‘non-convergence’.\nThere are many possible reasons for non-convergence, and it does not necessarily mean the fit is incorrect. However it is is cause for concern, and should be addressed before using the model, else you may end up reporting inferences which do not hold. There are lots of different things which we can try which might help our model to converge. A select few are detailed below:\n\n\n\n\n\n\nThings we can try\n\n\n\n\n\n\nmost likely solutions:\n\ndouble-check the model specification and the data\nConsider simplifying your model (more on this below)\n\n\n\nCenter and scale continuous predictor variables (e.g. with scale)\nChange the optimization method (for example, here we change it to bobyqa):\nlmer(..., control = lmerControl(optimizer=\"bobyqa\"))\nglmer(..., control = glmerControl(optimizer=\"bobyqa\"))\nUse allFit() to try the fit with all available optimizers. This will of course be slow, but is considered ‘the gold standard’; “if all optimizers converge to values that are practically equivalent, then we would consider the convergence warnings to be false positives.”\nallopts &lt;- allFit(model)\nsummary(allopts)\n\n\n\n\nFine-tune an optimizer. Using the optCtrl argument to [g]lmerControl (see ?convergence for details), we can have a lot of control over the optimizer. Recall that the optimizer is a method of iteratively assessing a set of parameters to maximise the probability of seeing the observed data3. We can change things such as the number of steps the algorithm keeps trying for, and the thresholds at which the algorithm stops (Figure 1).\n\n\n\n\n\n\n\n\n\n\nFigure 1: An optimizer will stop after a certain number of iterations, or when it meets a tolerance threshold",
    "crumbs": [
      "8: Model Building"
    ]
  },
  {
    "objectID": "08_modelbuilding.html#singular-fits",
    "href": "08_modelbuilding.html#singular-fits",
    "title": "8: Model Building",
    "section": "Singular Fits",
    "text": "Singular Fits\nAs well as convergence warnings, you may have noticed that some of our models over the last few weeks have been giving a warning message:\n\nboundary (singular) fit: see ?isSingular\n\nUp to now, we’ve been largely ignoring these messages, but we should really have been addressing them in some way. ‘Singular fit’ warnings indicate that our model is likely to be ‘overfitted’ - that is, the random effects structure which we have specified is too complex to be supported by the data.\nFor simple random effect structures (i.e. a random intercept + a random slope), we can often see this issue reflected in the variance components of the random effect, when variances get estimated at (or very close to) zero, and/or when correlations get estimated at (or very close to) 1 or -1 (Figure 2). With more complex structures it is not always so easily visible, but we can do a double check for this issue using the handy isSingular(model) function - if it returns TRUE then it indicates our model might be overfitted.\n\n\n\n\n\n\n\n\nFigure 2: In simple random effect structures we can often easily see issues of overfitting as they are reflected in variances being estimated as 0 and/or correlations being estimated as perfect correlations of 1 or -1\n\n\n\n\n\nWhat do we do in these cases? Simplify, simplify, simplify!\n\n\n\n\n\n\nScales can matter!\n\n\n\n\n\n\nThe scale of our predictors can sometimes play a part here. If we were fitting a model of shoe_size ~ height, then the estimated coefficient is going to depend on how we measure height. If we measure it in millimeters, then we’ll probably have a very small coefficient (people’s shoe size will only change by a tiny amount for every 1mm height they gain), but if we measure height in kilometers, then we’ll have a very big coefficient (“grow an extra kilometer in height, and your shoe size will increase 10000 sizes”!!).\nIn the multilevel model, we’re estimating the variances in these relationships across a set of groups (or ‘clusters’). If the coefficient is in millimeters, then the variance is in millimeters too, and so the number will be quite small. If it’s in kilometers, the coefficient is in units 100,000 times bigger, and so is the variance.\nScaling predictors doesn’t change the relationship being studied, but it does change the numeric values we are asking our relationship to be presented in. As the estimation of multilevel models can get into difficulty when variances are too close to zero, you may occasionally receive messages such as those below.\nPay attention to them, and check your variables. If some are on very different scales, then consider trying to rescale them to something that is still meaningful for you.\n\nWarning messages: 1: Some predictor variables are on very different scales: consider rescaling\n\n\nWarning messages: 1: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, :Model is nearly unidentifiable: large eigenvalue ratio - Rescale variables?",
    "crumbs": [
      "8: Model Building"
    ]
  },
  {
    "objectID": "08_modelbuilding.html#footnotes",
    "href": "08_modelbuilding.html#footnotes",
    "title": "8: Model Building",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthis doesn’t mean simply including every predictor in the fixed effects also in the random effects part. ‘possible’ refers to ‘possible given the study design’↩︎\nthe order doesn’t matter with the colon, so (1 | higher_grouping ) + (1 |  higher_grouping:lower_grouping) is just the same↩︎\ninstead of maximising the likelihood, more often (for practical reasons) our algorithms try to minimise \\(-2 \\times\\) the log-likelihood↩︎",
    "crumbs": [
      "8: Model Building"
    ]
  },
  {
    "objectID": "07_ranef.html",
    "href": "07_ranef.html",
    "title": "7: Random Effect Structures",
    "section": "",
    "text": "This reading:\n\nExtending the multilevel model to encompass more complex grouping structures\n\nGroups nested inside higher level groups\nNon-nested group structures",
    "crumbs": [
      "7: Random Effect Structures"
    ]
  },
  {
    "objectID": "07_ranef.html#example-1-two-levels",
    "href": "07_ranef.html#example-1-two-levels",
    "title": "7: Random Effect Structures",
    "section": "Example 1: Two levels",
    "text": "Example 1: Two levels\nBelow is an example of a study that has a similar structure to those that we’ve seen thus far, in which we have just two levels (observations that are grouped in some way).\n\n\nStudy Design\nSuppose, for instance, that we conducted an experiment on a sample of 20 staff members from the Psychology department to investigate effects of CBD consumption on stress over the course of the working week. Participants were randomly allocated to one of two conditions: the control group continued as normal, and the CBD group were given one CBD drink every day. Over the course of the working week (5 days) participants stress levels were measured using a self-report questionnaire.\nWe can see our data here:\n\npsychstress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek1.csv\")\nhead(psychstress)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Holly N     Self-report     1 -0.417\n2 Psych Holly N     Self-report     2  0.924\n3 Psych Holly N     Self-report     3  0.634\n4 Psych Holly N     Self-report     4  1.21 \n5 Psych Holly N     Self-report     5  0.506\n6 Psych Tom   Y     Self-report     1 -0.557\n\n\n\n\nPlot\n\n\nCode\n# take the dataset, and make the x axis of our plot the 'day' variable, \n# and the y axis the 'stress' variable: \n# color everything by the CBD groups\nggplot(psychstress, aes(x = day, y = stress, col=CBD)) + \n  geom_point() + # add points to the plot\n  geom_line() + # add lines to the plot\n  facet_wrap(~pid) # split it by participant\n\n\n\n\n\n\n\n\n\n\n\nModel\nWe might fit a model that looks something like this:\n\n\nCode\nlibrary(lme4)\n# re-center 'day' so the intercept is day 1\npsychstress$day &lt;- psychstress$day-1 \n\n# fit a model of stress over time: stress~day\n# estimate differences between the groups in their stress change: day*CBD\n# people vary in their overall stress levels: 1|pid\n# people vary in their how stress changes over the week: day|pid\nm2level &lt;- lmer(stress ~ 1 + day * CBD + \n                  (1 + day | pid), data = psychstress)\n\n\nNote that there is a line in the model summary output just below the random effects that shows us the information about the groups, telling us that we have 100 observations that are grouped into 20 different participants’.\n\nsummary(m2level)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + day * CBD + (1 + day | pid)\n   Data: psychstress\n\nREML criterion at convergence: 127.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.17535 -0.65204 -0.02667  0.64622  1.81574 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n pid      (Intercept) 0.199441 0.44659      \n          day         0.004328 0.06579  0.02\n Residual             0.112462 0.33535      \nNumber of obs: 100, groups:  pid, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.13178    0.14329   0.920\nday          0.07567    0.03461   2.186\nCBDY        -0.08516    0.24221  -0.352\nday:CBDY    -0.19128    0.05851  -3.270\n\nCorrelation of Fixed Effects:\n         (Intr) day    CBDY  \nday      -0.339              \nCBDY     -0.592  0.201       \nday:CBDY  0.201 -0.592 -0.339",
    "crumbs": [
      "7: Random Effect Structures"
    ]
  },
  {
    "objectID": "07_ranef.html#example-2-three-level-nested",
    "href": "07_ranef.html#example-2-three-level-nested",
    "title": "7: Random Effect Structures",
    "section": "Example 2: Three level Nested",
    "text": "Example 2: Three level Nested\nLet’s suppose that instead of simply sampling 20 staff members from the Psychology department, we instead went out and sampled lots of people from different departments across the University. The dataset below contains not just our 20 Psychology staff members, but also data from 220 other people from departments such as History, Philosophy, Art, etc..\n\nneststress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_nested.csv\")\nhead(neststress)\n\n# A tibble: 6 × 6\n  dept  pid      CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 CMVM  Ryan     Y     Self-report     1  0.933\n2 CMVM  Ryan     Y     Self-report     2  0.997\n3 CMVM  Ryan     Y     Self-report     3  0.408\n4 CMVM  Ryan     Y     Self-report     4  0.581\n5 CMVM  Ryan     Y     Self-report     5  0.442\n6 CMVM  Nicholas Y     Self-report     1  0.138\n\n\nIn this case, we have observations that are grouped by participants, and those participants can be grouped into the department in which they work. Three levels of nesting!\nYou can see in the Figure 6 below that there is variation between departments (i.e. people working in Art are a bit more relaxed, Political Science and CMVM is stressful, etc), and then within each of those, there is variation between participants (i.e. some people working in Art are more stressed than other people in Art).\n\n\nCode\nggplot(neststress, aes(x=day, y=stress,col=CBD))+\n  # plot points\n  geom_point()+\n  # split by departments\n  facet_wrap(~dept)+\n  # make a line for each participant\n  geom_line(aes(group=pid),alpha=.3)+ \n  # plot the mean and SE for each day.\n  stat_summary(geom=\"pointrange\",col=\"black\")\n\n\n\n\n\n\n\n\nFigure 6: A longitudinal study in which participants are nested within department\n\n\n\n\n\nTo account for these multiple sources of variation, we can fit a model that says both ( ... | dept) (“things vary by department”) and ( ... | dept:pid) (“things vary by participants within departments”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nneststress$day &lt;- neststress$day-1\n\nmnest &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nNote that we can have different random slopes for departments vs those for participants. Our model above includes all random slopes that are feasible given the study design.\n\n\n\n\n\n\nexplanations of each random slope\n\n\n\n\n\n\nparticipants can vary in their baseline stress levels.\n\n(1 | dept:pid)\n\nparticipants can vary in how stress changes over the week. e.g., some participants might get more stressed over the week, some might get less stressed\n\n(days | dept:pid)\n\n\nparticipants cannot vary in how CBD changes their stress level. because each participant is either CBD or control, “the effect of CBD on stress” doesn’t exist for a single participant (and so can’t very between participants)\n\n(CBD | dept:pid)\n\n\nparticipants cannot vary in how CBD affects their changes in stress over the week. For the same reason as above.\n\n( day*CBD | dept:pid)\n\ndepartments can vary in their baseline stress levels.\n\n(1 | dept)\n\n\ndepartments can vary in how stress changes over the week.\n\n(days | dept)\n\ndepartments can vary in how CBD changes stress levels. because each department contains some participants in the CBD group and some in the control group, “the effect of CBD on stress” does exist for a given department, and so could vary between departments. e.g. Philosophers taking CBD get really relaxed, but CBD doesn’t affect Mathematicians that much.\n\n(CBD | dept)\n\n\ndepartments can vary in how CBD affects changes in stress over the week\n\n( day*CBD | dept)\n\n\n\n\n\nNote that the above model is a singular fit, but it gives us a better place to start simplifying from. If we remove the day*CBD interaction in the by-department random effects, we get a model that converges:\n\nmnest2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nAnd plot our fitted values\n\n\nCode\nlibrary(broom.mixed)\naugment(mnest2) |&gt; \n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    # split by departments\n    facet_wrap(~dept) + \n    # make a line for each participant\n    geom_line(aes(group=pid),alpha=.3)+\n    # average fitted value for CBD vs control:  \n    stat_summary(geom=\"line\",aes(col=CBD),lwd=1)\n\n\n\n\n\n\n\n\nFigure 7: Plot of fitted values of the model. Individual lines for each participant, facetted by department. Thicker lines represent the department average fitted values split by CBD group\n\n\n\n\n\nAnd we can see in our summary that there is a lot of by-department variation - departments vary in their baseline stress levels with a standard deviation of 0.81, and within departments, participants vary in baseline stress scores with a standard deviation of 0.38.\n\nsummary(mnest2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n dept:pid (Intercept) 0.147661 0.38427             \n          day         0.012142 0.11019  -0.03      \n dept     (Intercept) 0.648410 0.80524             \n          day         0.001979 0.04449  -0.18      \n          CBDY        0.055388 0.23535   0.40 -0.22\n Residual             0.129765 0.36023             \nNumber of obs: 1200, groups:  dept:pid, 240; dept, 12\n...\nExamining ranef(mnest2) now gives us a list of dept:pid random effects, and then of dept random effects. We can plot them using dotplot.ranef.mer(), as seen below. From these, we can see for instance, that the effect of CBD is more negative for Theology, and Sociology and Maths have higher slopes of day. These map with the plot of fitted values we saw in Figure 7 - the department lines are going up more Math and Sociology than in other departments, and in Theology the blue CBD line is much lower relative to the red control line than in other departments.\n\ndotplot.ranef.mer(ranef(mnest2))$dept",
    "crumbs": [
      "7: Random Effect Structures"
    ]
  },
  {
    "objectID": "07_ranef.html#example-3-crossed",
    "href": "07_ranef.html#example-3-crossed",
    "title": "7: Random Effect Structures",
    "section": "Example 3: Crossed",
    "text": "Example 3: Crossed\nForgetting about participants nested in departments, let’s return to our sample of 20 staff members from the Psychology department. In our initial study design, we had just one self report measure of stress each day for each person.\nHowever, we might just as easily have taken more measurements. i.e. on Day 1, we could have recorded Martin’s stress levels 10 times. Furthermore, we could have used 10 different measurements of stress, rather than just a self-report measure. We could measure his cortisol levels, blood pressure, heart rate variability, give him different questionnaires, ask an informant like his son to report his stress, and so on. And we could have done the same for everybody.\n\nstresscross &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_crossed.csv\")\nhead(stresscross)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure          day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Aja   N     Alpha-Amylase      1  0.269\n2 Psych Aja   N     Blood Pressure     1  0.855\n3 Psych Aja   N     Cortisol           1  0.278\n4 Psych Aja   N     EEQ                1  0.470\n5 Psych Aja   N     HRV                1 -0.404\n6 Psych Aja   N     Informant          1  0.774\n\n\nIn this case, we can group our participants in two different ways. For each participant we have 5 datapoints for each of 10 different measures of stress. So we have 5x10 = 50 observations for each participant. But if we group them by measure instead, then we have each measure 5 times for 20 participants, so 5x20 = 100 observations of each measure. And there is no hierarchy here - the “blood pressure” measure is the same measure for Martin as it is for Dan and Aja etc. It makes sense to think of by-measure variability as not being ‘within-participants’.\nThis means we can choose when plotting whether to split the plots by participants, with a different line for each measure (Figure 8), or split by measure with a different line for each participant (Figure 9)\n\n\nfacet = participant, line = measure\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  #make a line for each measure\n  geom_line(aes(group=measure))+\n  facet_wrap(~pid)\n\n\n\n\n\n\n\n\nFigure 8: crossed designs with participants and measures. we can facet by participant and plot a line for each measure\n\n\n\n\n\n\n\nfacet = measure, line = participant\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  # make a line for each ppt\n  geom_line(aes(group=pid))+\n  facet_wrap(~measure)\n\n\n\n\n\n\n\n\nFigure 9: crossed designs with participants and measures. we can facet by measure and plot a line for each participant\n\n\n\n\n\n\n\nWe can fit a model that therefore accounts for the by-participant variation (“things vary between participants”) and the by-measure variation (“things vary between measures”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nstresscross$day &lt;- stresscross$day-1\n\nmcross &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nNote that just as with the nested example above, we can have different random slopes for measures vs those for participants, depending upon what effects can vary given the study design.\nAs before, removing the interaction in the random effects achieves model convergence:\n\nmcross2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nAnd again we might plot our fitted values either of the ways we plotted our initial data in Figure 8 above, only with the .fitted values obtained from the augment() function:\n\n\nCode\naugment(mcross2) |&gt;\n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    geom_point()+\n    geom_line(aes(group=pid))+\n    facet_wrap(~measure)\n\n\n\n\n\n\n\n\n\nOur random effect variances show the estimated variance in different terms (the intercept, slopes of day, effect of CBD) between participants, and between measures.\nFrom the below it is possible to see, for instance, that there is considerable variability between how measures respond to CBD (they vary in the effect of CBD on stress with a standard deviation of 0.53)\n\nsummary(mcross2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n pid      (Intercept) 0.316578 0.56265             \n          day         0.014693 0.12121  -0.51      \n measure  (Intercept) 0.087111 0.29515             \n          day         0.008542 0.09242   0.88      \n          CBDY        0.283635 0.53257  -0.10  0.11\n Residual             0.088073 0.29677             \nNumber of obs: 1000, groups:  pid, 20; measure, 10\n...\nAgain, our dotplots of random effects help to also show this picture. We can see that the measures of “blood pressure”, “alpha-amylase”, “cortisol”, and “HRV” all have more effects of CBD that are more negative. We can see this in our plot of fitted values - these measures look like CBD vs control differnce is greater than in other measures.\n\ndotplot.ranef.mer(ranef(mcross2))$measure",
    "crumbs": [
      "7: Random Effect Structures"
    ]
  },
  {
    "objectID": "10_centering.html",
    "href": "10_centering.html",
    "title": "10: Centering",
    "section": "",
    "text": "This reading:\n\nCentering and scaling predictors in single level regression\nGroup-mean centering predictors in the multilevel model to separate out “within”-group effects from “between”-group effects\nOptional: contextual effects and the Mundlak model",
    "crumbs": [
      "10: Centering"
    ]
  },
  {
    "objectID": "10_centering.html#centering-predictors-in-lm",
    "href": "10_centering.html#centering-predictors-in-lm",
    "title": "10: Centering",
    "section": "Centering predictors in lm()",
    "text": "Centering predictors in lm()\nThere are lots of ways we can transform a variable. For instance, we can transform something in millimeters to being in centimeters by dividing it by 10. We could transform a height variable into height above/below 2 meters variable by subtracting 2 meters from it.\nA couple of common transformations we have seen already are ‘centering’ and ‘standardising’:\n\nWhen we “center” a variable, we subtracting some number (often the mean of the variable) from every value. So if we ‘mean-center’ a variable measuring height in cm, and the mean height of my sample is 175cm, then a value of 190 becomes +15, and a value of 150 becomes -25, and so on.\n\nWhen we ‘standardise’ a variable, we mean-center it and then divide the resulting values by the standard deviation. So if the standard deviation of heights in my sample is 15, then the value of 190 becomes \\(\\frac{190-175}{15} = \\frac{15}{15} = 1\\), and the 150 becomes \\(\\frac{150-175}{15} = \\frac{-25}{15} = -1.67\\).\n\nHow does this choice affect the linear models we might be fitting? The short answer is that it doesn’t! The overall fit of lm() is not changed in any way when we apply these linear1 transformations to predictors or outcomes.\nHowever, transformations do change what we get out of our model:\n\nIf we re-center a predictor on some new value (such as the mean), then all this does is change what “zero” means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is “when all predictors are zero”. And we are changing what “zero” represents!\nWhen we scale a predictor, this will change the slope. Why? Because it changes what “moving 1” represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains exactly the same, we are only changing the units that we are using to expressing that slope.\n\nThe example below shows a model of heart rates (HR) predicted by hours slept (hrs_sleep). In Figure 1 you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don’t affect the model itself - the regression line (and the uncertainty in the line) is the same in each plot. We can see that re-centering changes what the intercept represents:\n\nIn the top left plot, “0” represents zero hours slept, so the intercept (big blue dot) is the estimated heart rate for someone who didn’t sleep at all.\nSimilarly, in the top right plot, “0” now represents the mean hours slept, so the intercept is the heart rate for someone who slept an average amount, and in the bottom right plot, “0” now represents 8 hours of sleep (the recommended amount).\nIn the bottom left plot (where hours slept is ‘standardized’), not only have we changed what “0” represents, but we have changed what moving “1” represents. Rather being an increase of 1 hour of sleep, in this plot it represents an increase of 1 standard deviation hours sleep (whatever that is for our sample - it looks to be about 2.5). This means our estimated slope is the change in heart rate when having 1 SD more hours sleep (approx 2.5).\n\n\n\n\n\n\n\n\n\nFigure 1: Centering and scaling predictors in linear regression models. Intercepts and their interpretation change when re-centered, and slope coefficients and their interpretation change when scaling, but the overall model stays the same.\n\n\n\n\n\nThe thing to note is that the lines themselves are all the same, because the models are all exactly the same. We can prove this to ourselves by comparing the 4 models:\n\n\nCode\nhrdat &lt;- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\n\n# original model:\nmod_orig &lt;- lm(HR ~ hrs_sleep, data = hrdat)\n# model with hrs_sleep mean centered\nmod_mc &lt;- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n# model with hrs_sleep standardised\nmod_z &lt;- lm(HR ~ scale(hrs_sleep), data = hrdat)\n# model with hrs_sleep centered on 8 hours the I() function\n# is a handy function that is just needed because the symbols\n# + and - normally get interprted in lm() as adding \n# and removing predictors. \nmod_8 &lt;- lm(HR ~ I(hrs_sleep-8), data = hrdat) \n\n# all models are identical fit\nanova(mod_orig, mod_mc, mod_z, mod_8)\n\n\nAnalysis of Variance Table\n\nModel 1: HR ~ hrs_sleep\nModel 2: HR ~ scale(hrs_sleep, scale = FALSE)\nModel 3: HR ~ scale(hrs_sleep)\nModel 4: HR ~ I(hrs_sleep - 8)\n  Res.Df    RSS Df   Sum of Sq F Pr(&gt;F)\n1     68 3524.5                        \n2     68 3524.5  0 -4.5475e-13         \n3     68 3524.5  0  0.0000e+00         \n4     68 3524.5  0  0.0000e+00         \n\n\n\n\n\n\n\n\nCentering when we have interactions\n\n\n\n\n\nWhen we have an interactions in a model such as lm(y~x+z+x:z), the individual coefficients for x and z are specifically the associations “when the other variable included in the interaction is zero”. Because re-centering a variable changes the meaning of “zero”, this means that these two coefficients will change.\nFor instance, a model of heart rates (HR) that includes an interaction between hrs_sleep and whether someone smokes, our coefficient for smoke estimates the difference in HR between smokers vs non-smokers who get zero hours of sleep (red to blue point in the left-hand plot of Figure 2). If we mean-center the hrs_sleep variable in our model, then it becomes the estimated difference in HR between smokers vs non-smokers who get the average hours of sleep (red to blue point in the right-hand plot of Figure 2).\n\n\n\n\n\n\n\n\nFigure 2: mean-centering a variable that is involved in an interaction will change the point at which the marginal effect of other variable is estimated at",
    "crumbs": [
      "10: Centering"
    ]
  },
  {
    "objectID": "10_centering.html#centering-predictors-in-multilevel-models",
    "href": "10_centering.html#centering-predictors-in-multilevel-models",
    "title": "10: Centering",
    "section": "Centering predictors in multilevel models",
    "text": "Centering predictors in multilevel models\nIn multilevel models, things can be a little bit different.\n\n\nFor one thing, there can be practical benefits to centering and/or scaling predictors with respect to actually fitting these models. Because multilevel models involve estimating group-level variability in intercepts and slopes, if our intercept is very far away from our data (e.g., if all our data is from ages 60 to 80, and we are estimating variability at age 0), then slight changes in a slope can have huge influences on estimated intercepts, resulting in models that don’t converge. We can see from the longitudinal example (Chapter 5), with the idea represented in Figure 3 - using raw age values the intercepts and slopes are highly correlated and the model won’t converge, but when we recenter the age variable on 60 and the intercept variation would become the variability in peoples’ cognition at the start of the study period, and the random intercepts are not so determined by the random slopes.\n\n\n\n\n\n\n\n\n\nFigure 3: lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the ‘intercept’ is estimated back at age 0\n\n\n\n\n\n\n\nHowever, in some cases (typically in observational, rather than experimental studies), having multi-level data may mean that we can actually transform a predictor in a couple of ways - we can center it on a constant number like the overall mean/min/max, but we can also consider transformations within each group. The key here is that we don’t always have just have one “overall” mean for a predictor, but often we have different means for each group.\n\nGroup mean centering\n\nDataset: lmm_bflpe.csv\nThese data are simulated based on the “Big-fish-little-pond” effect in educational literature.\nWe are interested in better understanding the relationship between school children’s grades and their academic self-concept (their self-perception of ability in specific and general academic disciplines).\nWe have data from 20 classes of children, capturing information on their grades at school (range 0 to 10), and a measure of academic self-concept:\n\nbfdat &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_bflpe.csv\")\nhead(bfdat)\n\n# A tibble: 6 × 4\n  grade class self_concept child\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1  5.14     1         1.54     1\n2  5.3      1         3.62     2\n3  3.91     1        -0.65     3\n4  4.48     1        -0.93     4\n5  5        1         0.28     5\n6  5.32     1         2.36     6\n\n\n\nIn initial exploratory plots, we can see that there is plenty of variation in childrens’ grades - we have children scoring 3-4, all the way up to scoring almost 9. Note also, however, that the class’s average grades also vary substantially. For instance, classes 8,9,10 and 11 all have a very high average grade.\n\n\nCode\nlibrary(patchwork)\nggplot(bfdat, aes(x=grade,y=self_concept))+\n  geom_point(alpha=.2) +\n\nggplot(bfdat,aes(x=class,y=grade))+\n  geom_jitter(height=0,width=.1,alpha=.2)+\n  stat_summary(geom=\"pointrange\")+\n  coord_flip()\n\n\n\n\n\n\n\n\n\nWhen we plot the individual childrens’ score of ‘self-concept’ against grades, the picture becomes a bit clearer once we separate by the classes, where we can see that within each class there is a fairly positive trend.\n\n\nCode\nggplot(bfdat,aes(x=grade,y=self_concept))+\n  geom_point(size=2,alpha=.4) +\n\nggplot(bfdat,aes(x=grade,y=self_concept))+\n  geom_point(size=2,alpha=.4)+\n  facet_wrap(~class)\n\n\n\n\n\n\n\n\n\nBy contrast, the relationship between children’s self-concept scores and the average grade of their class shows a different pattern:\n\n\nCode\nbfdat &lt;- \n  bfdat |&gt; \n    group_by(class) |&gt;\n    mutate(\n      grade_avg = mean(grade)\n    )\nggplot(bfdat,aes(x=grade_avg,y=self_concept))+\n  stat_summary(geom=\"pointrange\")+\n  labs(x=\"class average grade\")\n\n\n\n\n\n\n\n\n\nSo there are clearly two different things going on here!\n\nWe have a positive association between a children’s grades relative to their peers’ grades and their self-concept. This maybe makes sense - comparisons with other people around you will influence your feelings of self worth.\nWe almost see a negative association between the average grade of a child’s class and the child’s self-concept — i.e., children from classes with high grades tend to have slightly lower self-concept!\n\nIn the typical multilevel model that we might fit for this study (below), we just get out one single effect estimate, which represents the expected change in self-concept when a child’s grade increases by 1. But we have just seen how a child’s grades are driven by two things - their class as a whole, and their relative standing in their class.\n\nrsmod &lt;- lmer(self_concept ~ grade + (1 + grade | class),\n              data = bfdat)\nsummary(rsmod)\n\n\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -17.5685     1.8579  -9.456\ngrade         2.9327     0.3462   8.471\n\n\nWhat we want to do here is separate out effects that are “within” (i.e. having higher/lower grades than your classmates) from those that are “between” (i.e. being from a class with higher/lower grades than other classes). To get at these two effects, we are going to explicitly separate our predictor variable into two different parts:\n\nthe group average\nindividual deviations from the group average.\n\nWe can calculate these by first using group_by() to make the calculations be applied separately for each class, and then calculating the mean() grade (for each class), and the deviations for each child from their class’s average:\n\nbfdat &lt;- \n  bfdat |&gt; \n  group_by(class) |&gt;\n  mutate(\n    grade_avg = mean(grade),\n    grade_dev = grade - mean(grade)\n  )\n\nhead(bfdat)\n\n# A tibble: 6 × 6\n# Groups:   class [1]\n  grade class self_concept child grade_avg grade_dev\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  5.14     1         1.54     1      4.78     0.357\n2  5.3      1         3.62     2      4.78     0.517\n3  3.91     1        -0.65     3      4.78    -0.873\n4  4.48     1        -0.93     4      4.78    -0.303\n5  5        1         0.28     5      4.78     0.217\n6  5.32     1         2.36     6      4.78     0.537\n\n\nNote that the actual grade for each child can still be made from our two new columns, calculated as the avg_grade + grade_dev.\nSo let’s plot the association between each of these new variables and the self-concept scores:\n\n\nCode\nggplot(bfdat, aes(x=grade_avg, y=self_concept))+\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n\nggplot(bfdat, aes(x=grade_dev, y=self_concept))+\n  geom_point() +\n  geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\n\nSo we can see that there are two different things going on here - the effect on self-concept of being in a high-performing class, as well as the effect of performing higher for your class.",
    "crumbs": [
      "10: Centering"
    ]
  },
  {
    "objectID": "10_centering.html#the-within-between-model",
    "href": "10_centering.html#the-within-between-model",
    "title": "10: Centering",
    "section": "The within-between model",
    "text": "The within-between model\nNow that we have split up the variable grade into two parts (group average, and deviations-from-group-averages), we can actually put these in as separate predictors into our model!\nThis type of model is sometimes referred to as a “within-between” model. You can see below both the standard model with random slopes, and the ‘within-between’ model, in both lmer() syntax and in equation form.\nNote that we while we replace one predictor (x) with its two constituent parts (the group means of x and the deviations from those group means), it is only the within effect that we can have a random slope for. This will hopefully make sense when we think a little about it, because the group-means are “between groups” - having a random slope of group_mean_x|group is similar to the idea of handedness|person, because for a single group, we don’t have “an effect on y of that group having a high average x”, so we can’t consider it to be an effect that varies by-group.\n\n\nrandom slopes model\n\nlmer(y ~ 1 + x + (1 + x | g), data)\n\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nwithin-between model\n\ndata &lt;- data |&gt;\n  group_by(g) |&gt;\n  mutate(\n    x_avg = mean(x),\n    x_dev = x - mean(x)\n  )\n\nlmer(y ~ 1 + x_dev + x_avg + (1 + x_dev | g), data)\n\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot (x_{ij} - \\bar{x}_i) + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\n\nIn the context of our educational study of grades and self-concept in school children, we can fit a model that disaggregates within (grade relative to class) and between (class average grade) effects:\n\nwbmod &lt;- lmer(self_concept ~ grade_dev + grade_avg + \n                (1 + grade_dev|class), \n              data = bfdat)\n\nThe fixed effects from this model (below) now show two effects, as opposed to only one that we would get from our typical model:\n\n\ntypical random slopes model\n\n\n\n\n\n\n\n\nterm\nest\nSE\nt\n\n\n\n\n(Intercept)\n-17.57\n1.86\n-9.46\n\n\ngrade\n2.93\n0.35\n8.47\n\n\n\n\n\n\n\n\nwithin-between model\n\n\n\n\n\n\n\n\nterm\nest\nSE\nt\n\n\n\n\n(Intercept)\n3.46\n1.83\n1.89\n\n\ngrade_dev\n3.11\n0.35\n8.97\n\n\ngrade_avg\n-0.69\n0.30\n-2.33\n\n\n\n\n\n\n\n\n\n\nthe “within” effect: for every one grade higher a child is relative to their classmates, their self-concept is expected to increase by 3.11.\nthe “between” effect: for every 1 grade higher a class average is (and when a child’s relative standing in the class stays constant), a child’s self-concept is expected to decrease by -0.69.\n\nSo what exactly does the effect (the estimate of 2.93) from our more traditional model show here? Is it the within effect or the between effect? It’s actually a smushing together of both parts - it is the estimated effect on self-concept when a child’s grade increases by 1, but it is confounded by the fact that as childrens’ grades increase then their class average increases a bit too, meaning that the between effect pulls this back down. In short - it’s not actually a very useful estimate for us at all, because it conflates the two different effects.",
    "crumbs": [
      "10: Centering"
    ]
  },
  {
    "objectID": "10_centering.html#optional-contextual-effects-and-the-mundlak-model",
    "href": "10_centering.html#optional-contextual-effects-and-the-mundlak-model",
    "title": "10: Centering",
    "section": "Optional: contextual effects and the mundlak model",
    "text": "Optional: contextual effects and the mundlak model\nAlong with the within-between model, we could also choose to adjust for the group averages while continuing to use the original raw predictor in the model. This is often called the “Mundlak model” in reference to Yair Mundlak who wrote about it in the context of avoiding group-level confounding (see Chapter 9 #optional-extra-group-confounding).\nThe formulation is very similar to the within-between model, but we don’t use the “deviations from group means”, we simply use the original predictor along with the group means:\n\n\nwithin-between model\n\ndata &lt;- data |&gt;\n  group_by(g) |&gt;\n  mutate(\n    x_avg = mean(x),\n    x_dev = x - mean(x)\n  )\n\nlmer(y ~ 1 + x_dev + x_avg + (1 + x_dev | g), data)\n\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot (x_{ij} - \\bar{x}_i) + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nmundlak model\n\ndata &lt;- data |&gt;\n  group_by(g) |&gt;\n  mutate(\n    x_avg = mean(x)\n  )\n\nlmer(y ~ 1 + x + x_avg + (1 + x | g), data)\n\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\n We can fit the Mundlak formulation to our grades & self-concept data as follows:\n\nmlakmod &lt;- lmer(self_concept ~ grade + grade_avg + \n                  (1 + grade|class), \n                data = bfdat)\n\nThere are two things to note here when comparing the Mundlak formulation to the ‘within-between’ model. Firstly, these two models provide the same ‘within’ effect (the fixed effects of grade_dev and grade in the tables below), because they both get the effect of a child’s grade increasing by 1, while holding their class’s average grade constant.2 Secondly, the estimated effects for the grade_avg predictor differ substantially between the two models:\n\n\nWithin-between model\n\n\n\n\n\n\n\n\nterm\nest\nSE\nt\n\n\n\n\n(Intercept)\n3.46\n1.83\n1.89\n\n\ngrade_dev\n3.11\n0.35\n8.97\n\n\ngrade_avg\n-0.69\n0.30\n-2.33\n\n\n\n\n\n\n\n\nMundlak model\n\n\n\n\n\n\n\n\nterm\nest\nSE\nt\n\n\n\n\n(Intercept)\n3.28\n2.21\n1.48\n\n\ngrade\n3.10\n0.34\n9.12\n\n\ngrade_avg\n-3.67\n0.34\n-10.91\n\n\n\n\n\n\n\n\n\nThe difference here is that they are capturing two distinct effects. The within-between formulation captures a ‘between effect’ and the Mundlak formulation provides something that gets termed the “contextual effect”.\nThe key thing to distinguish between these two is to think about what is being “held constant”. In the within-between model, the effect of avg_grade is estimated while holding constant the child’s relative standing in the group. In the Mundlak model, the effect is estimated while holding constant a child’s actual grade.\nIt may help to think about this in terms of a single child. Suppose we have a child who has a grade of 5.14, from a class with an average of 4.783. So that child is 0.357 above their class average.\n\n\n# A tibble: 1 × 5\n# Groups:   class [1]\n  class child grade_avg grade_dev self_concept\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       \n1 i     j          4.78     0.357 y           \n\n\nBoth models are estimating “what would we expect to happen to the child’s self-concept if their class had an average of 5.783 instead?”\nThe within-between model estimates this but holds constant the child’s deviation above the average, so we’re comparing the scenario where the child is 0.357 above a class average of 5.783, as opposed to being 0.357 above a class average of 4.783.\n\n\n# A tibble: 2 × 6\n# Groups:   class [1]\n  class child grade_avg grade_dev grade self_concept\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 i     j          4.78     0.357  5.14 y           \n2 i     j          5.78     0.357  6.14 y-0.69      \n\n\nBy contrast, the Mundlak model holds constant the child’s actual grade, meaning that we’re comparing the scenario where the child has a grade of 5.14 and is in a class with an average of 5.783, as opposed to having that same grade of 5.14 but being in a class with an average of 4.783:\n\n\n# A tibble: 2 × 5\n# Groups:   class [1]\n  class child grade_avg grade self_concept\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 i     j          4.78  5.14 y           \n2 i     j          5.78  5.14 y-3.67      \n\n\nWe can think of this visually. Take a given child from a given class, and think about what would happen if their whole class average grade increased by 1. In Figure 4, the red line is “class 1” in our data, and the blue line is a counterfactual world of “class 1 if its average grade increased by 1”. The large red dot represents the expected self-concept for “child 1”.\nIn the within-between model, we’re estimating the self-concept difference for a child between the red (actual) and blue (counterfactual class with a higher average), but where the child stays the same amount “above average” in this counterfactual class. In the Mundlak model, we’re estimating the self-concept difference when a child stays at the same grade but is in a different context (is placed in a class where the average is 1 higher).\n\n\n        11         12         13         14         15         16         17 \n-0.6887448 -0.6887448 -0.6887448 -0.6887448 -0.6887448 -0.6887448 -0.6887448 \n        18         19         20 \n-0.6887448 -0.6887448 -0.6887448 \n\n\n        11         12         13         14         15         16         17 \n-0.5562254 -0.5562254 -0.5562254 -0.5562254 -0.5562254 -0.5562254 -0.5562254 \n        18         19         20 \n-0.5562254 -0.5562254 -0.5562254 \n\n\n\n\n\n\n\n\nFigure 4: These plots show an actual class (red) and the counterfactual scenario (blue) where the class average is 1 higher. The large dots show comparisons between a given child in these scenarios - the left-hand plot shows the child’s relative standing in the group staying the same (the within-between model estimates this), and the right-hand plot shows the child’s raw value staying the same (this is what the Mundlak model estimates)",
    "crumbs": [
      "10: Centering"
    ]
  },
  {
    "objectID": "10_centering.html#footnotes",
    "href": "10_centering.html#footnotes",
    "title": "10: Centering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthe fit of models does change if we apply a non-linear transformation, such as \\(x^2\\), \\(log(x)\\), etc., and this can sometimes be useful for studying effects that are more likely to be non-linear!↩︎\nwhen we exclude the random intercepts, the within-effects from both models are numerically identical, but there may be small differences due to the random slopes of x|g for the mundlak model and x_dev|g for the within-between model↩︎",
    "crumbs": [
      "10: Centering"
    ]
  },
  {
    "objectID": "06_poly.html",
    "href": "06_poly.html",
    "title": "6: Polynomial Growth in MLM",
    "section": "",
    "text": "This reading:\n\nThe basics of modelling non-linear change via polynomial terms.\nWalkthrough Example: Polynomial growth in multilevel context\n\nFor additional reading, Winter & Wieling, 2016 is pretty good (mainly focus on sections 1-3)\nWe have already seen in the last couple of weeks that we can use MLM to study something ‘over the course of X’. This might be “over the course of adolescence” (i.e. y ~ age), or “over the course of an experiment” (y ~ trial_number). The term “longitudinal” is commonly used to refer to any data in which repeated measurements are taken over a continuous domain. This opened up the potential for observations to be unevenly spaced, or missing at certain points.\nIt also, as will be the focus of this week, opens the door to thinking about how many effects of interest may display patterns that are non-linear. There are lots of techniques to try and summarise non-linear trajectories, and here we are going to focus on the method of including higher-order polynomials as predcitors.",
    "crumbs": [
      "6: Polynomial Growth in MLM"
    ]
  },
  {
    "objectID": "06_poly.html#raw-polynomials",
    "href": "06_poly.html#raw-polynomials",
    "title": "6: Polynomial Growth in MLM",
    "section": "Raw Polynomials",
    "text": "Raw Polynomials\nThere are two types of polynomial we can construct. “Raw” (or “Natural”) polynomials are the straightforward ones that you would expect (example in the table below), where the original value of \\(x\\) is squared/cubed.\n\n\n\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n1\n1\n1\n\n\n2\n4\n8\n\n\n3\n9\n27\n\n\n4\n16\n64\n\n\n5\n25\n125\n\n\n…\n…\n…\n\n\n\nWe can quickly get these in R using the poly() function. As we want to create “raw” polynomials, we need to make sure to specify raw = TRUE or we get something else (we’ll talk about what they are in a second!).\n\npoly(1:10, degree = 3, raw=TRUE)\n\n       1   2    3\n [1,]  1   1    1\n [2,]  2   4    8\n [3,]  3   9   27\n [4,]  4  16   64\n [5,]  5  25  125\n [6,]  6  36  216\n [7,]  7  49  343\n [8,]  8  64  512\n [9,]  9  81  729\n[10,] 10 100 1000\nattr(,\"degree\")\n[1] 1 2 3\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nLet’s now use these with our example data we had been plotting above.\nFirst lets add new variables to the dataset, which are the polynomials of our \\(x\\) variable:\n\nsyndat &lt;- \n  syndat |&gt; \n    mutate(\n      # poly1 is the first column\n      poly1 = poly(age, degree = 3, raw = TRUE)[,1],\n      # poly2 is the second\n      poly2 = poly(age, degree = 3, raw = TRUE)[,2],\n      # poly3 is the third\n      poly3 = poly(age, degree = 3, raw = TRUE)[,3]\n    )\n\nAnd now lets use them in our model as predictors:\n\ncubicmod &lt;- lm(syndens ~ poly1 + poly2 + poly3, data = syndat)\n\n\n\n\n\n\n\nother ways to get polynomials into the model\n\n\n\n\n\nAs we’re working with raw polynomials, we could just do:\n\nsyndat |&gt; \n  mutate(\n    poly1 = age,\n    poly2 = age^2,\n    poly3 = age^3\n  )\n\nOr we could even just specify the calculations for each term inside the call to lm():\n\nlm(syndens ~ age + I(age^2) + I(age^3), data = syndat)\n\nOr even use the poly() function:\n\nlm(syndens ~ poly(age, degree=3, raw=TRUE), data = syndat)\n\n\n\n\n\n\n\n\n\n\nA handy function from Dan\n\n\n\n\n\nDan has a nice function that may be handy. It adds the polynomials to your dataset for you:\n\n# import Dan's code and make it available in our own R session\n# you must do this in every script you want to use this function\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\nsyndat &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_synapticdens.csv\")\nsyndat &lt;- code_poly(df = syndat, predictor = 'age', poly.order = 3, \n                    orthogonal = FALSE, draw.poly = FALSE)\nhead(syndat)\n\n# A tibble: 6 × 6\n    age syndens age.Index poly1 poly2 poly3\n  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.1   0.615         1   0.1  0.01 0.001\n2   0.2   0.908         2   0.2  0.04 0.008\n3   0.6   0.1           3   0.6  0.36 0.216\n4   0.7   1.81          4   0.7  0.49 0.343\n5   0.9   1.44          5   0.9  0.81 0.729\n6   1     0.615         6   1    1    1    \n\n\n\n\n\nJust to see it in action, let’s take a look at the predicted values from our model.\nTake for instance, the 9th row below. The predicted value of y (shown in the .fitted column) is:\n\\(\\hat y_9 = b_0 + b_1 \\cdot x_9 + b_2 \\cdot x^2_9 + b_3 \\cdot x^3_9\\)\n\\(\\hat y_9 = b_0 + b_1 \\cdot 2 + b_2 \\cdot 4 + b_3 \\cdot 8\\)\n\\(\\hat y_9 = -1.843 + 3.375 \\cdot 2 + -0.332 \\cdot 4 + 0.0097 \\cdot 8\\)\n\\(\\hat y_9 = 3.66\\).\n\nlibrary(broom)\naugment(cubicmod) \n\n# A tibble: 74 × 10\n   syndens poly1 poly2 poly3 .fitted  .resid   .hat .sigma   .cooksd .std.resid\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1   0.615   0.1  0.01 0.001 -1.51    2.12   0.170    1.62 0.104         1.42  \n 2   0.908   0.2  0.04 0.008 -1.18    2.09   0.155    1.63 0.0886        1.39  \n 3   0.1     0.6  0.36 0.216  0.0651  0.0349 0.109    1.65 0.0000155     0.0226\n 4   1.81    0.7  0.49 0.343  0.361   1.45   0.0992   1.64 0.0240        0.933 \n 5   1.44    0.9  0.81 0.729  0.933   0.510  0.0829   1.65 0.00239       0.326 \n 6   0.615   1    1    1      1.21   -0.596  0.0759   1.65 0.00294      -0.379 \n 7   0.615   1.6  2.56 4.10   2.75   -2.13   0.0465   1.63 0.0217       -1.33  \n 8   0.310   1.7  2.89 4.91   2.98   -2.67   0.0433   1.62 0.0316       -1.67  \n 9   1.21    2    4    8      3.66   -2.45   0.0361   1.62 0.0217       -1.52  \n10   2.19    2.1  4.41 9.26   3.87   -1.68   0.0343   1.64 0.00968      -1.04  \n# ℹ 64 more rows\n\n\nIf we plot the predictions with poly1 on the x-axis (poly1 is just the same as our age variable with a different name!), we can see that we are able to model a non-linear relationship between y and x (between synaptic density and age), via a combination of linear parameters!\n\nlibrary(broom)\naugment(cubicmod, interval=\"confidence\") |&gt;\n  ggplot(aes(x=poly1))+\n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n  geom_line(aes(y=.fitted),col=\"darkorange\") +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper),fill=\"darkorange\", alpha=.2)+\n  labs(x=\"age\") # our x-axis, \"poly1\", is just age!  \n\n\n\n\n\n\n\nFigure 6: a cubic model\n\n\n\n\n\nNow lets look at our coefficients:\n\nsummary(cubicmod)\n\n...\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.842656   0.704193  -2.617   0.0109 *  \npoly1        3.375159   0.345570   9.767 1.06e-14 ***\npoly2       -0.331747   0.044664  -7.428 2.06e-10 ***\npoly3        0.009685   0.001614   6.001 7.79e-08 ***\n---\nWith polynomials the interpretation is a little tricky because we have 3 coefficients that together explain the curvy line we see in Figure 6, and these coefficients are all dependent upon one another.\n\n(Intercept) = When all predictors are zero, i.e. the synaptic density at age 0.\n\npoly1 coefficient = The instantaneous change in \\(y\\) when \\(x=0\\).\npoly2 coefficient = Represents “rate of change of the rate of change” at \\(x=0\\). In other words, the curvature at \\(x=0\\).\n\npoly3 coefficient = Represents how the curvature is changing. It gets more abstract as the order of polynomials increase, so the easiest way to think about it is “the wiggliness”\n\nI’ve tried to represent what each term adds in Figure 7. The intercept is the purple point where age is zero. The poly1 coefficient is represented by the dashed blue line - the tangent of the curve at age zero. The poly2 coef, rperesented by the dashed green line, is how the angle of the blue line is changing at age zero. Finally, the poly3 coefficient tells us how much this curvature is changing (which gets us to our dashed orange line).\nNote that these interpretations are all dependent upon the others - e.g. the interpretation of poly2 refers to how the angle of poly1 is changing.\n\n\n\n\n\n\n\n\nFigure 7: the instantaneous rate of change at x=0 (blue), the rate of change in the rate of change (i.e. curvature, green), and ‘rate of change in rate of change in rate of change’ (i.e. wiggliness, orange)",
    "crumbs": [
      "6: Polynomial Growth in MLM"
    ]
  },
  {
    "objectID": "06_poly.html#orthogonal-polynomials",
    "href": "06_poly.html#orthogonal-polynomials",
    "title": "6: Polynomial Growth in MLM",
    "section": "Orthogonal Polynomials",
    "text": "Orthogonal Polynomials\nThe poly() function also enables us to compute “orthogonal polynomials”. This is the same information as the raw polynomials, re-expressed into a set of uncorrelated variables.\nRaw polynomials are correlated, which is what results makes their interpretation depend upon one another. For example, if we take the numbers 1,2,3,4,5, then these numbers are by definition correlated with their squares 1,4,9,16,25. As we increase from 1 to 5, we necessarily increase from 1 to 25.\nHowever, if we first center the set of numbers, so that 1,2,3,4,5 becomes -2,1,0,1,2, then their squares are 4,1,0,1,4 - they’re not correlated!\nOrthogonal polynomials essentially do this centering and scaling for \\(k\\) degrees of polynomial terms.\n\n\nSo while raw polynomials look like this:\n\nmatplot(poly(1:10, 3, raw=T), type=\"l\", lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nOrthogonal polynomials look like this:\n\nmatplot(poly(1:10, 3, raw=F), type=\"l\", lwd=2)\n\n\n\n\n\n\n\n\n\n\nThis orthogonality allows us to essentially capture express the linear trend, curvature, and ‘wiggliness’ of the trajectory independently from one another, rather than relative to one another.\nUltimately, models using raw polynomials and using orthogonal polynomials are identical, but the coefficients we get out represent different things.\nLet’s overwrite our poly variables with orthogonal polynomials, by setting raw = FALSE:\n\nsyndat &lt;- \n  syndat |&gt; \n    mutate(\n      poly1 = poly(age,degree = 3, raw=FALSE)[,1],\n      poly2 = poly(age,degree = 3, raw=FALSE)[,2],\n      poly3 = poly(age,degree = 3, raw=FALSE)[,3],\n    )\n\nAnd fit our model:\n\nOcubicmod &lt;- lm(syndens ~poly1+poly2+poly3,syndat)\nsummary(Ocubicmod)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.5917     0.1902  34.649  &lt; 2e-16 ***\npoly1        12.9161     1.6365   7.892 2.88e-11 ***\npoly2       -14.3156     1.6365  -8.748 7.68e-13 ***\npoly3         9.8212     1.6365   6.001 7.79e-08 ***\n---\nThe interpretation of the estimates themselves are not really very tangible anymore, because the scaling of the orthogonal polynomials has lost a clear link back to “age”.\nAs the polynomial terms are centered on the mean of age, the intercept is the estimated synaptic density at the mean age (the purple dot in Figure 8). The poly1, poly2 and poly3 coefficient represent the independent overall linear trend, centered curvature, and “wiggliness” of the relationship between synaptic density and age (as shown in the blue, green and orange lines in Figure 8 respectively).\n\n\n\n\n\n\n\n\nFigure 8: the independent rate of change (blue), curvature (green) and wiggliness (orange) of the y~x relationship",
    "crumbs": [
      "6: Polynomial Growth in MLM"
    ]
  },
  {
    "objectID": "06_poly.html#raw-vs-orthognal",
    "href": "06_poly.html#raw-vs-orthognal",
    "title": "6: Polynomial Growth in MLM",
    "section": "Raw vs Orthognal",
    "text": "Raw vs Orthognal\nThe two models we have seen, one with raw polynomials, and one with orthogonal polynomials, are identical.\nFor proof, compare the two:\n\nanova(\n  lm(syndens ~ poly(age, 3, raw = TRUE), data = syndat),\n  lm(syndens ~ poly(age, 3, raw = FALSE), data = syndat)\n)\n\nAnalysis of Variance Table\n\nModel 1: syndens ~ poly(age, 3, raw = TRUE)\nModel 2: syndens ~ poly(age, 3, raw = FALSE)\n  Res.Df    RSS Df   Sum of Sq F Pr(&gt;F)\n1     70 187.47                        \n2     70 187.47  0 -1.1369e-13         \n\n\nSo why would we choose one vs the other?\nThe main reason is if we are interested in evaluating things relative to baseline, in which case raw polynomials allow us to do just that. If we are instead interested in evaluating the trends across the timecourse, then we would want orthogonal polynomials.\nConsider two examples:\n\n\nExample 1\nA student advisor who meets with students as they start university wants to know about how happiness evolves over the course of students’ year at univeristy, and wonders if this is different between introverted and extraverted individuals.\nIn this case, they would want raw polynomials, so that they can assess whether the two personality types differ when they first come to University, and how this is likely to evolve from that point.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\nA company has four stores across the UK, and they want to know if the stores have differed in how variable their earnings have been across the year.\nIn this case, looking at change relative to month 1 isn’t very useful. It would, for instance, tell us that the linear trend for store2’s earnings is upwards, whereas the linear trend for store 1 is flat. This makes store2 look better.\nIf we used orthogonal polynomials instead, we would see that the linear trend for store 2 is actually negative compared to store1.\n\n\n\n\n\n\n\n\n\n\n\n\nRaw? Orthogonal?\nFor non-linear relationships, a good plot is usually the most important thing!",
    "crumbs": [
      "6: Polynomial Growth in MLM"
    ]
  },
  {
    "objectID": "00_datasets.html",
    "href": "00_datasets.html",
    "title": "Practice Datasets",
    "section": "",
    "text": "Below are various datasets from these readings and from lectures and exercises across our courses. For each one, there is a quick explanation of the study design which also details the research aims of the project.\nPick one of the datasets to test yourself with fitting, checking, interpreting, and reporting on multilevel models.\n\n\n\n\n\n\n\nschoolmot.csv - motivation and grades in school children #cross-sectional\n\n\n\n\n\nThis dataset contains information on 900 children from 30 different schools across Scotland. The data was collected as part of a study looking at whether education-related motivation is associated with school grades. This is expected to be different for state vs privately funded schools.\nAll children completed an ‘education motivation’ questionnaire, and their end-of-year grade average has been recorded.\nData are available at https://uoepsy.github.io/data/schoolmot.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nmotiv\nChild's Education Motivation Score (range 0 - 10)\n\n\nfunding\nFunding ('state' or 'private')\n\n\nschoolid\nName of School that the child attends\n\n\ngrade\nChild's end-of-year grade average (0-100)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n\nmod1 &lt;- lmer(grade ~ motiv * funding + \n               (1 + motiv | schoolid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_lifesatscot.csv - #cross-sectional\n\n\n\n\n\nThese data come from 112 people across 12 different Scottish dwellings (cities and towns). Information is captured on their ages and a measure of life satisfaction. The researchers are interested in if there is an association between age and life-satisfaction.\nData are available at https://uoepsy.github.io/data/lmm_lifesatscot.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nage\nAge (years)\n\n\nlifesat\nLife Satisfaction score\n\n\ndwelling\nDwelling (town/city in Scotland)\n\n\nsize\nSize of Dwelling (&gt; or &lt;100k people)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_lifesatscot.csv\")\n\nmod &lt;- lmer(lifesat ~ 1 + age + (1 + age | dwelling), df)\n\n# if you want to see a cross-level interaction (not relevant for RQ):\ndf$age &lt;- df$age/10 # makes fitting easier\nmod2 &lt;- lmer(lifesat ~ 1 + age * size + (1 + age | dwelling), df)\n\n\n\n\n\n\n\n\n\n\nlmm_jsup.csv - Workplace pride#cross-sectional\n\n\n\n\n\nA questionnaire was sent to all UK civil service departments, and the lmm_jsup.csv dataset contains all responses that were received. Some of these departments work as hybrid or ‘virtual’ departments, with a mix of remote and office-based employees. Others are fully office-based.\nThe questionnaire included items asking about how much the respondent believe in the department and how it engages with the community, what it produces, how it operates and how treats its people. A composite measure of ‘workplace-pride’ was constructed for each employee. Employees in the civil service are categorised into 3 different roles: A, B and C. The roles tend to increase in responsibility, with role C being more managerial, and role A having less responsibility. We also have data on the length of time each employee has been in the department (sometimes new employees come straight in at role C, but many of them start in role A and work up over time).\nWe’re interested in whether the different roles are associated with differences in workplace-pride.\nData are available at https://uoepsy.github.io/data/lmm_jsup.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndepartment_name\nName of government department\n\n\ndept\nDepartment Acronym\n\n\nvirtual\nWhether the department functions as hybrid department with various employees working remotely (1), or as a fully in-person office (0)\n\n\nrole\nEmployee role (A, B or C)\n\n\nseniority\nEmployees seniority point. These map to roles, such that role A is 0-4, role B is 5-9, role C is 10-14. Higher numbers indicate more seniority\n\n\nemployment_length\nLength of employment in the department (years)\n\n\nwp\nComposite Measure of 'Workplace Pride'\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\n# either Roles or Seniority would work here:\nmod &lt;- lmer(wp ~ employment_length + seniority + (1 + seniority | dept), df)\n\n# doesn't converge:\nmod &lt;- lmer(wp ~ employment_length + role + (1 + role | dept), df)\nmod &lt;- lmer(wp ~ employment_length + role + (1 | dept), df)\n\n\n\n\n\n\n\n\n\n\nwellbeingwork3.rda - work patterns and mental wellbeing #longitudinal\n\n\n\n\n\nThe “Wellbeing in Work” dataset contains information on employee wellbeing, assessed at baseline (start of study), 12 months post, 24 months post, and 36 months post. over the course of 36 months. Participants were randomly assigned to one of three employment conditions:\n\ncontrol: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.\n\nunlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description.\nfourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nThe researchers have two main questions: Overall, did the participants’ wellbeing stay the same or did it change? Did the employment condition groups differ in the how wellbeing changed over the assessment period?\nData are available (in .rda format) at https://uoepsy.github.io/data/wellbeingwork3.rda\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nID\nParticipant ID\n\n\nTimePoint\nTimepoint (0 = baseline, 1 = 12 months, 2 = 24 months, 3 = 36 months)\n\n\nCondition\nEmployment Condition ('control' = 5 day week, 28 days of leave. 'unlimited_leave' = 5 days a week, unlimited leave. 'fourday_week' = 4 day week, 28 days of leave)\n\n\nWellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n\n\n\n\n\n\n\n\n\n\n\n\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\n\nmod1 &lt;- lmer(Wellbeing~TimePoint+(1+TimePoint|ID), \n             data = wellbeingwork3)\nmod2 &lt;- lmer(Wellbeing~TimePoint+Condition+(1+TimePoint|ID), \n             data = wellbeingwork3)\nmod3 &lt;- lmer(Wellbeing~TimePoint*Condition+(1+TimePoint|ID), \n             data = wellbeingwork3)\nanova(mod1,mod2,mod3)\n\nsummary(mod3)\n\n\n\n\n\n\n\n\n\n\ntoy2.csv - Toys!#cross-sectional\n\n\n\n\n\nThis example builds on one from the USMR course, where the lectures explored linear regression with a “toy dataset” looking at how hours of practice influences the reading age of different toy characters (see USMR Week 7 Lecture). Here, we broaden our scope to the investigation of how practice affects reading age for all toys (not just Martin’s Playmobil characters).\nData are available at https://uoepsy.github.io/data/toy2.csv containing information on 129 different toy characters that come from a selection of different families/types of toy.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ntoy_type\nType of Toy\n\n\nyear\nYear Released\n\n\ntoy\nCharacter\n\n\nhrs_week\nHours of practice per week\n\n\nR_AGE\nReading Age\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/toy2.csv\")\n\nmod &lt;- lmer(R_AGE ~ 1 + hrs_week + (1 + hrs_week | toy_type), df)\n\n\n\n\n\n\n\n\n\n\nLAAwellbeing.csv - mental wellbeing across Scotland#cross-sectional\n\n\n\n\n\nResearchers want to study the relationship between time spent outdoors and mental wellbeing, across all of Scotland. They contact all the Local Authority Areas (LAAs) and ask them to collect data for them, with participants completing the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being, and being asked to estimate the average number of hours they spend outdoors each week. Twenty of the Local Authority Areas provided data.\nData are available at https://uoepsy.github.io/data/LAAwellbeing.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier\n\n\nname\nParticipant Name\n\n\nlaa\nLocal Authority Area\n\n\noutdoor_time\nNumber of hours spent outdoors per week\n\n\nwellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n\n\ndensity\nPopulation density of local authority area (number of people per square km)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n\nmod1 &lt;- lmer(wellbeing ~ density + outdoor_time + \n       (1 + outdoor_time | laa), \n       data = df) \n\nsummary(mod1)\n\neaster egg: check for influential people!\n\n\n\n\n\n\n\n\n\nstressint.csv - CBT and stress levels#longitudinal\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in changes in stress across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier\n\n\nstress\nStress (range 0 to 100)\n\n\ntime\nTime (pre/post/during)\n\n\ngroup\nWhether participant is in the CBT group or control group\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressint.csv\")\ndf$time &lt;- factor(df$time, levels=c(\"Pre\",\"During\",\"Post\"))\n\nTemptation is to fit the below, but it won’t work, because each pid has only 1 obs for each time-point, so we’re overfitting (the whole Error: number of observations (=150) &lt;= number of random effects (=150) for term message).\n\nmod1 &lt;- lmer(stress ~ time*group + \n               (1 + time|ppt), \n             data = df)\n\n\nmod1 &lt;- lmer(stress ~ time*group + \n               (1 |ppt), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\ndrivingmusicwithin.csv - the influence of music on driving speeds#repeated-measures\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npid\nParticipant Identifier\n\n\nspeed\nAvg Speed Driven on Route (mph)\n\n\nmusic\nMusic listened to while driving (classical music / rap music / spoken word)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/drivingmusicwithin.csv\")\n\nTemptation is to fit the below, but it won’t work, because each pid has only 1 obs for each music, so we’re overfitting\n\nmod1 &lt;- lmer(speed ~ music + \n               (1 + music | pid), \n             data = df)\n\n\nmod1 &lt;- lmer(speed ~ music + \n               (1 | pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\ndapr3_mannequin.csv - the role of mannequins in clothing purchases#repeated-measures\n\n\n\n\n\nDoes clothing seem more attractive to shoppers when it is viewed on a model, and is this dependent on item price? 30 participants were presented with a set of pictures of items of clothing, and rated each item how likely they were to buy it. Each participant saw 20 items, ranging in price from £5 to £100. 15 participants saw these items worn by a model, while the other 15 saw the items against a white background.\nData are available at https://uoepsy.github.io/data/dapr3_mannequin.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npurch_rating\nPurchase Rating (sliding scale 0 to 100, with higher ratings indicating greater perceived likelihood of purchase)\n\n\nprice\nPrice presented with item (range £5 to £100)\n\n\nppt\nParticipant Identifier\n\n\ncondition\nWhether items are seen on a model or on a white background\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/dapr3_mannequin.csv\")\n\n#scale price to help convergence\n#change 1 in price is now change of £10\ndf$price &lt;- df$price/10\n\nmod1 &lt;- lmer(purch_rating ~ price*condition + \n       (1+price|ppt), \n       data = df)\n  \nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: purch_rating ~ price * condition + (1 + price | ppt)\n   Data: df\n\nREML criterion at convergence: 4754.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7237 -0.6389  0.0491  0.6836  3.2354 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ppt      (Intercept)  59.361   7.7046       \n          price         0.713   0.8444  -0.78\n Residual             148.039  12.1671       \nNumber of obs: 600, groups:  ppt, 30\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)           41.2807     2.4672  16.732\nprice                  2.4767     0.3270   7.575\nconditionmodel        -1.8533     3.4891  -0.531\nprice:conditionmodel   1.1600     0.4624   2.509\n\nCorrelation of Fixed Effects:\n            (Intr) price  cndtnm\nprice       -0.804              \nconditinmdl -0.707  0.568       \nprc:cndtnmd  0.568 -0.707 -0.804\n\n\n\n\n\n\n\n\n\n\n\ncrqeds.csv - routine and emotion dysregulation in children#cross-sectional\n\n\n\n\n\nAre children with more day-to-day routine better at regulating their emotions? A study of 200 children from 20 schools (9 private schools and 11 state schools) completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ).\nData are available at https://uoepsy.github.io/data/crqeds.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nschoolid\nSchool Identifier\n\n\nEDS\nEmotion Dysregulation Score (range 1-6, higher values indicate more *dys*regulation of emotions)\n\n\nCRQ\nChildhood Routine Questionnaire Score (range 0-7, higher values indicate more day-to-day routine)\n\n\nschooltype\nSchool type (private / state)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/crqeds.csv\")\n\nmod1 &lt;- lmer(EDS ~ schooltype + CRQ + \n       (1 + CRQ | schoolid), \n       data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_ef_sdmt.csv - Audio interference in executive functioning#repeated-measures\n\n\n\n\n\nThis data is from a simulated study that aims to investigate the following research question:\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n30 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room. Unfortunately, lots of the tests were not administered correctly, and so not every participant has the full 15 trials worth of data.\nData are available at https://uoepsy.github.io/data/lmm_ef_sdmt.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\naudio\nAudio heard during the test ('no_audio', 'white_noise','music')\n\n\nheadphones\nWhether the participant listened via speakers (S) in the room or via noise cancelling headphones (H)\n\n\nSDMT\nSymbol Digit Modalities Test (SDMT) score\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_ef_sdmt.csv\")\n\nmod1 &lt;- lmer(SDMT ~ audio * headphones + \n               (1 + audio | PID), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_nssjobsat.csv - #cross-sectional\n\n\n\n\n\nLet’s suppose we are studying employee job satisfaction at the university, and we want to estimate the association between pay-scale and job satisfaction, controlling for the NSS rating of departments.\nWe have 399 employees from 25 different departments, and we got them to fill in a job satisfaction questionnaire, and got information on what their payscale was. We have also taken information from the national student survey on the level of student satisfaction for each department.\nEach datapoint here represents an individual employee, and these employees are grouped into departments.\nData are available at https://uoepsy.github.io/data/msmr_nssjobsat.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nNSSrating\nNational Student Satisfaction Rating for the Department\n\n\ndept\nDepartment name\n\n\npayscale\nPay scale of employee\n\n\njobsat\nJob satisfaction of employee\n\n\njobsat_binary\nBinary question of whether the employee considered themselves to be satisfied with their work (1) or not (0)\n\n\n\n\n\n\n\n\n\n\n\n\ndf&lt;-read_csv(\"https://uoepsy.github.io/data/msmr_nssjobsat.csv\")\n\nmod &lt;- lmer(jobsat ~ 1 + NSSrating + payscale + (1 + payscale | dept), df)\n\n\n\n\n\n\n\n\n\n\nlmm_apespecies.csv & lmm_apeage.csv - dominance in adolescence of great apes#longitudinal\n\n\n\n\n\nWe have data from a large sample of great apes who have been studied between the ages of 1 to 10 years old (i.e. during adolescence). Our data includes 4 species of great apes: Chimpanzees, Bonobos, Gorillas and Orangutans. Each ape has been assessed on a primate dominance scale at various ages. Data collection was not very rigorous, so apes do not have consistent assessment schedules (i.e., one may have been assessed at ages 1, 3 and 6, whereas another at ages 2 and 8).\nThe researchers are interested in examining how the adolescent development of dominance in great apes differs between species.\nData on the dominance scores of the apes are available at https://uoepsy.github.io/data/lmm_apeage.csv and the information about which species each ape is are in https://uoepsy.github.io/data/lmm_apespecies.csv.\n\n\n\n\n\n\nTable 1: Data Dictionary: lmm_apespecies.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nape\nApe Name\n\n\nspecies\nSpecies (Bonobo, Chimpanzee, Gorilla, Orangutan)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Data Dictionary: lmm_apeage.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nape\nApe Name\n\n\nage\nAge at assessment (years)\n\n\ndominance\nDominance (Z-scored)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndfape1 &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_apespecies.csv\")\ndfape2 &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_apeage.csv\")\n\ndf &lt;- full_join(dfape1, dfape2)\n\nsome cleaning is needed:\n\ndf &lt;- df |&gt; \n  mutate(\n    # fix species typos\n    species = case_when(\n      species %in% c(\"chimp\",\"chimpanzee\") ~ \"chimp\",\n      species %in% c(\"gorilla\",\"gorrila\") ~ \"gorilla\",\n      TRUE ~ species\n    )\n  ) |&gt;\n    filter(\n      # get rid of ages -99\n      age &gt; 0, \n      # keep when dominance is between -5 and 5 \n      # (5 here is a slightly arbitrary choice, but you can see from\n      # our checks that this will only exclude the two extreme datapoints\n      # that are 21.2 and 19.4\n      (dominance &lt; 5 & dominance &gt; -5) \n    )\n\nmodel comparison answers “do species differ in growth of dominance?”\nfor “how do specific species differ?” we look at fixed effects.\n\nmod &lt;- lmer(dominance ~ age * species + (1 + age | ape), df)\nmod.rstr &lt;- lmer(dominance ~ age + species + (1 + age | ape), df)\n\nanova(mod.rstr, mod)\n\n\n\n\n\n\n\n\n\n\nlmm_mindfuldecline.csv - mindfulness and cognitive decline#longitudinal\n\n\n\n\n\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.\nData are available at https://uoepsy.github.io/data/lmm_mindfuldecline.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nsitename\nSite Identifier\n\n\nppt\nParticipant Identifier\n\n\ncondition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n\n\nvisit\nStudy Visit Number (1 - 10)\n\n\nage\nAge (in years) at study visit\n\n\nACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n\n\nimp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_mindfuldecline.csv\")\n\nif we don’t recenter age, then the intercept variability (1|ppt) is estimated way back 60 years before we collected any data. The slopes will basically perfectly predicted these intercept differences, so the model will be singular.\nre-centering will help.\n\n#recenter age\ndf$ageC &lt;- df$age-60\n\nmod1 &lt;- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_mindfuldeclineFULL.csv - Multi-center Mindful Cognitive Aging#longitudinal#more-complex-groupings\n\n\n\n\n\nA large study involving 14 different research centers is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. Each site recruits between 15 and 30 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). For each center, roughly half of the participants engaged with daily mindfulness sessions, while the remaining participants did not.\nData are available at https://uoepsy.github.io/data/lmm_mindfuldeclineFULL.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nsitename\nSite Identifier\n\n\nppt\nParticipant Identifier\n\n\ncondition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n\n\nvisit\nVisit number (1 - 10)\n\n\nage\nAge (years) at visit\n\n\nACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n\n\nimp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n\n\n\n\n\n\n\n\n\n\n\nas above but it’s nested\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_mindfuldeclineFULL.csv\")\ndf$ageC &lt;- df$age-60\n\n# maximal model won't converge\nmmod &lt;- lmer(ACE ~ 1 + ageC * condition + \n              ( 1 + ageC * condition | sitename) +\n               (1 + ageC | sitename:ppt), \n             data = df)\n\n# probably simplify to \nmod &lt;- lmer(ACE ~ 1 + ageC * condition + \n              ( 1 + ageC | sitename) +\n               (1 + ageC | sitename:ppt), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nstressweek1.csv - CBD drinks and stress levels: Version 1#longitudinal\n\n\n\n\n\nSuppose that we conducted an experiment on a sample of 20 staff members from the Psychology department to investigate effects of CBD consumption on stress over the course of the working week. Participants were randomly allocated to one of two conditions: the control group continued as normal, and the CBD group were given one CBD drink every day. Over the course of the working week (5 days) participants stress levels were measured using a self-report questionnaire.\nData are available at https://uoepsy.github.io/data/stressweek1.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndept\nDepartment\n\n\npid\nParticipant Name\n\n\nCBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n\n\nmeasure\nMeasure used to assess stress levels\n\n\nday\nDay of the working week (1 to 5)\n\n\nstress\nStress Level (standardised)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek1.csv\")\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n               (1 + day | pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nstressweek_nested.csv - CBD drinks and stress levels: Version 2#longitudinal#more-complex-groupings\n\n\n\n\n\nAs for Version 1 of this study (see above), but instead of a sample of 20 participants from the psychology staff, we have 240 people from various departments such as History, Philosophy, Art, etc..\nData are available at https://uoepsy.github.io/data/stressweek_nested.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndept\nDepartment\n\n\npid\nParticipant Name\n\n\nCBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n\n\nmeasure\nMeasure used to assess stress levels\n\n\nday\nDay of the working week (1 to 5)\n\n\nstress\nStress Level (standardised)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_nested.csv\")\n\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\n# removed day*CBD|dept to obtain convergence\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n               (1 + day + CBD | dept) +\n               (1 + day | dept:pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nstressweek_crossed.csv - CBD drinks and stress levels: Version 3#longitudinal#more-complex-groupings\n\n\n\n\n\nAs for Version 1 of this study (see above), with 20 staff members from the Psychology department, but instead of taking a measurement only on a self-report scale, we took 10 different measures every time point (cortisol levels, blood pressure, heart rate variability, various questionnaires etc).\nData are available at https://uoepsy.github.io/data/stressweek_crossed.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndept\nDepartment\n\n\npid\nParticipant Name\n\n\nCBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n\n\nmeasure\nMeasure used to assess stress levels\n\n\nday\nDay of the working week (1 to 5)\n\n\nstress\nStress Level (standardised)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_crossed.csv\")\n\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\n# removed day*CBD|dept to obtain convergence\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | measure) +\n                (1 + day | pid), \n             data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\ncogdecline.csv - domain differences in cognitive aging#longitudinal#more-complex-groupings\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 48, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nData are available at https://uoepsy.github.io/data/cogdecline.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ncID\nCenter ID\n\n\npptID\nParticipant Identifier\n\n\neduc\nEducational attainment (years of education)\n\n\nage\nAge at visit (years)\n\n\nprocessing_speed\nScore on Processing Speed domain task\n\n\nspatial_visualisation\nScore on Spatial Visualisation domain task\n\n\nmemory\nScore on Memory domain task\n\n\nreasoning\nScore on Reasoning domain task\n\n\nvocabulary\nScore on Vocabulary domain task\n\n\n\n\n\n\n\n\n\n\n\nwe need to reshape this data! we don’t have a single variable “score” here, because it’s actually split across 5 columns (one for each domain). So we reshape those columns to have a variable called “score” and a variable indicating which domain it is a score of (we’ll call it “domain”):\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/cogdecline.csv\")\n\n# reshape\ndf &lt;- df |&gt; pivot_longer(processing_speed:vocabulary,\n                   names_to = \"domain\",\n                   values_to = \"score\")\n\n# recenter age and educ\ndf$age &lt;- (df$age - 48)/5\ndf$educ &lt;- df$educ - min(df$educ)\n\nthis won’t converge:\n\nmod1 &lt;- lmer(score ~ educ + age * domain + \n               (1 + educ + age * domain | cID) + \n               (1 + age * domain | cID:pptID),\n             data = df)\n\nthis will - it’s using the trick of putting the domain on the RHS here, to still have different slopes of score~age for each domain:\n\nmod1 &lt;- lmer(score ~ educ + age * domain + \n               (1 | cID) + \n               (1 + age | cID:pptID) +\n               (1 + age | cID:pptID:domain),\n             data = df,\n             control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nlmm_gadeduc.csv - Psychoeducation treatment effects#longitudinal#more-complex-groupings\n\n\n\n\n\nThis is synthetic data from a randomised controlled trial to evaluate the efficacy of a psychoeducational treatment on anxiety disorders, in which 30 therapists randomly assigned patients (each therapist saw between 2 and 28 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nData are available at https://uoepsy.github.io/data/lmm_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\n\nthis one needs lots of reshaping and sorting out first, but it can actually be done with minimal code using things like separate()\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\ndf &lt;- df |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nmod &lt;- lmer(GAD~ visit*group+ \n              (1+visit*group|therapist)+\n              (1+visit|therapist:patient),\n            df)\n\n\n\n\n\n\n\n\n\n\nabs_intervention.csv - evaluating an intervention to reduce adolescent aggressive behaviours#longitudinal#more-complex-groupings\n\n\n\n\n\nIn 2010 A US state’s commissioner for education was faced with growing community concern about rising levels of adolescent antisocial behaviours.\nAfter a series of focus groups, the commissioner approved the trialing of an intervention in which yearly Parent Management Training (PMT) group sessions were offered to the parents of a cohort of students entering 10 different high schools. Every year, the parents were asked to fill out an informant-based version of the Aggressive Behaviour Scale (ABS), measuring verbal and physical abuse, socially inappropriate behavior, and resisting care. Where possible, the same parents were followed up throughout the child’s progression through high school. Alongside this, parents from a cohort of students entering 10 further high schools in the state were recruited to also complete the same informant-based ABS, but were not offered the PMT group sessions.\nThe commissioner has two main questions: Does the presentation of aggressive behaviours increase as children enter the secondary school system? Is there any evidence for the effectiveness of Parent Management Training (PMT) group sessions in curbing the rise of aggressive behaviors during a child’s transition into the secondary school system?\nData are available at https://uoepsy.github.io/data/abs_intervention.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nschoolid\nSchool Name\n\n\nppt\nParticipant Identifier\n\n\nage\nAge (years)\n\n\ninterv\nWhether or not parents attended Parent Management Training (PMT) group sessions (0 = No, 1 = Yes)\n\n\nABS\nAggressive Behaviours Scale. Measures verbal and physical abuse, socially inappropriate behavior, and resisting care. Scores range from 0 to 100, with higher scores indicating more aggressive behaviours.\n\n\n\n\n\n\n\n\n\n\n\nCheck here: https://uoepsy.github.io/dapr3/2425/lectures/05recap.html#/the-research-process-model-specification\n\n\n\n\n\n\n\n\n\nlmm_laughs.csv - the visual aspect of humour#repeated-measures#more-complex-groupings\n\n\n\n\n\nThese data are simulated to imitate an experiment that investigates the effect of visual non-verbal communication (i.e. gestures, facial expressions) on joke appreciation. 90 Participants took part in the experiment, in which they each rated how funny they found a set of 30 jokes. For each participant, the order of these 30 jokes was randomly set for each run of the experiment. For each participant, the set of jokes was randomly split into two halves, with the first half being presented in audio-only, and the second half being presented in audio and video. This meant that each participant saw 15 jokes with video and 15 without, and each joke would be presented in with video roughly half of the times it was seen.\nThe researchers want to investigate whether the delivery (audio/audiovideo) of jokes is associated with differences in humour-ratings.\nData are available at https://uoepsy.github.io/data/lmm_laughs.csv\n\n\n\n\nTable 3: Data Dictionary: lmm_laughs.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identification Number\n\n\njoke_label\nJoke presented\n\n\njoke_id\nJoke Identification Number\n\n\ndelivery\nExperimental manipulation: whether joke was presented in audio-only ('audio') or in audiovideo ('video')\n\n\nrating\nHumour rating chosen on a slider from 0 to 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_laughs.csv\")\n\nmod &lt;- lmer(rating ~ delivery + \n              (1 + delivery | joke_id) +\n              (1 + delivery | ppt),\n            data = df)\n\nworth looking at random effects for different jokes to see which ones are funniest etc.\nThe jokes are all from some study 10 years ago that aimed to find “the worlds funniest joke”\n\n\n\n\n\n\n\n\n\nNGV.csv - Video game aggression and the dark triad#repeated-measures#more-complex-groupings\n\n\n\n\n\nThese data are from an experiment designed to investigate how the realism of video games is associated with more/less unnecessarily aggressive gameplay, and whether this differs depending upon a) the playing mode (playing on a screen vs VR headset), and b) individual differences in the ‘dark triad’ personality traits.\nThe experiment involved playing 10 levels of a game in which the objective was to escape a maze. Various obstacles and other characters were present throughout the maze, and players could interact with these by side-stepping or jumping over them, or by pushing or shooting at them. All of these actions took the same amount of effort to complete (pressing a button), and each one achieved the same end (moving beyond the obstacle and being able to continue through the maze).\nEach participant completed all 10 levels twice, once in which all characters were presented as cartoons, and once in which all characters were presented as realistic humans and animals. The layout of the level was identical in both, the only difference being the depiction of objects and characters. For each participant, these 20 levels (\\(2 \\times 10\\) mazes) were presented in a random order. Half of the participants played via a screen, and the other half played via a VR headset. For each level played, we have a record of “needless game violence” (NGV) which was calculated via the number of aggressive (pushing/shooting) actions taken (+0.5 for every action that missed an object, +1 for every action aimed at an inanimate object, and +2 for every action aimed at an animate character).\nPrior to the experiment, each participant completed the Short Dark Triad 3 (SD-3), which measures the three traits of machiavellianism, narcissism, and psychopathy.\nData are available at https://uoepsy.github.io/data/NGV.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant number\n\n\nage\nParticipant age (years)\n\n\nlevel\nMaze level (1 to 20)\n\n\ncharacter\nWhether the objects and characters in the level were presented as 'cartoon' or as 'realistic'\n\n\nmode\nWhether the participant played via a screen or with a VR headset\n\n\nP\nPsycopathy Trait from SD-3 (score 1-5)\n\n\nN\nNarcissism Trait from SD-3 (score 1-5)\n\n\nM\nMachiavellianism Trait from SD-3 (score 1-5)\n\n\nNGV\nNeedless Game Violence metric\n\n\n\n\n\n\n\n\n\n\n\n(you could also try to fit the interactions in the random effects here but i’m not going to even try!)\n\nm0 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + character + mode + P + M + N | level), data = ngv)\n\nafter some simplification, I end up at the model below. You might end up at a slightly different random effect structure, and that is completely okay! The important thing is to be transparent in your decisions.\n\nm1 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + mode | level), data = ngv)\n\n\n\n\n\n\n\n\n\n\nerm_belief.csv - erm.. I don’t believe you#repeated-measures#more-complex-groupings\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random. Participants rated each statement on how believable it is on a scale of 0 to 100.\nData are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier\n\n\ntrial_n\nTrial number\n\n\nsentence\nStatement identifier\n\n\ncondition\nCondition (fluent v disfluent)\n\n\nbelief\nbelief rating (0-100)\n\n\nstatement\nStatement\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/erm_belief.csv\")\n# make trial_n numeric\ndf$trial_n &lt;- as.numeric(gsub(\"trial_\",\"\",df$trial_n))\n# relevel condition:\ndf$condition &lt;- factor(df$condition, levels=c(\"fluent\",\"disfluent\"))\n\n# having (trial_n | ppt) doesn't converge, so simplify to:\nmod1 &lt;- lmer(belief ~ trial_n + condition + \n               (1 | ppt) + \n               (1 + condition | statement), \n             data = df)\n\nsummary(mod1)\n\neaster egg: plot your ranefs\n\ndotplot.ranef.mer(ranef(mod1))$statement\n\n\n\n\n\n\n\n\n\n\nlmm_alcgad.csv - relative levels of anxiety and alcohol use#repeated-measures\n\n\n\n\n\nA research study is investigating how anxiety is associated with drinking habits. Data was collected from 50 participants from 5 centers. Researchers administered the generalised anxiety disorder (GAD-7) questionnaire to measure levels of anxiety over the past week, and collected information on the units of alcohol participants had consumed within the week. Each participant was observed on 10 different occasions.\nThe researchers are also interested in testing an intervention (given to half of the participants) - they want to know if this changes the association between anxiety and alcohol consumption.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nalcunits\nNumber of units of alcohol consumed over the past week\n\n\ngad\nScore on the Generalised Anxiety Disorder scale (scores 0-3 on 7 questions are totalled for an overall score)\n\n\nintervention\nWhether the participant is part of the intervention group (1) or not (0)\n\n\ncenter\nCenter ID\n\n\nppt\nParticipant ID\n\n\n\n\n\n\n\n\n\n\n\nWe actually have participants nested in center here, but there are only 5 centers..\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_alcgad.csv\")\n\nfirst though, we’ll calculate ppt means and ppt deviations-from-means\n\ndf &lt;- df |&gt; \n  group_by(ppt) |&gt;\n  mutate(\n    gadm = mean(gad),\n    gaddev = gad - mean(gad)\n  ) |&gt; ungroup()\n\nthis works, but bear in mind we do only have 5 centers..\n\nmod &lt;- lmer(alcunits ~ gadm + gaddev + \n       (1 + gaddev | center) +\n       (1 + gaddev | center:ppt),\n     df)\n\nthis would be defensible too:\n\nmod &lt;- lmer(alcunits ~ center + gadm + gaddev + \n       (1 + gaddev | ppt),\n     df)\n\nWe also have the intervention question, which to meet convergence might need to do something like:\n\nmod &lt;- lmer(alcunits ~ center + (gadm + gaddev)*intervention + \n       (1 | ppt),\n     df)\n\n\n\n\n\n\n\n\n\n\ndapr3_tgu.csv - Physiotherapy and physical functioning #repeated-measures\n\n\n\n\n\nA researcher is interested in the efficacy of physiotherapy in helping people to regain normal physical functioning. They are curious whether doing more physiotherapy leads to better outcomes, or if it is possibly that the patients who tend to do more of their exercises tend to have better outcomes. 20 in-patients from 2 different hospitals (1 private, 1 govt funded) were monitored over the course of their recovery following knee-surgery. Every day, the time each patient spent doing their physiotherapy exercises was recorded. At the end of each day, participants completed the “Time get up and go” task, a measure of physical functioning.\nData are available at https://uoepsy.github.io/data/dapr3_tgu.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ntgu\nTime Get up and Go Task - measure of physical functioning. Scored in minutes, with lower scores indicating better physical functioning\n\n\nphys\nMinutes of physiotherapy exercises completed that day\n\n\nhospital\nHospital ID\n\n\npatient\nPatient ID\n\n\nprioritylevel\nPriority level of patients' surgery (rank 1-4, with 1 being most urgent surgey, and 4 being least urgent)\n\n\nprivate\n0 = government funded hospital, 1 = private hospital\n\n\n\n\n\n\n\n\n\n\n\npatients are nested in hospitals, but there are only 2 hospitals!\njust shove it in as fixed predictor\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/dapr3_tgu.csv\")\n\ndf &lt;- df |&gt; group_by(patient) |&gt;\n  mutate(\n    physm = mean(phys),\n    physdev = phys - mean(phys)\n  ) |&gt; ungroup()\n\nmod &lt;- lmer(tgu ~ hospital + physm + physdev + \n              (1 + physdev | patient),\n            data = df)\n\n\n\n\n\n\n\n\n\n\nlmm_bflpe.csv - big fish little pond #cross-sectional\n\n\n\n\n\nThese data are simulated based on the “Big-fish-little-pond” effect in educational literature.\nWe are interested in better understanding the relationship between school children’s grades and their academic self-concept (their self-perception of ability in specific and general academic disciplines).\nWe have data from 20 classes of children, capturing information on their grades at school (range 0 to 10), and a measure of academic self-concept:\nData are available at https://uoepsy.github.io/data/lmm_bflpe.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ngrade\nChildren's School Grade\n\n\nclass\nClass Identifier\n\n\nself_concept\nChildren's Self-Concept Score\n\n\nchild\nChild Identifier\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_bflpe.csv\")\n\ndf &lt;- df |&gt; group_by(class) |&gt;\n  mutate(\n    gradem = mean(grade),\n    gradedev = grade - mean(grade)\n  )\n\nmod &lt;- lmer(self_concept ~ gradem + gradedev + \n              (1 + gradedev | class), \n            data = df)\n\n\n\n\n\n\n\n\n\n\nhangry.csv - hunger + anger = hanger#cross-sectional\n\n\n\n\n\nThis study is interested in evaluating whether peoples’ hunger levels are associated with their levels of irritability (i.e., “the hangry hypothesis”), and if this differs between people on a diet vs those who aren’t. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nData are available at https://uoepsy.github.io/data/hangry.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant Identifier\n\n\nfivetwo\nWhether the participant follows the five-two diet\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/hangry.csv\")\n\ndf &lt;- df |&gt; group_by(ppt) |&gt;\n  mutate(\n    hungerm = mean(q_hunger),\n    hungerdev = q_hunger - mean(q_hunger)\n  )\n\nmod &lt;- lmer(q_irritability ~ (hungerm + hungerdev) * fivetwo +\n              (1 + hungerdev | ppt), \n            data = df,\n            control=lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\n\n\n\n\n\npvt_bilingual.csv - Vocabulary development in monolingual and bilingual children#longitudinal#non-linear#more-complex-groupings\n\n\n\n\n\n488 children from 30 schools were included in the study. Children were assessed on a yearly basis for 7 years throughout primary school on a measure of vocabulary administered in English, the Picture Vocab Test (PVT). 295 were monolingual English speakers, and 193 were bilingual (english + another language).\nPrevious research conducted on monolingual children has suggested that that scores on the PVT increase steadily up until the age of approximately 7 or 8 at which point they begin to plateau. The aim of the present study is to investigate differences in the development of vocabulary between monolingual and bilingual children.\nData are available at https://uoepsy.github.io/data/pvt_bilingual.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nchild\nChild's name\n\n\nschool\nSchool Identifier\n\n\nisBilingual\nBinary variable indicating whether the child is monolingual (0) or bilingual (1)\n\n\nage\nAge (years)\n\n\nPVT\nScore on the Picture Vocab Test (PVT). Scores range 0 to 60\n\n\n\n\n\n\n\n\n\n\n\nLet’s read in the data:\n\npvt &lt;- read_csv(\"https://uoepsy.github.io/data/pvt_bilingual.csv\")\n\nWe have 30 distinct schools:\n\nn_distinct(pvt$school)\n\n[1] 30\n\n\nAnd 418 distinct children. Is that right?\n\nn_distinct(pvt$child)\n\n[1] 418\n\n\nGiven that the pvt$child variable is just the first name of the child, it’s entirely likely that there will be, for instance more than one “Martin”.\nThis says that there are 487!\n\npvt |&gt; count(school, child) |&gt; nrow()\n\n[1] 487\n\n\nBut wait… we could still have issues. What if there were 2 “Martin”s at the same school??\n\npvt |&gt; \n  # count the school-children groups\n  count(school, child) |&gt; \n  # arrange the output so that the highest \n  # values of the 'n' column are at the top\n  arrange(desc(n))\n\n# A tibble: 487 × 3\n   school    child        n\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;\n 1 School 14 James       14\n 2 School 14 Michelle    14\n 3 School 15 Michael     14\n 4 School 21 Daniel      14\n 5 School 3  Jackson     14\n 6 School 5  Raven       14\n 7 School 9  Kaamil      14\n 8 School 1  Allyssa      7\n 9 School 1  Armon        7\n10 School 1  Dakota       7\n# ℹ 477 more rows\n\n\nAha! There are 7 cases where schools have two children of the same name. Remember that each child was measured at 7 timepoints. We shouldn’t have people with 14!\nIf we actually look at the data, we’ll see that it is very neatly organised, with each child’s data together. This means that we could feasibly make an educated guess that, e.g., the “Jackson” from “School 3” in rows 155-161 is different from the “Jackson” from “School 3” at rows 190-196.\nBecause of the ordering of our data, we can do something like this:\n\npvt &lt;- \n  pvt |&gt;\n  # group by the school and child\n  group_by(school, child) |&gt;\n  mutate(\n    # make a new variable which counts from 1 to \n    # the number of rows for each school-child\n    n_obs = 1:n()\n  ) |&gt;\n  # ungroup the data\n  ungroup() |&gt;\n  mutate(\n    # change it so that if the n_obs is &gt;7, the \n    # child becomes \"[name] 2\", to indicate they're the second\n    # child with that name\n    child = ifelse(n_obs&gt;7, paste0(child,\" 2\"), child)\n  )\n\nNow we have 494!\n\npvt |&gt; count(school, child) |&gt; nrow()\n\n[1] 494\n\n\nAnd nobody has anything other than 7 observations!\n\npvt |&gt; count(school, child) |&gt;\n  filter(n != 7)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: school &lt;chr&gt;, child &lt;chr&gt;, n &lt;int&gt;\n\n\nPhew!\nOkay, let’s just fit an intercept-only model:\n\npvt_null &lt;- lmer(PVT ~ 1 +  (1 | school/child), data = pvt)\nsummary(pvt_null)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: PVT ~ 1 + (1 | school/child)\n   Data: pvt\n\nREML criterion at convergence: 23844.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7768 -0.5984  0.0068  0.5744  4.1806 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n child:school (Intercept) 33.48    5.786   \n school       (Intercept) 19.76    4.445   \n Residual                 43.59    6.602   \nNumber of obs: 3458, groups:  child:school, 494; school, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  27.5263     0.8667   31.76\n\n\nAs we can see from summary(bnt_null), the random intercept variances are 33.48 for child-level, 19.76 for school-level, and the residual variance is 43.59.\nSo child level differences account for \\(\\frac{33.48}{33.48 + 19.76 + 43.59} = 0.35\\) of the variance in PVT scores, and child & school differences together account for \\(\\frac{33.48 + 19.76}{33.48 + 19.76 + 43.59} = 0.55\\) of the variance.\nHere’s an initial plot too:\n\nggplot(pvt, aes(x=age,y=PVT,col=factor(isBilingual)))+\n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")\n\n\n\n\n\n\n\n\nI feel like either raw or orthogonal polynomials would be fine here - there’s nothing explicit from the study background about stuff “at baseline”. There’s the stuff about the plateau at 7 or 8, but we can get that from the model plots. Orthogonal will allow us to compare the trajectories overall (their linear trend, the ‘curviness’ and ‘wiggliness’).\nAn additional benefit of orthogonal polynomials is that we are less likely to get singular fits when we include polynomial terms in our random effects. Remember, raw polynomials are correlated, so often the by-participant variances in raw poly terms are highly correlated.\nI’ve gone for 3 degrees of polynomials here because the plot above shows a bit of an S-shape for the bilinguals.\n\npvt &lt;- pvt |&gt; mutate(\n  poly1 = poly(age, 3)[,1],\n  poly2 = poly(age, 3)[,2],\n  poly3 = poly(age, 3)[,3],\n  isBilingual = factor(isBilingual)\n)\n\nThese models do not converge.\nI’ve tried to preserve the by-child random effects of time, because while I think Schools probably do vary, School’s all teach the same curriculum, whereas there’s a lot of varied things that can influence a child’s vocabulary, both in and out of school\n\nmod1 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + (poly1 + poly2 + poly3)*isBilingual | school) + \n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod2 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * (poly1 + poly2) + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod3 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod4 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod5 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# relative to the variance in time slopes, there's v little by-school variance in bilingual differences in vocab\n\nmod6 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly2 +  poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# looks like curvature doesn't vary between schools much as linear and wiggliness \n\nthis one does!\n\nmod7 &lt;- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\n\nlibrary(broom.mixed)\naugment(mod7) |&gt; \n  mutate(\n    poly1 = round(poly1, 3) # because of rounding errors that make plot weird\n  ) |&gt;\n  ggplot(aes(x=poly1,col=isBilingual))+\n  stat_summary(geom=\"pointrange\",aes(y=PVT))+\n  stat_summary(geom=\"line\", aes(y=.fitted))\n\n\n\n\n\n\n\n\nrefitted with lmerTest:\n\nmod7 &lt;- lmerTest::lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nsummary(mod7)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + (1 + poly1 +  \n    poly3 | school) + (1 + (poly1 + poly2 + poly3) | school:child)\n   Data: pvt\n\nREML criterion at convergence: 23076.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.4457 -0.5499 -0.0004  0.5261  4.4194 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr             \n school:child (Intercept)   34.86   5.904                    \n              poly1       1842.60  42.926    0.01            \n              poly2       3945.86  62.816   -0.08  0.23      \n              poly3        684.38  26.161    0.19  0.61  0.87\n school       (Intercept)   19.97   4.468                    \n              poly1        928.32  30.468   -0.40            \n              poly3        335.59  18.319   -0.52 -0.12      \n Residual                   31.93   5.651                    \nNumber of obs: 3458, groups:  school:child, 494; school, 30\n\nFixed effects:\n                    Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)          27.9464     0.8987   31.1128  31.098  &lt; 2e-16 ***\npoly1               157.0421     9.5750   39.5964  16.401  &lt; 2e-16 ***\npoly2               -48.9606     8.0944  517.6541  -6.049 2.80e-09 ***\npoly3                -1.2793     8.1739   61.5750  -0.157  0.87614    \nisBilingual1         -1.2113     0.5863  465.7927  -2.066  0.03939 *  \npoly1:isBilingual1   16.6104    12.3121  501.9133   1.349  0.17791    \npoly2:isBilingual1   56.2609    12.9500  517.6541   4.344 1.68e-05 ***\npoly3:isBilingual1  -34.3286    11.8629 1217.8366  -2.894  0.00387 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) poly1  poly2  poly3  isBln1 pl1:B1 pl2:B1\npoly1       -0.215                                          \npoly2       -0.013  0.026                                   \npoly3       -0.184 -0.003  0.072                            \nisBilingul1 -0.250 -0.001  0.020 -0.020                     \nply1:sBlng1  0.000 -0.500 -0.020 -0.022 -0.001              \nply2:sBlng1  0.008 -0.016 -0.625 -0.045 -0.033  0.033       \nply3:sBlng1 -0.009 -0.020 -0.050 -0.566  0.034  0.038  0.079\n\n\n\n\n\n\n\n\n\n\nterm\nest\np\ninterpretation\n\n\n\n\n(Intercept)\n27.95\n*\naverage vocab score at the mean age (for monolingual)\n\n\npoly1\n157.04\n*\nvocab increases over time (for monolingual children)\n\n\npoly2\n-48.96\n*\nthe increase in vocab becomes more gradual (for monolingual children)\n\n\npoly3\n-1.28\n\nno significant wiggliness to vocab trajectory of the average monolingual child\n\n\nisBilingual1\n-1.21\n*\naverage vocab score at mean age is lower for bilingual vs monolingual children\n\n\npoly1:isBilingual1\n16.61\n\nno significant difference in linear trend of vocab for bilingual vs monolingual\n\n\npoly2:isBilingual1\n56.26\n*\ncurvature for vocab trajectory of bilingual children significantly differs from that of monolinguals\n\n\npoly3:isBilingual1\n-34.33\n*\nwiggliness for vocab trajectory of bilingual children significantly differs from that of monolinguals\n\n\n\n\n\n\n\nFrom the random effects, we get even more information!\nSchool’s vary in the average child vocab score at mean age with an SD of 4.5. More school level variation in linear trends than in curvature. Schools with higher vocab scores at the mean age tend to have lower linear increase, and also a more negative curvature.\nWithin schools, children vary in the vocab scores at mean age with an SD of 5.9. Lots of child-level variation in curvature and linear increases, slightly less variation in wiggliness.\n\nVarCorr(mod7)\n\n Groups       Name        Std.Dev. Corr                \n school:child (Intercept)  5.9043                      \n              poly1       42.9256   0.005              \n              poly2       62.8161  -0.078  0.227       \n              poly3       26.1606   0.191  0.609  0.871\n school       (Intercept)  4.4683                      \n              poly1       30.4683  -0.403              \n              poly3       18.3190  -0.524 -0.116       \n Residual                  5.6510                      \n\n\n\n\n\n\n\n\n\n\n\nAz.rda - trajectories of memory, simple, and complex daily functioning tasks in patients with Alzheimer’s#longitudinal#non-linear\n\n\n\n\n\nThese data are available at https://uoepsy.github.io/data/Az.rda. You can load the dataset using:\n\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n\nand you will find the Az object in your environment.\nThe Az object contains information on 30 Participants with probable Alzheimer’s Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100.\nWe’re interested in whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nSubject\nUnique Subject Identifier\n\n\nTime\nTime point of the study (1 to 10)\n\n\nTask\nTask type (Memory, cADL, sADL)\n\n\nPerformance\nScore on test (range 0 to 100)\n\n\n\n\n\n\n\n\n\n\n\n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nmidlifeape.csv - mid-life happiness slump in great apes#longitudinal#non-linear\n\n\n\n\n\nPrevious research has evidenced a notable dip in happiness for middle-aged humans. Interestingly, this phenomenon has even been observed in other primates, such as chimpanzees.\nThe present study is interested in examining whether the ‘middle-age slump’ happens to a similar extent for Orangutans as it does for Chimpanzees.\n200 apes (117 Chimps and 83 Orangutans) were included in the study. All apes were studied from early adulthood (10-12 years old for most great apes), and researchers administered the Happiness in Primates (HiP) scale to each participant every 3 years, up until the age of 40.\nData are available at https://uoepsy.github.io/data/midlife_ape.csv.\nThe dataset has already been cleaned, and the researchers have confirmed that it includes 117 Chimps and 83 Orangutans, and every ape has complete data (i.e. 10 rows for each ape).\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\napeID\nApe's Name (all names are chosen to be unique)\n\n\nage\nAge (in years) at assessment\n\n\nspecies\nSpecies (chimp v orangutan)\n\n\nHiP\nHappiness in Primate Scale (range 1 to 18)\n\n\ntimepoint\nStudy visit (1 to 10)\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/midlife_ape.csv\")\n\n# add polynomials\ndf &lt;- df |&gt; \n  mutate(\n    poly1 = poly(timepoint,2,raw=F)[,1],\n    poly2 = poly(timepoint,2,raw=F)[,2]\n  )\n\nmod1 = lmer(HiP ~ 1 + (poly1 + poly2) * species +\n            (1 + poly1 + poly2 | apeID), \n            data = df)\n\nsummary(mod1)\n\n\n\n\n\n\n\n\n\n\nmemorytap.csv - Memory Recall & Finger Tapping#repeated-measures#binomial-outcome\n\n\n\n\n\nResearchers are interested in investigating whether, after accounting for effects of sentence length, rhythmic tapping of fingers aids memory recall. They recruited 40 participants. Each participant was tasked with studying and then recalling 10 randomly generated sentences between 1 and 14 words long. For 5 of these sentences, participants were asked to tap their fingers along with speaking the sentence in both the study period and in the recall period. For the remaining 5 sentences, participants were asked to sit still.\nData are available at https://uoepsy.github.io/data/memorytap.csv, and contains information on the length (in words) of each sentence, the condition (static vs tapping) under which it was studied and recalled, and whether the participant was correct in recalling it.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nppt\nParticipant Identifier (n=40)\n\n\nslength\nNumber of words in sentence\n\n\ncondition\nCondition under which sentence is studied and recalled ('static' = sitting still, 'tap' = tapping fingers along to sentence)\n\n\ncorrect\nWhether or not the sentence was correctly recalled\n\n\n\n\n\n\n\n\n\n\n\n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nnwl.RData - novel word learning#longitudinal#binomial-outcome\n\n\n\n\n\nData are available at\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\n\nIn the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test. Data were also collect from healthy controls.  Our broader research aim today is to compare the two lesion location groups (those with anterior vs. posterior lesions) with respect to their accuracy of responses over the course of the study.\n Figure 1 shows the differences between lesion location groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)\n\n\n\n\n\n\n\n\nFigure 1: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ngroup\nWhether participant is a stroke patient ('patient') or a healthy control ('control')\n\n\nlesion_location\nLocation of brain lesion: anterior vs posterior\n\n\nblock\nExperimental block (1-9). Blocks 1-7 were learning blocks, immediately followed by a test in block 8. Block 9 was a follow-up test at a later point\n\n\nPropCorrect\nProportion of 30 responses in a given block that the participant got correct\n\n\nNumCorrect\nNumber of responses (out of 30) in a given block that the participant got correct\n\n\nNumError\nNumber of responses (out of 30) in a given block that the participant got incorrect\n\n\nID\nParticipant Identifier\n\n\nPhase\nExperimental phase, corresponding to experimental block(s): 'Learning', 'Immediate','Follow-up'\n\n\n\n\n\n\n\n\n\n\n\n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\ntestenhancedlearning.RData#longitudinal#more-complex-groupings#binomial-outcome\n\n\n\n\n\nAn experiment was run to conceptually replicate “test-enhanced learning” (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (StudyStudy), the other group studied the material once then did a test (StudyTest). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (Test_word).\nThe critical (replication) prediction is that the StudyStudy group perform better on the immediate test, but the StudyTest group will retain the material better and thus perform better on the 1-week follow-up test.\nWe have two options for how we measure “test performance”:\n\nThe time taken to correctly recall a given word.\n\nWhether or not a given word was correctly recalled\n\nThe following code loads the data into your R environment by creating a variable called tel:\n\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nSubject_ID\nUnique Participant Identifier\n\n\nGroup\nGroup denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest)\n\n\nDelay\nTime of recall test ('min' = Immediate, 'week' = One week later)\n\n\nTest_word\nWord being recalled (175 different test words)\n\n\nCorrect\nWhether or not the word was correctly recalled\n\n\nRtime\nTime to recall word (milliseconds)\n\n\n\n\n\n\n\n\n\n\n\n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nmsmr_trolley.csv - trolley problems#repeated-measures#binomial-outcome\n\n\n\n\n\nThe “Trolley Problem” is a thought experiment in moral philosophy that asks you to decide whether or not to pull a lever to divert a trolley. Pulling the lever changes the trolley direction from hitting 5 people to a track on which it will hit one person.\n\n\n\n\n\n\n\n\n\nPrevious research has found that the “framing” of the problem will influence the decisions people make:\n\n\n\n\n\n\n\n\npositive frame\nneutral frame\nnegative frame\n\n\n\n\n5 people will be saved if you pull the lever; one person on another track will be saved if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n5 people will be saved if you pull the lever, but another person will die. One people will be saved if you do not pull the lever, but 5 people will die. All your actions are legal and understandable. Will you pull the lever?\nOne person will die if you pull the lever. 5 people will die if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n\n\n\n\n\n\n\nWe conducted a study to investigate whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved).\n120 participants were recruited, and each gave answers to 12 versions of the thought experiment. For each participant, four versions followed each of the positive/neutral/negative framings described above, and for each framing, 2 would save 5 people and 2 would save 15 people.\nData are available at https://uoepsy.github.io/data/msmr_trolley.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nframe\nframing of the thought experiment (positive/neutral/negative\n\n\nlives\nlives at stake in the thought experiment (5 or 15)\n\n\nlever\nWhether or not the participant chose to pull the lever (1 = yes, 0 = no)\n\n\n\n\n\n\n\n\n\n\n\n\n# TODO ANALYSIS\n\n\n\n\n\n\n\n\n\n\nmonkey social status and problem solving ability#repeated-measures#binomial-outcome\n\n\n\n\n\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a “dominant” or “subordinate” status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either “easy” or “difficult”, and the researchers recorded whether or not the monkey solved each problem.\nWe’re interested in how the social status of monkeys is associated with the ability to solve problems.\nData are available at https://uoepsy.github.io/data/msmr_monkeystatus.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nstatus\nSocial Status of monkey (adolescent, subordinate adult, or dominant adult)\n\n\ndifficulty\nProblem difficulty ('easy' vs 'difficult')\n\n\nmonkeyID\nMonkey Name\n\n\nsolved\nWhether or not the problem was successfully solved by the monkey\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\n\n# relevel difficulty\ndf$difficulty &lt;- factor(df$difficulty, \n                        levels=c(\"easy\",\"difficult\"))\n\nmod1 &lt;- glmer(solved ~ difficulty + status +\n                (1 + difficulty | monkeyID), \n              family=binomial,\n              data = df)\n\nsummary(mod1)",
    "crumbs": [
      "Additional Docs",
      "Practice Datasets"
    ]
  }
]