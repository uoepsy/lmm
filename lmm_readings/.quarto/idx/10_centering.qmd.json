{"title":"10: Centering","markdown":{"yaml":{"title":"10: Centering","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Centering predictors in `lm()`","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n- Centering and scaling predictors in single level regression\n- Group-mean centering predictors in the multilevel model to separate out \"within\"-group effects from \"between\"-group effects\n- Optional: contextual effects and the Mundlak model\n\n:::\n\n\n\n\nThere are lots of ways we can transform a variable. For instance, we can transform something in millimeters to being in centimeters by dividing it by 10. We could transform a `height` variable into `height above/below 2 meters` variable by subtracting 2 meters from it.   \n\nA couple of common transformations we have seen already are 'centering' and 'standardising':  \n\n- When we \"center\" a variable, we subtracting some number (often the mean of the variable) from every value. So if we 'mean-center' a variable measuring height in cm, and the mean height of my sample is 175cm, then a value of 190 becomes +15, and a value of 150 becomes -25, and so on.  \n- When we 'standardise' a variable, we mean-center it and then divide the resulting values by the standard deviation. So if the standard deviation of heights in my sample is 15, then the value of 190 becomes $\\frac{190-175}{15} = \\frac{15}{15} = 1$, and the 150 becomes $\\frac{150-175}{15} = \\frac{-25}{15} = -1.67$.  \n\nHow does this choice affect the linear models we might be fitting? The short answer is that it doesn't! The overall fit of `lm()` is not changed in any way when we apply these linear^[the fit of models _does_ change if we apply a non-linear transformation, such as $x^2$, $log(x)$, etc., and this can sometimes be useful for studying effects that are more likely to be non-linear!] transformations to predictors or outcomes. \n\nHowever, transformations _do_ change what we get out of our model:  \n\n- If we re-center a predictor on some new value (such as the mean), then all this does is change what \"zero\" means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is \"when all predictors are zero\". And we are changing what \"zero\" represents! \n- When we scale a predictor, this will change the slope. Why? Because it changes what \"moving 1\" represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains _exactly the same_, we are only changing the *units* that we are using to expressing that slope.\n\nThe example below shows a model of heart rates (`HR`) predicted by hours slept (`hrs_sleep`). In @fig-scalexlm you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don't affect the model itself - the regression line (and the uncertainty in the line) is the same in each plot. We can see that re-centering changes what the intercept represents:  \n\n- In the top left plot, \"0\" represents zero hours slept, so the intercept (big blue dot) is the estimated heart rate for someone who didn't sleep at all. \n- Similarly, in the top right plot, \"0\" now represents the mean hours slept, so the intercept is the heart rate for someone who slept an average amount, and in the bottom right plot, \"0\" now represents 8 hours of sleep (the recommended amount).\n- In the bottom left plot (where hours slept is 'standardized'), not only have we changed what \"0\" represents, but we have changed what moving \"1\" represents. Rather being an increase of 1 hour of sleep, in this plot it represents an increase of 1 standard deviation hours sleep (whatever that is for our sample - it looks to be about `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`). This means our estimated slope is the change in heart rate when having 1 SD more hours sleep (approx `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`).\n\n```{r}\n#| label: fig-scalexlm\n#| fig-cap: \"Centering and scaling predictors in linear regression models. Intercepts and their interpretation change when re-centered, and slope coefficients and their interpretation change when scaling, but the overall model stays the same.\"\n#| out-width: \"100%\"\n#| echo: false\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(title=\"Original\", x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~scale(x,scale=F),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=map_dbl(seq(7,-7), ~mean(df$x)-.),\n                     labels=seq(-7,7))+\n  labs(title=\"Mean Centered\", x=\"Hours Slept\\n(mean centered)\",y=\"HR\")\n  \n  \nmod = lm(y~scale(x),df)\np3 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),\n                     breaks=c(mean(df$x)-(2*sd(df$x)), mean(df$x)-sd(df$x), \n                              mean(df$x), \n                              mean(df$x)+sd(df$x), mean(df$x)+(2*sd(df$x))),\n                     labels=c(-2,-1,0,1,2))+\n  labs(title=\"Standardised X\", x=\"Hours Slept\\n(standardised)\",y=\"HR\")\n\nmod = lm(y~x,df %>% mutate(x=x-8))\np4 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=8,xend=8,y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=8,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14, labels=c(0:14)-8)+\n  labs(title=\"Centered on 8 hours\", x=\"Hours Slept\\n(relative to 8 hours)\",y=\"HR\")\np1 + p2 + p3 + p4 \n```\n\nThe thing to note is that the lines themselves are all the same, because the models are all exactly the same. We can prove this to ourselves by comparing the 4 models: \n\n```{r}\n#| code-fold: true\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\n\n# original model:\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n# model with hrs_sleep mean centered\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n# model with hrs_sleep standardised\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\n# model with hrs_sleep centered on 8 hours the I() function\n# is a handy function that is just needed because the symbols\n# + and - normally get interprted in lm() as adding \n# and removing predictors. \nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat) \n\n# all models are identical fit\nanova(mod_orig, mod_mc, mod_z, mod_8)\n```\n\n::: {.callout-note collapse=\"true\"}\n#### Centering when we have interactions\n\nWhen we have an interactions in a model such as `lm(y~x+z+x:z)`, the individual coefficients for `x` and `z` are specifically the associations \"when the other variable included in the interaction is zero\". Because re-centering a variable changes the meaning of \"zero\", this means that these two coefficients will change.  \n\nFor instance, a model of heart rates (`HR`) that includes an interaction between `hrs_sleep` and whether someone smokes, our coefficient for `smoke` estimates the difference in HR between smokers vs non-smokers who get zero hours of sleep (red to blue point in the left-hand plot of @fig-intcent). If we mean-center the `hrs_sleep` variable in our model, then it becomes the estimated difference in HR between smokers vs non-smokers who get the average hours of sleep (red to blue point in the right-hand plot of @fig-intcent). \n\n\n```{r}\n#| label: fig-intcent\n#| fig-cap: \"mean-centering a variable that is involved in an interaction will change the point at which the marginal effect of other variable is estimated at\" \n#| out-width: \"100%\"\n#| echo: false\nhrdat <- hrdat %>% \n  mutate(\n    smoke = ifelse(smoke %in% c(\"v\",\"y\"),\"y\",\"n\"),\n    hrs_sleepC = hrs_sleep - mean(hrs_sleep)\n  )\neg2mod <- lm(HR ~ hrs_sleep * smoke, data = hrdat)\neg2mod_cent <- lm(HR ~ hrs_sleepC * smoke, data = hrdat)\nas.data.frame(effects::effect(\"hrs_sleep*smoke\",eg2mod,\n              xlevels=list(hrs_sleep=0:15))) |>\n  ggplot(aes(x=hrs_sleep,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleep,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Original X\") +\n  \n\nas.data.frame(effects::effect(\"hrs_sleepC*smoke\",eg2mod_cent,\n              xlevels=list(hrs_sleepC=(0:15)-mean(hrdat$hrs_sleep)))) |>\n  ggplot(aes(x=hrs_sleepC,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleepC,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Mean Centered X\") +\n  plot_layout(guides=\"collect\")\n\n\n  \n```\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n## Centering predictors in multilevel models\n\nIn multilevel models, things can be a little bit different. \n\n::::{.columns}\n:::{.column width=\"65%\"}\n\nFor one thing, there can be practical benefits to centering and/or scaling predictors with respect to actually *fitting* these models. Because multilevel models involve estimating group-level variability in intercepts and slopes, if our intercept is very far away from our data (e.g., if all our data is from ages 60 to 80, and we are estimating variability at age 0), then slight changes in a slope can have huge influences on estimated intercepts, resulting in models that don't converge. We can see from the longitudinal example ([Chapter 5](05_long.html#modelling-change-over-time){target=\"_blank\"}), with the idea represented in @fig-centconv - using raw `age` values the intercepts and slopes are highly correlated and the model won't converge, but when we recenter the `age` variable on 60 and the intercept variation would become the variability in peoples' cognition _at the start of the study period_, and the random intercepts are not so determined by the random slopes.  \n:::\n\n:::{.column width=\"35%\"}\n\n```{r} \n#| label: fig-centconv\n#| fig-cap: \"lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the 'intercept' is estimated back at age 0\"  \n#| echo: false\n#| fig-height: 4\nmmd <- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nmod1 <- lmer(ACE ~ 1 + age + \n               (1 +age| ppt), \n             data = mmd)\nbroom.mixed::augment(mod1) |>\n  ggplot(aes(x=age,group=ppt))+\n  geom_point(aes(y=ACE))+\n  stat_smooth(geom=\"line\",method=lm,se=F,fullrange=T,\n              alpha=.4,\n              aes(y=.fitted))+\n  xlim(0,78)\n```\n:::\n::::\n\nHowever, in some cases (typically in observational, rather than experimental studies), having multi-level data may mean that we can actually transform a predictor in a couple of ways - we can center it on a constant number like the overall mean/min/max, but we can also consider transformations _within each group_. The key here is that we don't always have just have _one_ \"overall\" mean for a predictor, but often we have different means for each group.  \n\n### Group mean centering\n\n```{r}\n#| echo: false\nset.seed(345)\nN = 200\n  n_groups = 20\n  g = rep(1:n_groups, e = N/n_groups)\n  u = rnorm(n_groups,0,3)[g]\n  x = rnorm(N,u)\n\n  re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,0,0,.5),ncol=2))\n  re1 = re[,1][g]\n  re_x = re[,2][g]\n  lp = (0 + re1) + (1 + re_x) * x -1.5*u\n  y = rnorm(N, mean = lp, sd = 1)\n  \n  df = data.frame(grade=(scale(x)[,1]*13.6+60.214)/10, class = factor(g), \n                  self_concept=y)\n\n  # ggplot(df,aes(x=grade,y=self_concept,col=class))+\n  #   geom_point()+guides(col=\"none\")+\n  #   geom_smooth(method=lm,se=F)\n  \nbfdat <- df |> mutate(child=1:n(),grade=round(grade,2),self_concept=round(self_concept,2))\n\n#write_csv(bfdat, \"../../data/lmm_bflpe.csv\")\n```\n\n\n:::frame\n__Dataset: lmm_bflpe.csv__\n\nThese data are simulated based on the [\"Big-fish-little-pond\" effect](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6124391/){target=\"_blank\"} in educational literature. \n\nWe are interested in better understanding the relationship between school children's grades and their academic self-concept (their self-perception of ability in specific and general academic disciplines).  \n\nWe have data from `r n_distinct(bfdat$class)` classes of children, capturing information on their grades at school (range 0 to 10), and a measure of academic self-concept:  \n\n```{r}\nbfdat <- read_csv(\"https://uoepsy.github.io/data/lmm_bflpe.csv\")\nhead(bfdat)\n```\n\n:::\n\nIn initial exploratory plots, we can see that there is plenty of variation in childrens' grades - we have children scoring 3-4, all the way up to scoring almost 9. Note also, however, that the class's average grades also vary substantially. For instance, classes 8,9,10 and 11 all have a very high average grade.  \n```{r}\n#| code-fold: true\n#| fig-height: 4\nlibrary(patchwork)\nggplot(bfdat, aes(x=grade,y=self_concept))+\n  geom_point(alpha=.2) +\n\nggplot(bfdat,aes(x=class,y=grade))+\n  geom_jitter(height=0,width=.1,alpha=.2)+\n  stat_summary(geom=\"pointrange\")+\n  coord_flip()\n```\n\nWhen we plot the individual childrens' score of 'self-concept' against grades, the picture becomes a bit clearer once we separate by the classes, where we can see that *within each class* there is a fairly positive trend.  \n\n```{r}\n#| code-fold: true\n#| fig-height: 4\nggplot(bfdat,aes(x=grade,y=self_concept))+\n  geom_point(size=2,alpha=.4) +\n\nggplot(bfdat,aes(x=grade,y=self_concept))+\n  geom_point(size=2,alpha=.4)+\n  facet_wrap(~class)\n```\n\nBy contrast, the relationship between children's self-concept scores and the *average grade of their class* shows a different pattern:\n```{r}\n#| code-fold: true\n#| fig-height: 4\nbfdat <- \n  bfdat |> \n    group_by(class) |>\n    mutate(\n      grade_avg = mean(grade)\n    )\nggplot(bfdat,aes(x=grade_avg,y=self_concept))+\n  stat_summary(geom=\"pointrange\")+\n  labs(x=\"class average grade\")\n```\n\nSo there are clearly two different things going on here!  \n\n1. We have a positive association between a children's grades _relative to their peers' grades_ and their self-concept. This maybe makes sense - comparisons with other people around you will influence your feelings of self worth. \n2. We almost see a *negative* association between the average grade of a child's class and the child's self-concept --- i.e., children from classes with high grades tend to have slightly _lower_ self-concept!\n\nIn the typical multilevel model that we might fit for this study (below), we just get out one single effect estimate, which represents the expected change in self-concept when a child's grade increases by 1. But we have just seen how a child's grades are driven by two things - their class as a whole, and their relative standing in their class.  \n```{r}\n#| eval: false\nrsmod <- lmer(self_concept ~ grade + (1 + grade | class),\n              data = bfdat)\nsummary(rsmod)\n```\n```{r}\n#| echo: false\nrsmod <- lmer(self_concept ~ grade + (1 + grade | class),\n              data = bfdat)\n.pp(summary(rsmod),l=list(c(18:22)))\n```\n\nWhat we want to do here is separate out effects that are \"within\" (i.e. having higher/lower grades *than your classmates*) from those that are \"between\" (i.e. being from a class with higher/lower grades than other classes). To get at these two effects, we are going to explicitly separate our predictor variable into two different parts: \n\n1. the group average\n2. individual deviations from the group average. \n\nWe can calculate these by first using `group_by()` to make the calculations be applied separately for each class, and then calculating the `mean()` grade (for each class), and the deviations for each child from their class's average:  \n\n```{r}\nbfdat <- \n  bfdat |> \n  group_by(class) |>\n  mutate(\n    grade_avg = mean(grade),\n    grade_dev = grade - mean(grade)\n  )\n\nhead(bfdat)\n```\n\nNote that the actual grade for each child can still be made from our two new columns, calculated as the `avg_grade + grade_dev`.  \n\nSo let's plot the association between each of these new variables and the self-concept scores:\n```{r}\n#| code-fold: true\n#| fig-height: 4\nggplot(bfdat, aes(x=grade_avg, y=self_concept))+\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n\nggplot(bfdat, aes(x=grade_dev, y=self_concept))+\n  geom_point() +\n  geom_smooth(method=\"lm\")\n```\n\nSo we can see that there are two different things going on here - the effect on self-concept of being in a high-performing class, as well as the effect of performing higher _for your class_.  \n\n\n## The within-between model\n\nNow that we have split up the variable grade into two parts (group average, and deviations-from-group-averages), we can actually put these in as separate predictors into our model!  \n\nThis type of model is sometimes referred to as a \"within-between\" model. You can see below both the standard model with random slopes, and the 'within-between' model, in both `lmer()` syntax and in equation form.  \n\nNote that we while we replace one predictor (`x`) with its two constituent parts (the group means of `x` and the deviations from those group means), it is only the within effect that we can have a random slope for. This will hopefully make sense when we think a little about it, because the group-means are \"between groups\" - having a random slope of `group_mean_x|group` is similar to the idea of `handedness|person`, because for a single group, we don't have \"an effect on y of that group having a high average x\", so we can't consider it to be an effect that varies by-group.  \n\n\n\n::::panelset\n:::panel\n#### random slopes model\n\n```{r}\n#| echo: true\n#| eval: false\nlmer(y ~ 1 + x + (1 + x | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n\n:::panel\n#### within-between model  \n\n```{r}\n#| echo: true\n#| eval: false\ndata <- data |>\n  group_by(g) |>\n  mutate(\n    x_avg = mean(x),\n    x_dev = x - mean(x)\n  )\n\nlmer(y ~ 1 + x_dev + x_avg + (1 + x_dev | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot (x_{ij} - \\bar{x}_i) + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n::::\n\n<br>    \nIn the context of our educational study of grades and self-concept in school children, we can fit a model that disaggregates within (grade relative to class) and between (class average grade) effects:    \n\n```{r}\nwbmod <- lmer(self_concept ~ grade_dev + grade_avg + \n                (1 + grade_dev|class), \n              data = bfdat)\n```\n\nThe fixed effects from this model (below) now show two effects, as opposed to only one that we would get from our typical model:   \n\n::::{.columns}\n:::{.column width=\"50%\"}\n__typical random slopes model__\n\n```{r}\n#| echo: false\nbroom.mixed::tidy(rsmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n\n:::{.column width=\"50%\"}\n__within-between model__  \n\n```{r}\n#| echo: false\nbroom.mixed::tidy(wbmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n::::\n\n- the \"within\" effect: for every one grade higher a child is *relative to their classmates*, their self-concept is expected to increase by `r round(fixef(wbmod)[2],2)`.\n- the \"between\" effect: for every 1 grade higher a class average is (and when a child's relative standing in the class stays constant), a child's self-concept is expected to decrease by `r round(fixef(wbmod)[3],2)`.\n\nSo what exactly does the effect (the estimate of `r round(fixef(rsmod)[2],2)`) from our more traditional model show here? Is it the within effect or the between effect? It's actually a smushing together of both parts - it is the estimated effect on self-concept when a child's grade increases by 1, but it is confounded by the fact that as childrens' grades increase then their class average increases a bit too, meaning that the between effect pulls this back down. In short - it's not actually a very useful estimate for us at all, because it conflates the two different effects. \n\n\n\n\n```{r}\n#| eval: false\n#| echo: false\ngconf = function(){\n  N = 200\n  n_groups = 20\n  g = rep(1:n_groups, e = N/n_groups)\n  u = rnorm(n_groups,0,5)[g]\n  x = rnorm(N,u)\n\n  re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,0,0,.5),ncol=2))\n  re1 = re[,1][g]\n  re_x = re[,2][g]\n  lp = (0 + re1) + (1 + re_x) * x + -3*u\n  y = rnorm(N, mean = lp, sd = 1)\n  \n  df = data.frame(x, g = factor(g), y, y)\n  # ggplot(df,aes(x=x,y=y,col=g))+\n  #   geom_point()+guides(col=\"none\")+\n  #   geom_smooth(method=lm,se=F)\n  \n  c(\n    ri=fixef(lmer(y~1+x+(1|g),df))['x'],\n    rs=fixef(lmer(y~1+x+(1+x|g),df))['x'],\n    mui=fixef(lmer(y~1+x+xm+(1|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mu=fixef(lmer(y~1+x+xm+(1+x|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mwbi=fixef(lmer(y~1+xd+xm+(1|g),df |> group_by(g) |>mutate(xm=mean(x),xd=x-xm)))['xd'],\n    mwb=fixef(lmer(y~1+xd+xm+(1+xd|g),df |> group_by(g) |>mutate(xm=mean(x),xd=x-xm)))['xd']\n  )\n}\n\nres = t(replicate(100,gconf()))\npar(mfrow=c(3,2))\nhist(res[,1]);hist(res[,2]);hist(res[,3]);hist(res[,4]);hist(res[,5])\npar(mfrow=c(1,1))\ncolMeans(res)\napply(res,2,sd)\n```\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n## Optional: contextual effects and the mundlak model\n\nAlong with the within-between model, we could also choose to adjust for the group averages while continuing to use the original raw predictor in the model. This is often called the \"Mundlak model\" in reference to [Yair Mundlak](https://doi.org/10.2307/1913646){target=\"_blank\"} who wrote about it in the context of avoiding group-level confounding (see [Chapter 9 #optional-extra-group-confounding](09_assump.html#optional-extra-group-confounding-and-the-random-effects-assumption){target=\"_blank\"}).\n\nThe formulation is very similar to the within-between model, but we don't use the \"deviations from group means\", we simply use the original predictor along with the group means:  \n\n\n::::panelset\n:::panel\n#### within-between model  \n\n```{r}\n#| echo: true\n#| eval: false\ndata <- data |>\n  group_by(g) |>\n  mutate(\n    x_avg = mean(x),\n    x_dev = x - mean(x)\n  )\n\nlmer(y ~ 1 + x_dev + x_avg + (1 + x_dev | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot (x_{ij} - \\bar{x}_i) + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n:::panel\n#### mundlak model\n\n```{r}\n#| echo: true\n#| eval: false\ndata <- data |>\n  group_by(g) |>\n  mutate(\n    x_avg = mean(x)\n  )\n\nlmer(y ~ 1 + x + x_avg + (1 + x | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n::::\n\n<br>\nWe can fit the Mundlak formulation to our grades & self-concept data as follows:  \n```{r}\nmlakmod <- lmer(self_concept ~ grade + grade_avg + \n                  (1 + grade|class), \n                data = bfdat)\n```\n\nThere are two things to note here when comparing the Mundlak formulation to the 'within-between' model. Firstly, these two models provide the same 'within' effect (the fixed effects of `grade_dev` and `grade` in the tables below), because they both get the effect of a child's grade increasing by 1, while holding their class's average grade constant.^[when we exclude the random intercepts, the within-effects from both models are numerically identical, but there may be small differences due to the random slopes of `x|g` for the mundlak model and `x_dev|g` for the within-between model] Secondly, the estimated effects for the `grade_avg` predictor differ substantially between the two models: \n\n::::{.columns}\n:::{.column width=\"50%\"}\n__Within-between model__  \n```{r}\n#| echo: false\nbroom.mixed::tidy(wbmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n\n:::{.column width=\"50%\"}\n__Mundlak model__  \n```{r}\n#| echo: false\nbroom.mixed::tidy(mlakmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n::::\n\n\nThe difference here is that they are capturing two distinct effects. The within-between formulation captures a 'between effect' and the Mundlak formulation provides something that gets termed the \"contextual effect\".  \n\nThe key thing to distinguish between these two is to think about what is being \"held constant\".\nIn the within-between model, the effect of `avg_grade` is estimated while holding constant the child's _relative standing_ in the group. In the Mundlak model, the effect is estimated while holding constant a child's _actual_ grade.  \n\nIt may help to think about this in terms of a single child. Suppose we have a child who has a grade of `r bfdat$grade[1]`, from a class with an average of `r bfdat$grade_avg[1]`. So that child is `r bfdat$grade_dev[1]` _above their class average._  \n```{r}\n#| echo: false\nbfdat[1, ] |> transmute(class=\"i\", child=\"j\", grade_avg, grade_dev, self_concept=\"y\")\n```\n\nBoth models are estimating \"what would we expect to happen to the child's self-concept if their class had an average of `r bfdat$grade_avg[1]+1` instead?\"  \n\nThe within-between model estimates this but holds constant the child's deviation above the average, so we're comparing the scenario where the child is `r bfdat$grade_dev[1]` above a class average of `r bfdat$grade_avg[1]+1`, as opposed to being `r bfdat$grade_dev[1]` above a class average of `r bfdat$grade_avg[1]`.\n```{r}\n#| echo: false\nbind_rows(\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg, grade_dev, grade, self_concept=\"y\"),\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg=grade_avg+1, grade_dev, grade=grade+1,\n                        self_concept=paste0(\"y\", round(fixef(wbmod)[3],2)))\n)\n```\n\nBy contrast, the Mundlak model holds constant the child's _actual_ grade, meaning that we're comparing the scenario where the child has a grade of `r bfdat$grade[1]` and is in a class with an average of `r bfdat$grade_avg[1]+1`, as opposed to having that same grade of `r bfdat$grade[1]` but being in a class with an average of `r bfdat$grade_avg[1]`:  \n```{r}\n#| echo: false\nbind_rows(\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg, grade, self_concept=\"y\"),\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg=grade_avg+1, grade, \n                        self_concept=paste0(\"y\", round(fixef(mlakmod)[3],2)))\n)\n```\n\nWe can think of this visually. Take a given child from a given class, and think about what would happen if their whole class average grade increased by 1. In @fig-mlak, the red line is \"class 1\" in our data, and the blue line is a counterfactual world of \"class 1 if its average grade increased by 1\". The large red dot represents the expected self-concept for \"child 1\".  \n\nIn the within-between model, we're estimating the self-concept difference for a child between the red (actual) and blue (counterfactual class with a higher average), but where the child stays the same amount \"above average\" in this counterfactual class. In the Mundlak model, we're estimating the self-concept difference when a child stays at the same grade but is in a different context (is placed in a class where the average is 1 higher).  \n\n```{r}\n#| label: fig-mlak\n#| echo: false\n#| out-width: \"100%\"\n#| fig-cap: \"These plots show an actual class (red) and the counterfactual scenario (blue) where the class average is 1 higher. The large dots show comparisons between a given child in these scenarios - the left-hand plot shows the child's relative standing in the group staying the same (the within-between model estimates this), and the right-hand plot shows the child's raw value staying the same (this is what the Mundlak model estimates)\"\nccc = bfdat[1,] |> mutate(\n  fit = predict(wbmod,newdata=bfdat[1,]),\n  wbfit = fit + fixef(wbmod)[3],\n  mufit = fit + fixef(mlakmod)[3],\n)\n\ncfact = \n  bind_rows(\n    bfdat |> filter(class==1) |> mutate(c=\"actual\"),\n    bfdat |> filter(class==1) |> \n      mutate(grade=grade+1,grade_avg=grade_avg+1,c=\"counterfactual\")\n  )\ncfact$.fitted = predict(wbmod,newdata=cfact)\ncfact$.fitted[11:20]-cfact$.fitted[1:10]\n\nplotwb = broom.mixed::augment(wbmod) |>\n  mutate(grade=grade_avg+grade_dev) |>\n  ggplot(aes(x=grade,y=.fitted,group=class))+\n  geom_line(alpha=.3)+\n  geom_point(aes(y=self_concept),alpha=.1) +\n  geom_line(data=cfact,aes(col=c,group=interaction(c,class)),lwd=1) +\n  geom_point(data=cfact,aes(y=self_concept,col=c), alpha=.6) +\n  geom_point(data=ccc, aes(y=fit),col=\"red\",size=3)+\n  geom_point(data=ccc, aes(x=grade+1,y=wbfit),col=\"blue\",size=3)+\n  theme(legend.position = \"bottom\")\n\ncfact2 = \n  bind_rows(\n    bfdat |> filter(class==1) |> mutate(c=\"actual\"),\n    bfdat |> filter(class==1) |> \n      mutate(grade=grade+1,grade_avg=grade_avg+1,c=\"counterfactual\")\n  )\ncfact2$.fitted = predict(mlakmod,newdata=cfact2)\ncfact2$.fitted[11:20]-cfact2$.fitted[1:10]\n\nplotmu = broom.mixed::augment(mlakmod) |>\n  ggplot(aes(x=grade,y=.fitted,group=class))+\n  geom_line(alpha=.3)+\n  geom_point(aes(y=self_concept),alpha=.1) +\n  geom_line(data=cfact2,aes(col=c,group=interaction(c,class)),lwd=1) +\n    geom_point(data=cfact2,aes(y=self_concept,col=c), alpha=.6) +\n  geom_point(data=ccc, aes(y=fit),col=\"red\",size=3)+\n  geom_point(data=ccc, aes(y=mufit),col=\"blue\",size=3)+\n  guides(col=\"none\")\n\n\nplotwb + plotmu\n\n\n\n\n\n```\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n- Centering and scaling predictors in single level regression\n- Group-mean centering predictors in the multilevel model to separate out \"within\"-group effects from \"between\"-group effects\n- Optional: contextual effects and the Mundlak model\n\n:::\n\n\n\n## Centering predictors in `lm()`\n\nThere are lots of ways we can transform a variable. For instance, we can transform something in millimeters to being in centimeters by dividing it by 10. We could transform a `height` variable into `height above/below 2 meters` variable by subtracting 2 meters from it.   \n\nA couple of common transformations we have seen already are 'centering' and 'standardising':  \n\n- When we \"center\" a variable, we subtracting some number (often the mean of the variable) from every value. So if we 'mean-center' a variable measuring height in cm, and the mean height of my sample is 175cm, then a value of 190 becomes +15, and a value of 150 becomes -25, and so on.  \n- When we 'standardise' a variable, we mean-center it and then divide the resulting values by the standard deviation. So if the standard deviation of heights in my sample is 15, then the value of 190 becomes $\\frac{190-175}{15} = \\frac{15}{15} = 1$, and the 150 becomes $\\frac{150-175}{15} = \\frac{-25}{15} = -1.67$.  \n\nHow does this choice affect the linear models we might be fitting? The short answer is that it doesn't! The overall fit of `lm()` is not changed in any way when we apply these linear^[the fit of models _does_ change if we apply a non-linear transformation, such as $x^2$, $log(x)$, etc., and this can sometimes be useful for studying effects that are more likely to be non-linear!] transformations to predictors or outcomes. \n\nHowever, transformations _do_ change what we get out of our model:  \n\n- If we re-center a predictor on some new value (such as the mean), then all this does is change what \"zero\" means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is \"when all predictors are zero\". And we are changing what \"zero\" represents! \n- When we scale a predictor, this will change the slope. Why? Because it changes what \"moving 1\" represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains _exactly the same_, we are only changing the *units* that we are using to expressing that slope.\n\nThe example below shows a model of heart rates (`HR`) predicted by hours slept (`hrs_sleep`). In @fig-scalexlm you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don't affect the model itself - the regression line (and the uncertainty in the line) is the same in each plot. We can see that re-centering changes what the intercept represents:  \n\n- In the top left plot, \"0\" represents zero hours slept, so the intercept (big blue dot) is the estimated heart rate for someone who didn't sleep at all. \n- Similarly, in the top right plot, \"0\" now represents the mean hours slept, so the intercept is the heart rate for someone who slept an average amount, and in the bottom right plot, \"0\" now represents 8 hours of sleep (the recommended amount).\n- In the bottom left plot (where hours slept is 'standardized'), not only have we changed what \"0\" represents, but we have changed what moving \"1\" represents. Rather being an increase of 1 hour of sleep, in this plot it represents an increase of 1 standard deviation hours sleep (whatever that is for our sample - it looks to be about `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`). This means our estimated slope is the change in heart rate when having 1 SD more hours sleep (approx `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`).\n\n```{r}\n#| label: fig-scalexlm\n#| fig-cap: \"Centering and scaling predictors in linear regression models. Intercepts and their interpretation change when re-centered, and slope coefficients and their interpretation change when scaling, but the overall model stays the same.\"\n#| out-width: \"100%\"\n#| echo: false\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(title=\"Original\", x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~scale(x,scale=F),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=map_dbl(seq(7,-7), ~mean(df$x)-.),\n                     labels=seq(-7,7))+\n  labs(title=\"Mean Centered\", x=\"Hours Slept\\n(mean centered)\",y=\"HR\")\n  \n  \nmod = lm(y~scale(x),df)\np3 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),\n                     breaks=c(mean(df$x)-(2*sd(df$x)), mean(df$x)-sd(df$x), \n                              mean(df$x), \n                              mean(df$x)+sd(df$x), mean(df$x)+(2*sd(df$x))),\n                     labels=c(-2,-1,0,1,2))+\n  labs(title=\"Standardised X\", x=\"Hours Slept\\n(standardised)\",y=\"HR\")\n\nmod = lm(y~x,df %>% mutate(x=x-8))\np4 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=8,xend=8,y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=8,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14, labels=c(0:14)-8)+\n  labs(title=\"Centered on 8 hours\", x=\"Hours Slept\\n(relative to 8 hours)\",y=\"HR\")\np1 + p2 + p3 + p4 \n```\n\nThe thing to note is that the lines themselves are all the same, because the models are all exactly the same. We can prove this to ourselves by comparing the 4 models: \n\n```{r}\n#| code-fold: true\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\n\n# original model:\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n# model with hrs_sleep mean centered\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n# model with hrs_sleep standardised\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\n# model with hrs_sleep centered on 8 hours the I() function\n# is a handy function that is just needed because the symbols\n# + and - normally get interprted in lm() as adding \n# and removing predictors. \nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat) \n\n# all models are identical fit\nanova(mod_orig, mod_mc, mod_z, mod_8)\n```\n\n::: {.callout-note collapse=\"true\"}\n#### Centering when we have interactions\n\nWhen we have an interactions in a model such as `lm(y~x+z+x:z)`, the individual coefficients for `x` and `z` are specifically the associations \"when the other variable included in the interaction is zero\". Because re-centering a variable changes the meaning of \"zero\", this means that these two coefficients will change.  \n\nFor instance, a model of heart rates (`HR`) that includes an interaction between `hrs_sleep` and whether someone smokes, our coefficient for `smoke` estimates the difference in HR between smokers vs non-smokers who get zero hours of sleep (red to blue point in the left-hand plot of @fig-intcent). If we mean-center the `hrs_sleep` variable in our model, then it becomes the estimated difference in HR between smokers vs non-smokers who get the average hours of sleep (red to blue point in the right-hand plot of @fig-intcent). \n\n\n```{r}\n#| label: fig-intcent\n#| fig-cap: \"mean-centering a variable that is involved in an interaction will change the point at which the marginal effect of other variable is estimated at\" \n#| out-width: \"100%\"\n#| echo: false\nhrdat <- hrdat %>% \n  mutate(\n    smoke = ifelse(smoke %in% c(\"v\",\"y\"),\"y\",\"n\"),\n    hrs_sleepC = hrs_sleep - mean(hrs_sleep)\n  )\neg2mod <- lm(HR ~ hrs_sleep * smoke, data = hrdat)\neg2mod_cent <- lm(HR ~ hrs_sleepC * smoke, data = hrdat)\nas.data.frame(effects::effect(\"hrs_sleep*smoke\",eg2mod,\n              xlevels=list(hrs_sleep=0:15))) |>\n  ggplot(aes(x=hrs_sleep,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleep,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Original X\") +\n  \n\nas.data.frame(effects::effect(\"hrs_sleepC*smoke\",eg2mod_cent,\n              xlevels=list(hrs_sleepC=(0:15)-mean(hrdat$hrs_sleep)))) |>\n  ggplot(aes(x=hrs_sleepC,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleepC,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Mean Centered X\") +\n  plot_layout(guides=\"collect\")\n\n\n  \n```\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n## Centering predictors in multilevel models\n\nIn multilevel models, things can be a little bit different. \n\n::::{.columns}\n:::{.column width=\"65%\"}\n\nFor one thing, there can be practical benefits to centering and/or scaling predictors with respect to actually *fitting* these models. Because multilevel models involve estimating group-level variability in intercepts and slopes, if our intercept is very far away from our data (e.g., if all our data is from ages 60 to 80, and we are estimating variability at age 0), then slight changes in a slope can have huge influences on estimated intercepts, resulting in models that don't converge. We can see from the longitudinal example ([Chapter 5](05_long.html#modelling-change-over-time){target=\"_blank\"}), with the idea represented in @fig-centconv - using raw `age` values the intercepts and slopes are highly correlated and the model won't converge, but when we recenter the `age` variable on 60 and the intercept variation would become the variability in peoples' cognition _at the start of the study period_, and the random intercepts are not so determined by the random slopes.  \n:::\n\n:::{.column width=\"35%\"}\n\n```{r} \n#| label: fig-centconv\n#| fig-cap: \"lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the 'intercept' is estimated back at age 0\"  \n#| echo: false\n#| fig-height: 4\nmmd <- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nmod1 <- lmer(ACE ~ 1 + age + \n               (1 +age| ppt), \n             data = mmd)\nbroom.mixed::augment(mod1) |>\n  ggplot(aes(x=age,group=ppt))+\n  geom_point(aes(y=ACE))+\n  stat_smooth(geom=\"line\",method=lm,se=F,fullrange=T,\n              alpha=.4,\n              aes(y=.fitted))+\n  xlim(0,78)\n```\n:::\n::::\n\nHowever, in some cases (typically in observational, rather than experimental studies), having multi-level data may mean that we can actually transform a predictor in a couple of ways - we can center it on a constant number like the overall mean/min/max, but we can also consider transformations _within each group_. The key here is that we don't always have just have _one_ \"overall\" mean for a predictor, but often we have different means for each group.  \n\n### Group mean centering\n\n```{r}\n#| echo: false\nset.seed(345)\nN = 200\n  n_groups = 20\n  g = rep(1:n_groups, e = N/n_groups)\n  u = rnorm(n_groups,0,3)[g]\n  x = rnorm(N,u)\n\n  re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,0,0,.5),ncol=2))\n  re1 = re[,1][g]\n  re_x = re[,2][g]\n  lp = (0 + re1) + (1 + re_x) * x -1.5*u\n  y = rnorm(N, mean = lp, sd = 1)\n  \n  df = data.frame(grade=(scale(x)[,1]*13.6+60.214)/10, class = factor(g), \n                  self_concept=y)\n\n  # ggplot(df,aes(x=grade,y=self_concept,col=class))+\n  #   geom_point()+guides(col=\"none\")+\n  #   geom_smooth(method=lm,se=F)\n  \nbfdat <- df |> mutate(child=1:n(),grade=round(grade,2),self_concept=round(self_concept,2))\n\n#write_csv(bfdat, \"../../data/lmm_bflpe.csv\")\n```\n\n\n:::frame\n__Dataset: lmm_bflpe.csv__\n\nThese data are simulated based on the [\"Big-fish-little-pond\" effect](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6124391/){target=\"_blank\"} in educational literature. \n\nWe are interested in better understanding the relationship between school children's grades and their academic self-concept (their self-perception of ability in specific and general academic disciplines).  \n\nWe have data from `r n_distinct(bfdat$class)` classes of children, capturing information on their grades at school (range 0 to 10), and a measure of academic self-concept:  \n\n```{r}\nbfdat <- read_csv(\"https://uoepsy.github.io/data/lmm_bflpe.csv\")\nhead(bfdat)\n```\n\n:::\n\nIn initial exploratory plots, we can see that there is plenty of variation in childrens' grades - we have children scoring 3-4, all the way up to scoring almost 9. Note also, however, that the class's average grades also vary substantially. For instance, classes 8,9,10 and 11 all have a very high average grade.  \n```{r}\n#| code-fold: true\n#| fig-height: 4\nlibrary(patchwork)\nggplot(bfdat, aes(x=grade,y=self_concept))+\n  geom_point(alpha=.2) +\n\nggplot(bfdat,aes(x=class,y=grade))+\n  geom_jitter(height=0,width=.1,alpha=.2)+\n  stat_summary(geom=\"pointrange\")+\n  coord_flip()\n```\n\nWhen we plot the individual childrens' score of 'self-concept' against grades, the picture becomes a bit clearer once we separate by the classes, where we can see that *within each class* there is a fairly positive trend.  \n\n```{r}\n#| code-fold: true\n#| fig-height: 4\nggplot(bfdat,aes(x=grade,y=self_concept))+\n  geom_point(size=2,alpha=.4) +\n\nggplot(bfdat,aes(x=grade,y=self_concept))+\n  geom_point(size=2,alpha=.4)+\n  facet_wrap(~class)\n```\n\nBy contrast, the relationship between children's self-concept scores and the *average grade of their class* shows a different pattern:\n```{r}\n#| code-fold: true\n#| fig-height: 4\nbfdat <- \n  bfdat |> \n    group_by(class) |>\n    mutate(\n      grade_avg = mean(grade)\n    )\nggplot(bfdat,aes(x=grade_avg,y=self_concept))+\n  stat_summary(geom=\"pointrange\")+\n  labs(x=\"class average grade\")\n```\n\nSo there are clearly two different things going on here!  \n\n1. We have a positive association between a children's grades _relative to their peers' grades_ and their self-concept. This maybe makes sense - comparisons with other people around you will influence your feelings of self worth. \n2. We almost see a *negative* association between the average grade of a child's class and the child's self-concept --- i.e., children from classes with high grades tend to have slightly _lower_ self-concept!\n\nIn the typical multilevel model that we might fit for this study (below), we just get out one single effect estimate, which represents the expected change in self-concept when a child's grade increases by 1. But we have just seen how a child's grades are driven by two things - their class as a whole, and their relative standing in their class.  \n```{r}\n#| eval: false\nrsmod <- lmer(self_concept ~ grade + (1 + grade | class),\n              data = bfdat)\nsummary(rsmod)\n```\n```{r}\n#| echo: false\nrsmod <- lmer(self_concept ~ grade + (1 + grade | class),\n              data = bfdat)\n.pp(summary(rsmod),l=list(c(18:22)))\n```\n\nWhat we want to do here is separate out effects that are \"within\" (i.e. having higher/lower grades *than your classmates*) from those that are \"between\" (i.e. being from a class with higher/lower grades than other classes). To get at these two effects, we are going to explicitly separate our predictor variable into two different parts: \n\n1. the group average\n2. individual deviations from the group average. \n\nWe can calculate these by first using `group_by()` to make the calculations be applied separately for each class, and then calculating the `mean()` grade (for each class), and the deviations for each child from their class's average:  \n\n```{r}\nbfdat <- \n  bfdat |> \n  group_by(class) |>\n  mutate(\n    grade_avg = mean(grade),\n    grade_dev = grade - mean(grade)\n  )\n\nhead(bfdat)\n```\n\nNote that the actual grade for each child can still be made from our two new columns, calculated as the `avg_grade + grade_dev`.  \n\nSo let's plot the association between each of these new variables and the self-concept scores:\n```{r}\n#| code-fold: true\n#| fig-height: 4\nggplot(bfdat, aes(x=grade_avg, y=self_concept))+\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n\nggplot(bfdat, aes(x=grade_dev, y=self_concept))+\n  geom_point() +\n  geom_smooth(method=\"lm\")\n```\n\nSo we can see that there are two different things going on here - the effect on self-concept of being in a high-performing class, as well as the effect of performing higher _for your class_.  \n\n\n## The within-between model\n\nNow that we have split up the variable grade into two parts (group average, and deviations-from-group-averages), we can actually put these in as separate predictors into our model!  \n\nThis type of model is sometimes referred to as a \"within-between\" model. You can see below both the standard model with random slopes, and the 'within-between' model, in both `lmer()` syntax and in equation form.  \n\nNote that we while we replace one predictor (`x`) with its two constituent parts (the group means of `x` and the deviations from those group means), it is only the within effect that we can have a random slope for. This will hopefully make sense when we think a little about it, because the group-means are \"between groups\" - having a random slope of `group_mean_x|group` is similar to the idea of `handedness|person`, because for a single group, we don't have \"an effect on y of that group having a high average x\", so we can't consider it to be an effect that varies by-group.  \n\n\n\n::::panelset\n:::panel\n#### random slopes model\n\n```{r}\n#| echo: true\n#| eval: false\nlmer(y ~ 1 + x + (1 + x | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n\n:::panel\n#### within-between model  \n\n```{r}\n#| echo: true\n#| eval: false\ndata <- data |>\n  group_by(g) |>\n  mutate(\n    x_avg = mean(x),\n    x_dev = x - mean(x)\n  )\n\nlmer(y ~ 1 + x_dev + x_avg + (1 + x_dev | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot (x_{ij} - \\bar{x}_i) + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n::::\n\n<br>    \nIn the context of our educational study of grades and self-concept in school children, we can fit a model that disaggregates within (grade relative to class) and between (class average grade) effects:    \n\n```{r}\nwbmod <- lmer(self_concept ~ grade_dev + grade_avg + \n                (1 + grade_dev|class), \n              data = bfdat)\n```\n\nThe fixed effects from this model (below) now show two effects, as opposed to only one that we would get from our typical model:   \n\n::::{.columns}\n:::{.column width=\"50%\"}\n__typical random slopes model__\n\n```{r}\n#| echo: false\nbroom.mixed::tidy(rsmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n\n:::{.column width=\"50%\"}\n__within-between model__  \n\n```{r}\n#| echo: false\nbroom.mixed::tidy(wbmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n::::\n\n- the \"within\" effect: for every one grade higher a child is *relative to their classmates*, their self-concept is expected to increase by `r round(fixef(wbmod)[2],2)`.\n- the \"between\" effect: for every 1 grade higher a class average is (and when a child's relative standing in the class stays constant), a child's self-concept is expected to decrease by `r round(fixef(wbmod)[3],2)`.\n\nSo what exactly does the effect (the estimate of `r round(fixef(rsmod)[2],2)`) from our more traditional model show here? Is it the within effect or the between effect? It's actually a smushing together of both parts - it is the estimated effect on self-concept when a child's grade increases by 1, but it is confounded by the fact that as childrens' grades increase then their class average increases a bit too, meaning that the between effect pulls this back down. In short - it's not actually a very useful estimate for us at all, because it conflates the two different effects. \n\n\n\n\n```{r}\n#| eval: false\n#| echo: false\ngconf = function(){\n  N = 200\n  n_groups = 20\n  g = rep(1:n_groups, e = N/n_groups)\n  u = rnorm(n_groups,0,5)[g]\n  x = rnorm(N,u)\n\n  re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,0,0,.5),ncol=2))\n  re1 = re[,1][g]\n  re_x = re[,2][g]\n  lp = (0 + re1) + (1 + re_x) * x + -3*u\n  y = rnorm(N, mean = lp, sd = 1)\n  \n  df = data.frame(x, g = factor(g), y, y)\n  # ggplot(df,aes(x=x,y=y,col=g))+\n  #   geom_point()+guides(col=\"none\")+\n  #   geom_smooth(method=lm,se=F)\n  \n  c(\n    ri=fixef(lmer(y~1+x+(1|g),df))['x'],\n    rs=fixef(lmer(y~1+x+(1+x|g),df))['x'],\n    mui=fixef(lmer(y~1+x+xm+(1|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mu=fixef(lmer(y~1+x+xm+(1+x|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mwbi=fixef(lmer(y~1+xd+xm+(1|g),df |> group_by(g) |>mutate(xm=mean(x),xd=x-xm)))['xd'],\n    mwb=fixef(lmer(y~1+xd+xm+(1+xd|g),df |> group_by(g) |>mutate(xm=mean(x),xd=x-xm)))['xd']\n  )\n}\n\nres = t(replicate(100,gconf()))\npar(mfrow=c(3,2))\nhist(res[,1]);hist(res[,2]);hist(res[,3]);hist(res[,4]);hist(res[,5])\npar(mfrow=c(1,1))\ncolMeans(res)\napply(res,2,sd)\n```\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n## Optional: contextual effects and the mundlak model\n\nAlong with the within-between model, we could also choose to adjust for the group averages while continuing to use the original raw predictor in the model. This is often called the \"Mundlak model\" in reference to [Yair Mundlak](https://doi.org/10.2307/1913646){target=\"_blank\"} who wrote about it in the context of avoiding group-level confounding (see [Chapter 9 #optional-extra-group-confounding](09_assump.html#optional-extra-group-confounding-and-the-random-effects-assumption){target=\"_blank\"}).\n\nThe formulation is very similar to the within-between model, but we don't use the \"deviations from group means\", we simply use the original predictor along with the group means:  \n\n\n::::panelset\n:::panel\n#### within-between model  \n\n```{r}\n#| echo: true\n#| eval: false\ndata <- data |>\n  group_by(g) |>\n  mutate(\n    x_avg = mean(x),\n    x_dev = x - mean(x)\n  )\n\nlmer(y ~ 1 + x_dev + x_avg + (1 + x_dev | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot (x_{ij} - \\bar{x}_i) + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n:::panel\n#### mundlak model\n\n```{r}\n#| echo: true\n#| eval: false\ndata <- data |>\n  group_by(g) |>\n  mutate(\n    x_avg = mean(x)\n  )\n\nlmer(y ~ 1 + x + x_avg + (1 + x | g), data)\n```\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + b_{2} \\cdot \\bar{x}_i + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho_{01} \\\\\n        \\rho_{01} & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n::::\n\n<br>\nWe can fit the Mundlak formulation to our grades & self-concept data as follows:  \n```{r}\nmlakmod <- lmer(self_concept ~ grade + grade_avg + \n                  (1 + grade|class), \n                data = bfdat)\n```\n\nThere are two things to note here when comparing the Mundlak formulation to the 'within-between' model. Firstly, these two models provide the same 'within' effect (the fixed effects of `grade_dev` and `grade` in the tables below), because they both get the effect of a child's grade increasing by 1, while holding their class's average grade constant.^[when we exclude the random intercepts, the within-effects from both models are numerically identical, but there may be small differences due to the random slopes of `x|g` for the mundlak model and `x_dev|g` for the within-between model] Secondly, the estimated effects for the `grade_avg` predictor differ substantially between the two models: \n\n::::{.columns}\n:::{.column width=\"50%\"}\n__Within-between model__  \n```{r}\n#| echo: false\nbroom.mixed::tidy(wbmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n\n:::{.column width=\"50%\"}\n__Mundlak model__  \n```{r}\n#| echo: false\nbroom.mixed::tidy(mlakmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(term,est=round(estimate,2),\n         SE=round(std.error,2),\n         t = round(statistic,2)) |>\n  gt::gt()\n```\n:::\n::::\n\n\nThe difference here is that they are capturing two distinct effects. The within-between formulation captures a 'between effect' and the Mundlak formulation provides something that gets termed the \"contextual effect\".  \n\nThe key thing to distinguish between these two is to think about what is being \"held constant\".\nIn the within-between model, the effect of `avg_grade` is estimated while holding constant the child's _relative standing_ in the group. In the Mundlak model, the effect is estimated while holding constant a child's _actual_ grade.  \n\nIt may help to think about this in terms of a single child. Suppose we have a child who has a grade of `r bfdat$grade[1]`, from a class with an average of `r bfdat$grade_avg[1]`. So that child is `r bfdat$grade_dev[1]` _above their class average._  \n```{r}\n#| echo: false\nbfdat[1, ] |> transmute(class=\"i\", child=\"j\", grade_avg, grade_dev, self_concept=\"y\")\n```\n\nBoth models are estimating \"what would we expect to happen to the child's self-concept if their class had an average of `r bfdat$grade_avg[1]+1` instead?\"  \n\nThe within-between model estimates this but holds constant the child's deviation above the average, so we're comparing the scenario where the child is `r bfdat$grade_dev[1]` above a class average of `r bfdat$grade_avg[1]+1`, as opposed to being `r bfdat$grade_dev[1]` above a class average of `r bfdat$grade_avg[1]`.\n```{r}\n#| echo: false\nbind_rows(\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg, grade_dev, grade, self_concept=\"y\"),\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg=grade_avg+1, grade_dev, grade=grade+1,\n                        self_concept=paste0(\"y\", round(fixef(wbmod)[3],2)))\n)\n```\n\nBy contrast, the Mundlak model holds constant the child's _actual_ grade, meaning that we're comparing the scenario where the child has a grade of `r bfdat$grade[1]` and is in a class with an average of `r bfdat$grade_avg[1]+1`, as opposed to having that same grade of `r bfdat$grade[1]` but being in a class with an average of `r bfdat$grade_avg[1]`:  \n```{r}\n#| echo: false\nbind_rows(\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg, grade, self_concept=\"y\"),\n  bfdat[1, ] |> transmute(class=\"i\", child=\"j\", \n                        grade_avg=grade_avg+1, grade, \n                        self_concept=paste0(\"y\", round(fixef(mlakmod)[3],2)))\n)\n```\n\nWe can think of this visually. Take a given child from a given class, and think about what would happen if their whole class average grade increased by 1. In @fig-mlak, the red line is \"class 1\" in our data, and the blue line is a counterfactual world of \"class 1 if its average grade increased by 1\". The large red dot represents the expected self-concept for \"child 1\".  \n\nIn the within-between model, we're estimating the self-concept difference for a child between the red (actual) and blue (counterfactual class with a higher average), but where the child stays the same amount \"above average\" in this counterfactual class. In the Mundlak model, we're estimating the self-concept difference when a child stays at the same grade but is in a different context (is placed in a class where the average is 1 higher).  \n\n```{r}\n#| label: fig-mlak\n#| echo: false\n#| out-width: \"100%\"\n#| fig-cap: \"These plots show an actual class (red) and the counterfactual scenario (blue) where the class average is 1 higher. The large dots show comparisons between a given child in these scenarios - the left-hand plot shows the child's relative standing in the group staying the same (the within-between model estimates this), and the right-hand plot shows the child's raw value staying the same (this is what the Mundlak model estimates)\"\nccc = bfdat[1,] |> mutate(\n  fit = predict(wbmod,newdata=bfdat[1,]),\n  wbfit = fit + fixef(wbmod)[3],\n  mufit = fit + fixef(mlakmod)[3],\n)\n\ncfact = \n  bind_rows(\n    bfdat |> filter(class==1) |> mutate(c=\"actual\"),\n    bfdat |> filter(class==1) |> \n      mutate(grade=grade+1,grade_avg=grade_avg+1,c=\"counterfactual\")\n  )\ncfact$.fitted = predict(wbmod,newdata=cfact)\ncfact$.fitted[11:20]-cfact$.fitted[1:10]\n\nplotwb = broom.mixed::augment(wbmod) |>\n  mutate(grade=grade_avg+grade_dev) |>\n  ggplot(aes(x=grade,y=.fitted,group=class))+\n  geom_line(alpha=.3)+\n  geom_point(aes(y=self_concept),alpha=.1) +\n  geom_line(data=cfact,aes(col=c,group=interaction(c,class)),lwd=1) +\n  geom_point(data=cfact,aes(y=self_concept,col=c), alpha=.6) +\n  geom_point(data=ccc, aes(y=fit),col=\"red\",size=3)+\n  geom_point(data=ccc, aes(x=grade+1,y=wbfit),col=\"blue\",size=3)+\n  theme(legend.position = \"bottom\")\n\ncfact2 = \n  bind_rows(\n    bfdat |> filter(class==1) |> mutate(c=\"actual\"),\n    bfdat |> filter(class==1) |> \n      mutate(grade=grade+1,grade_avg=grade_avg+1,c=\"counterfactual\")\n  )\ncfact2$.fitted = predict(mlakmod,newdata=cfact2)\ncfact2$.fitted[11:20]-cfact2$.fitted[1:10]\n\nplotmu = broom.mixed::augment(mlakmod) |>\n  ggplot(aes(x=grade,y=.fitted,group=class))+\n  geom_line(alpha=.3)+\n  geom_point(aes(y=self_concept),alpha=.1) +\n  geom_line(data=cfact2,aes(col=c,group=interaction(c,class)),lwd=1) +\n    geom_point(data=cfact2,aes(y=self_concept,col=c), alpha=.6) +\n  geom_point(data=ccc, aes(y=fit),col=\"red\",size=3)+\n  geom_point(data=ccc, aes(y=mufit),col=\"blue\",size=3)+\n  guides(col=\"none\")\n\n\nplotwb + plotmu\n\n\n\n\n\n```\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"10_centering.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","toc_float":true,"link-citations":true,"theme":["cosmo","assets/style-labs.scss"],"title":"10: Centering","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}