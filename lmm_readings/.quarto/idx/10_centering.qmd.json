{"title":"10: Centering","markdown":{"yaml":{"title":"10: Centering","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Centering predictors in `lm()`","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n**FORTHCOMING**\n\n:::\n\n\n\n\nThere are lots of ways we can transform a variable. For instance, we can transform something in millimeters to being in centimeters by dividing it by 10. We could transform a `height` variable into `height above/below 2 meters` variable by subtracting 2 meters from it.   \n\nA couple of common transformations we have seen already are 'centering' and 'standardising'. \nWhen we \"center\" a variable, we subtracting some number (often the mean of the variable) from every value. So if we 'mean-center' a variable measuring height in cm, and the mean height of my sample is 175cm, then a value of 190 becomes +15, and a value of 150 becomes -25, and so on.  \nWhen we 'standardise' a variable, we mean-center it and then divide the resulting values by the standard deviation. So if the standard deviation of heights in my sample is 15, then the value of 190 becomes $\\frac{190-175}{15} = \\frac{15}{15} = 1$, and the 150 becomes $\\frac{150-175}{15} = \\frac{-25}{15} = -1.67$.  \n\nHow does this choice affect the linear models we might be fitting? The short answer is that it doesn't! The overall fit of `lm()` is not changed in any way when we apply these linear transformations to predictors or outcomes.^[the fit of models _does_ change if we apply a non-linear transformation, such as $x^2$, $log(x)$, etc., and this can sometimes be useful for studying effects that are more likely to be non-linear!]. However, transformations _do_ change what we get out of our model. \n\nIf we re-center a predictor on some new value (such as the mean), then all this does is change what \"zero\" means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is \"when all predictors are zero\". And we are changing what \"zero\" represents! When we scale a predictor, this will change the slope. Why? Because it changes what \"moving 1\" represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains _exactly the same_, we are only changing the *units* that we are using to expressing that slope.\n\nThe example below shows a model of heart rates (`HR`) predicted by hours slept (`hrs_sleep`). In @fig-scalexlm you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don't affect the model itself - the regression line (and the uncertainty in the line) is the same in each plot. We can see that re-centering changes what the intercept represents. In the top left plot, \"0\" represents zero hours slept, so the intercept is the estimated heart rate for someone who didn't sleep at all. Similarly, in the top right plot, \"0\" now represents the mean hours slept, so the intercept is the heart rate for someone who slept an average amount. In the bottom left plot (where hours slept is 'standardized'), not only have we changed what \"0\" represents, but we have changed what moving \"1\" represents. Rather being an increase of 1 hour of sleep, in this plot it represents an increase of 1 standard deviation hours sleep (whatever that is for our sample - it looks to be about `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`). This means our estimated slope is the change in heart rate when having 1 SD more hours sleep (approx `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`).\n\n```{r}\n#| label: fig-scalexlm\n#| fig-cap: \"Centering and scaling predictors in linear regression models. Intercepts and their interpretation change when re-centered, and slope coefficients and their interpretation change when scaling, but the overall model stays the same.\"\n#| out-width: \"100%\"\n#| echo: false\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(title=\"Original\", x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~scale(x,scale=F),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=map_dbl(seq(7,-7), ~mean(df$x)-.),\n                     labels=seq(-7,7))+\n  labs(title=\"Mean Centered\", x=\"Hours Slept\\n(mean centered)\",y=\"HR\")\n  \n  \nmod = lm(y~scale(x),df)\np3 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),\n                     breaks=c(mean(df$x)-(2*sd(df$x)), mean(df$x)-sd(df$x), \n                              mean(df$x), \n                              mean(df$x)+sd(df$x), mean(df$x)+(2*sd(df$x))),\n                     labels=c(-2,-1,0,1,2))+\n  labs(title=\"Standardised X\", x=\"Hours Slept\\n(standardised)\",y=\"HR\")\n\nmod = lm(y~x,df %>% mutate(x=x-8))\np4 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=8,xend=8,y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=8,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14, labels=c(0:14)-8)+\n  labs(title=\"Centered on 8 hours\", x=\"Hours Slept\\n(relative to 8 hours)\",y=\"HR\")\np1 + p2 + p3 + p4 \n```\n\nThe thing to note is that the lines themselves are all the same, because the models are all exactly the same. We can prove this to ourselves by comparing the 4 models: \n\n```{r}\n#| code-fold: true\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\n\n# original model:\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n# model with hrs_sleep mean centered\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n# model with hrs_sleep standardised\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\n# model with hrs_sleep centered on 8 hours the I() function is a handy function that is just needed because the + and - symbols normally get interprted in lm() as adding and removing predictors. \nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat) \n\n# all models are identical fit\nanova(mod_orig, mod_mc, mod_z, mod_8)\n```\n\n::: {.callout-note collapse=\"true\"}\n#### Centering when we have interactions\n\nWhen we have an interactions in a model such as `lm(y~x+z+x:z)`, the individual coefficients for `x` and `z` are specifically the associations \"when the other variable included in the interaction is zero\". Because re-centering a variable changes the meaning of \"zero\", this means that these two coefficients will change.  \n\nFor instance, a model of heart rates (`HR`) that includes an interaction between `hrs_sleep` and whether someone smokes, our coefficient for `smoke` estimates the difference in HR between smokers vs non-smokers who get zero hours of sleep (red to blue point in the left-hand plot of @fig-intcent). If we mean-center the `hrs_sleep` variable in our model, then it becomes the estimated difference in HR between smokers vs non-smokers who get the average hours of sleep (red to blue point in the right-hand plot of @fig-intcent). \n\n\n```{r}\n#| label: fig-intcent\n#| fig-cap: \"mean-centering a variable that is involved in an interaction will change the point at which the marginal effect of other variable is estimated at\" \n#| out-width: \"100%\"\n#| echo: false\nhrdat <- hrdat %>% \n  mutate(\n    smoke = ifelse(smoke %in% c(\"v\",\"y\"),\"y\",\"n\"),\n    hrs_sleepC = hrs_sleep - mean(hrs_sleep)\n  )\neg2mod <- lm(HR ~ hrs_sleep * smoke, data = hrdat)\neg2mod_cent <- lm(HR ~ hrs_sleepC * smoke, data = hrdat)\nas.data.frame(effects::effect(\"hrs_sleep*smoke\",eg2mod,\n              xlevels=list(hrs_sleep=0:15))) |>\n  ggplot(aes(x=hrs_sleep,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleep,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Original X\") +\n  \n\nas.data.frame(effects::effect(\"hrs_sleepC*smoke\",eg2mod_cent,\n              xlevels=list(hrs_sleepC=(0:15)-mean(hrdat$hrs_sleep)))) |>\n  ggplot(aes(x=hrs_sleepC,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleepC,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Mean Centered X\") +\n  plot_layout(guides=\"collect\")\n\n\n  \n```\n\n:::\n\n## Centering predictors in multilevel models\n\nIn multilevel models, things can be a little bit different. We have already seen in the longitudinal example ([Chapter 5](05_long.html#modelling-change-over-time){target=\"_blank\"}) that recentering a variable can have certain advantages in allowing us to fit multilevel models.  \n\nBecause multilevel models involve estimating group-level variability in intercepts and slopes, if our intercept is very far away from our data (e.g., if all our data is from ages 60 to 80, and we are estimating variability at age 0), then slight changes in a slope can have huge influences on intercepts (@fig-centconv), resulting in models that don't converge.\n\n```{r} \n#| label: fig-centconv\n#| fig-cap: \"lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the 'intercept' is estimated back at age 0\"  \n#| echo: false\nmmd <- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nmod1 <- lmer(ACE ~ 1 + age + \n               (1 +age| ppt), \n             data = mmd)\nbroom.mixed::augment(mod1) |>\n  ggplot(aes(x=age,group=ppt))+\n  geom_point(aes(y=ACE))+\n  stat_smooth(geom=\"line\",method=lm,se=F,fullrange=T,\n              alpha=.4,\n              aes(y=.fitted))+\n  xlim(0,78)\n```\n\nSo re-centering a predictor can help both to allow models to converge and to provide estimates of variability that are at more meaningful places (e.g., we could recenter the `age` variable on 60 and the intercept variation would become the variability in peoples' cognition _at the start of the study period_). Beyond that, re-centering on a constant number (such as the overall mean, min or max) doesn't change much in terms of our fixed effects. \n\nHowever, in some cases (typically in observational, rather than experimental studies), having multi-level data may mean that we don't just have _one_ \"overall\" mean for a predictor, but also a separate mean for each group.  \n\n### group mean centering\n\nLet's suppose we are interested in studying school childrens' self-concept. \n\nwe have classes\nclasses vary in the average grades\nplot x y\nplot x y and line for each class\n\nwhat if the association btw grade and selfconc is actually entirely down to your 'grade relative to peers'? \n\nin order to study 'grade relative to peers' we need to consider each child's relative standing - i.e. is it high _for the class_ or low _for the class_?  \n\nwe can look at \n\n```{r}\n\n```\n\n\n### within and between effects\n\n\n\n\n### the within between model\n\ny ~ 1 + xbarj + (x-xbarj) + ( 1 + (x-xbarj) | g )\n\n\n```{r}\n#| eval: false\ngconf = function(){\n  N = 200\n  n_groups = 20\n  g = rep(1:n_groups, e = N/n_groups)\n  u = rnorm(n_groups,0,5)[g]\n  x = rnorm(N,u)\n\n  re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,0,0,.5),ncol=2))\n  re1 = re[,1][g]\n  re_x = re[,2][g]\n  lp = (0 + re1) + (1 + re_x) * x + -3*u\n  y = rnorm(N, mean = lp, sd = 1)\n  \n  df = data.frame(x, g = factor(g), y, y)\n  # ggplot(df,aes(x=x,y=y,col=g))+\n  #   geom_point()+guides(col=\"none\")+\n  #   geom_smooth(method=lm,se=F)\n  \n  c(\n    ri=fixef(lmer(y~1+x+(1|g),df))['x'],\n    rs=fixef(lmer(y~1+x+(1+x|g),df))['x'],\n    mui=fixef(lmer(y~1+x+xm+(1|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mu=fixef(lmer(y~1+x+xm+(1+x|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mwb=fixef(lmer(y~1+xd+xm+(1+xd|g),df |> group_by(g) |>mutate(xm=mean(x),xd=x-xm)))['xd']\n  )\n}\n\nres = t(replicate(100,gconf()))\npar(mfrow=c(3,2))\nhist(res[,1]);hist(res[,2]);hist(res[,3]);hist(res[,4]);hist(res[,5])\npar(mfrow=c(1,1))\ncolMeans(res)\napply(res,2,sd)\n```\n\n\n### optional: contextual effects and the mundlak model\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n**FORTHCOMING**\n\n:::\n\n\n\n## Centering predictors in `lm()`\n\nThere are lots of ways we can transform a variable. For instance, we can transform something in millimeters to being in centimeters by dividing it by 10. We could transform a `height` variable into `height above/below 2 meters` variable by subtracting 2 meters from it.   \n\nA couple of common transformations we have seen already are 'centering' and 'standardising'. \nWhen we \"center\" a variable, we subtracting some number (often the mean of the variable) from every value. So if we 'mean-center' a variable measuring height in cm, and the mean height of my sample is 175cm, then a value of 190 becomes +15, and a value of 150 becomes -25, and so on.  \nWhen we 'standardise' a variable, we mean-center it and then divide the resulting values by the standard deviation. So if the standard deviation of heights in my sample is 15, then the value of 190 becomes $\\frac{190-175}{15} = \\frac{15}{15} = 1$, and the 150 becomes $\\frac{150-175}{15} = \\frac{-25}{15} = -1.67$.  \n\nHow does this choice affect the linear models we might be fitting? The short answer is that it doesn't! The overall fit of `lm()` is not changed in any way when we apply these linear transformations to predictors or outcomes.^[the fit of models _does_ change if we apply a non-linear transformation, such as $x^2$, $log(x)$, etc., and this can sometimes be useful for studying effects that are more likely to be non-linear!]. However, transformations _do_ change what we get out of our model. \n\nIf we re-center a predictor on some new value (such as the mean), then all this does is change what \"zero\" means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is \"when all predictors are zero\". And we are changing what \"zero\" represents! When we scale a predictor, this will change the slope. Why? Because it changes what \"moving 1\" represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains _exactly the same_, we are only changing the *units* that we are using to expressing that slope.\n\nThe example below shows a model of heart rates (`HR`) predicted by hours slept (`hrs_sleep`). In @fig-scalexlm you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don't affect the model itself - the regression line (and the uncertainty in the line) is the same in each plot. We can see that re-centering changes what the intercept represents. In the top left plot, \"0\" represents zero hours slept, so the intercept is the estimated heart rate for someone who didn't sleep at all. Similarly, in the top right plot, \"0\" now represents the mean hours slept, so the intercept is the heart rate for someone who slept an average amount. In the bottom left plot (where hours slept is 'standardized'), not only have we changed what \"0\" represents, but we have changed what moving \"1\" represents. Rather being an increase of 1 hour of sleep, in this plot it represents an increase of 1 standard deviation hours sleep (whatever that is for our sample - it looks to be about `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`). This means our estimated slope is the change in heart rate when having 1 SD more hours sleep (approx `r round(sd(read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")$hrs_sleep)*2)/2`).\n\n```{r}\n#| label: fig-scalexlm\n#| fig-cap: \"Centering and scaling predictors in linear regression models. Intercepts and their interpretation change when re-centered, and slope coefficients and their interpretation change when scaling, but the overall model stays the same.\"\n#| out-width: \"100%\"\n#| echo: false\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\ndf = hrdat |> mutate(x=hrs_sleep,y=HR)\nmod = lm(y~x,df)\np1 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=0,xend=0,y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=3,col=\"blue\")+\n  labs(title=\"Original\", x=\"Hours Slept\",y=\"HR\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14)\n\nmod = lm(y~scale(x,scale=F),df)\np2 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=map_dbl(seq(7,-7), ~mean(df$x)-.),\n                     labels=seq(-7,7))+\n  labs(title=\"Mean Centered\", x=\"Hours Slept\\n(mean centered)\",y=\"HR\")\n  \n  \nmod = lm(y~scale(x),df)\np3 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),\n                     breaks=c(mean(df$x)-(2*sd(df$x)), mean(df$x)-sd(df$x), \n                              mean(df$x), \n                              mean(df$x)+sd(df$x), mean(df$x)+(2*sd(df$x))),\n                     labels=c(-2,-1,0,1,2))+\n  labs(title=\"Standardised X\", x=\"Hours Slept\\n(standardised)\",y=\"HR\")\n\nmod = lm(y~x,df %>% mutate(x=x-8))\np4 = ggplot(df, aes(x=x,y=y))+geom_point(alpha=.3)+\n  geom_smooth(method=\"lm\")+\n  geom_smooth(method=\"lm\",fullrange=T,lty=\"dashed\",se=F)+\n  #ylim(0,max(df$y))+\n  geom_segment(x=8,xend=8,y=0,yend=100)+\n  geom_segment(x=0,xend=30,y=0,yend=0)+\n  geom_point(data=tibble(x=8,y=coef(mod)[1]),size=3,col=\"blue\")+\n  scale_x_continuous(limits=c(0,14),breaks=0:14, labels=c(0:14)-8)+\n  labs(title=\"Centered on 8 hours\", x=\"Hours Slept\\n(relative to 8 hours)\",y=\"HR\")\np1 + p2 + p3 + p4 \n```\n\nThe thing to note is that the lines themselves are all the same, because the models are all exactly the same. We can prove this to ourselves by comparing the 4 models: \n\n```{r}\n#| code-fold: true\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\n\n# original model:\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n# model with hrs_sleep mean centered\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n# model with hrs_sleep standardised\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\n# model with hrs_sleep centered on 8 hours the I() function is a handy function that is just needed because the + and - symbols normally get interprted in lm() as adding and removing predictors. \nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat) \n\n# all models are identical fit\nanova(mod_orig, mod_mc, mod_z, mod_8)\n```\n\n::: {.callout-note collapse=\"true\"}\n#### Centering when we have interactions\n\nWhen we have an interactions in a model such as `lm(y~x+z+x:z)`, the individual coefficients for `x` and `z` are specifically the associations \"when the other variable included in the interaction is zero\". Because re-centering a variable changes the meaning of \"zero\", this means that these two coefficients will change.  \n\nFor instance, a model of heart rates (`HR`) that includes an interaction between `hrs_sleep` and whether someone smokes, our coefficient for `smoke` estimates the difference in HR between smokers vs non-smokers who get zero hours of sleep (red to blue point in the left-hand plot of @fig-intcent). If we mean-center the `hrs_sleep` variable in our model, then it becomes the estimated difference in HR between smokers vs non-smokers who get the average hours of sleep (red to blue point in the right-hand plot of @fig-intcent). \n\n\n```{r}\n#| label: fig-intcent\n#| fig-cap: \"mean-centering a variable that is involved in an interaction will change the point at which the marginal effect of other variable is estimated at\" \n#| out-width: \"100%\"\n#| echo: false\nhrdat <- hrdat %>% \n  mutate(\n    smoke = ifelse(smoke %in% c(\"v\",\"y\"),\"y\",\"n\"),\n    hrs_sleepC = hrs_sleep - mean(hrs_sleep)\n  )\neg2mod <- lm(HR ~ hrs_sleep * smoke, data = hrdat)\neg2mod_cent <- lm(HR ~ hrs_sleepC * smoke, data = hrdat)\nas.data.frame(effects::effect(\"hrs_sleep*smoke\",eg2mod,\n              xlevels=list(hrs_sleep=0:15))) |>\n  ggplot(aes(x=hrs_sleep,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleep,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod)[1], yend=sum(coef(eg2mod)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Original X\") +\n  \n\nas.data.frame(effects::effect(\"hrs_sleepC*smoke\",eg2mod_cent,\n              xlevels=list(hrs_sleepC=(0:15)-mean(hrdat$hrs_sleep)))) |>\n  ggplot(aes(x=hrs_sleepC,col=smoke,fill=smoke,\n             y=fit,ymin=lower,ymax=upper))+\n  geom_ribbon(alpha=.1,col=NA)+\n  geom_smooth(method=lm,se=F,fullrange=T)+\n  geom_point(data=hrdat,inherit.aes=F,\n             aes(y=HR,x=hrs_sleepC,col=smoke),\n             size=3,alpha=.2)+\n  geom_point(x=0,y=coef(eg2mod_cent)[1], size=4, aes(col=\"n\"))+\n  geom_point(x=0,y=sum(coef(eg2mod_cent)[c(1,3)]), size=4, aes(col=\"y\"))+\n  geom_segment(x=0,xend=0,y=coef(eg2mod_cent)[1], yend=sum(coef(eg2mod_cent)[c(1,3)]), lty=\"dotted\", lwd=1,col=\"black\") + labs(title=\"Mean Centered X\") +\n  plot_layout(guides=\"collect\")\n\n\n  \n```\n\n:::\n\n## Centering predictors in multilevel models\n\nIn multilevel models, things can be a little bit different. We have already seen in the longitudinal example ([Chapter 5](05_long.html#modelling-change-over-time){target=\"_blank\"}) that recentering a variable can have certain advantages in allowing us to fit multilevel models.  \n\nBecause multilevel models involve estimating group-level variability in intercepts and slopes, if our intercept is very far away from our data (e.g., if all our data is from ages 60 to 80, and we are estimating variability at age 0), then slight changes in a slope can have huge influences on intercepts (@fig-centconv), resulting in models that don't converge.\n\n```{r} \n#| label: fig-centconv\n#| fig-cap: \"lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the 'intercept' is estimated back at age 0\"  \n#| echo: false\nmmd <- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nmod1 <- lmer(ACE ~ 1 + age + \n               (1 +age| ppt), \n             data = mmd)\nbroom.mixed::augment(mod1) |>\n  ggplot(aes(x=age,group=ppt))+\n  geom_point(aes(y=ACE))+\n  stat_smooth(geom=\"line\",method=lm,se=F,fullrange=T,\n              alpha=.4,\n              aes(y=.fitted))+\n  xlim(0,78)\n```\n\nSo re-centering a predictor can help both to allow models to converge and to provide estimates of variability that are at more meaningful places (e.g., we could recenter the `age` variable on 60 and the intercept variation would become the variability in peoples' cognition _at the start of the study period_). Beyond that, re-centering on a constant number (such as the overall mean, min or max) doesn't change much in terms of our fixed effects. \n\nHowever, in some cases (typically in observational, rather than experimental studies), having multi-level data may mean that we don't just have _one_ \"overall\" mean for a predictor, but also a separate mean for each group.  \n\n### group mean centering\n\nLet's suppose we are interested in studying school childrens' self-concept. \n\nwe have classes\nclasses vary in the average grades\nplot x y\nplot x y and line for each class\n\nwhat if the association btw grade and selfconc is actually entirely down to your 'grade relative to peers'? \n\nin order to study 'grade relative to peers' we need to consider each child's relative standing - i.e. is it high _for the class_ or low _for the class_?  \n\nwe can look at \n\n```{r}\n\n```\n\n\n### within and between effects\n\n\n\n\n### the within between model\n\ny ~ 1 + xbarj + (x-xbarj) + ( 1 + (x-xbarj) | g )\n\n\n```{r}\n#| eval: false\ngconf = function(){\n  N = 200\n  n_groups = 20\n  g = rep(1:n_groups, e = N/n_groups)\n  u = rnorm(n_groups,0,5)[g]\n  x = rnorm(N,u)\n\n  re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,0,0,.5),ncol=2))\n  re1 = re[,1][g]\n  re_x = re[,2][g]\n  lp = (0 + re1) + (1 + re_x) * x + -3*u\n  y = rnorm(N, mean = lp, sd = 1)\n  \n  df = data.frame(x, g = factor(g), y, y)\n  # ggplot(df,aes(x=x,y=y,col=g))+\n  #   geom_point()+guides(col=\"none\")+\n  #   geom_smooth(method=lm,se=F)\n  \n  c(\n    ri=fixef(lmer(y~1+x+(1|g),df))['x'],\n    rs=fixef(lmer(y~1+x+(1+x|g),df))['x'],\n    mui=fixef(lmer(y~1+x+xm+(1|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mu=fixef(lmer(y~1+x+xm+(1+x|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mwb=fixef(lmer(y~1+xd+xm+(1+xd|g),df |> group_by(g) |>mutate(xm=mean(x),xd=x-xm)))['xd']\n  )\n}\n\nres = t(replicate(100,gconf()))\npar(mfrow=c(3,2))\nhist(res[,1]);hist(res[,2]);hist(res[,3]);hist(res[,4]);hist(res[,5])\npar(mfrow=c(1,1))\ncolMeans(res)\napply(res,2,sd)\n```\n\n\n### optional: contextual effects and the mundlak model\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"10_centering.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["cosmo","assets/style-labs.scss"],"title":"10: Centering","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}