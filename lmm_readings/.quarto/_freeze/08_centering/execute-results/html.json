{
  "hash": "699c87642eedd3c28dcd94b2f958f0ce",
  "result": {
    "markdown": "---\ntitle: \"8: Centering\"\nparams: \n    SHOW_SOLS: FALSE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n:::lo\nThis reading:  \n\n**FORTHCOMING**\n\n:::\n\n\n\n## Centering predictors in `lm()`\n\nThere are lots of ways we can transform a variable. For instance, we can transform something in millimeters to being in centimeters by dividing it by 10. We could transform a `height` variable into `height above/below 2 meters` variable by subtracting 2 meters from it.   \n\nA couple of common transformations we have seen already are 'centering' and 'standardising'. \nWhen we \"center\" a variable, we subtracting some number (often the mean of the variable) from every value. So if we 'mean-center' a variable measuring height in cm, and the mean height of my sample is 175cm, then a value of 190 becomes +15, and a value of 150 becomes -25, and so on.  \nWhen we 'standardise' a variable, we mean-center it and then divide the resulting values by the standard deviation. So if the standard deviation of heights in my sample is 15, then the value of 190 becomes $\\frac{190-175}{15} = \\frac{15}{15} = 1$, and the 150 becomes $\\frac{150-175}{15} = \\frac{-25}{15} = -1.67$.  \n\nHow does this choice affect the linear models we might be fitting? The short answer is that it doesn't! The overall fit of `lm()` is not changed in any way when we apply these linear transformations to predictors or outcomes.^[the fit of models _does_ change if we apply a non-linear transformation, such as $x^2$, $log(x)$, etc., and this can sometimes be useful for studying effects that are more likely to be non-linear!]. However, transformations _do_ change what we get out of our model. \n\nIf we re-center a predictor on some new value (such as the mean), then all this does is change what \"zero\" means in our variable. This means that if we re-center a predictor in our linear model, the only thing that changes is our intercept. This is because the intercept is \"when all predictors are zero\". And we are changing what \"zero\" represents!  \n\nWhen we scale a predictor, this will change the slope. Why? Because it changes what \"moving 1\" represents. So if we standardise a variable, it changes both the intercept and the slope. However, note that the significance of the slope remains _exactly the same_, we are only changing the *units* that we are using to expressing that slope.\n\nThe example below shows a model of heart rates (`HR`) predicted by hours slept (`hrs_sleep`). In @fig-scalexlm you can see our original model (top left), and then various transformations applied to our predictor. Note how these transformations don't affect the model itself - the regression line (and the uncertainty in the line) is the same in each plot.   \n\nWe can see that re-centering changes what the intercept represents. In the top left plot, 0 represents zero hours slept, so the intercept is the estimated heart rate for someone who didn't sleep at all. Similarly, in the top right plot, 0 now represents the mean hours slept, so the intercept is the heart rate for someone who slept an average amount. In the bottom left plot (where hours slept is 'standardized'), not only have we changed what \"0\" represents, but we have changed what moving \"1\" represents. Rather being an increase of 1 hour of sleep, in this plot it represents an increase of 1 standard deviation hours sleep (whatever that is for our sample - it looks to be about 2.5). This means our estimated slope is the change in heart rate when having c2.5 more hours sleep. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Centering and scaling predictors in linear regression models. Intercepts and their interpretation change when re-centered, and slope coefficients and their interpretation change when scaling, but the overall model stays the same.](08_centering_files/figure-html/fig-scalexlm-1.png){#fig-scalexlm fig-align='center' width=100%}\n:::\n:::\n\n\nThe thing to note is that the lines themselves are all the same, because the models are all exactly the same. We can prove this to ourselves by comparing the 4 models: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nhrdat <- read_csv(\"https://uoepsy.github.io/data/usmr_hrsleep.csv\")\n\n# original model:\nmod_orig <- lm(HR ~ hrs_sleep, data = hrdat)\n# model with hrs_sleep mean centered\nmod_mc <- lm(HR ~ scale(hrs_sleep, scale=FALSE), data = hrdat)\n# model with hrs_sleep standardised\nmod_z <- lm(HR ~ scale(hrs_sleep), data = hrdat)\n# model with hrs_sleep centered on 8 hours the I() function is a handy function that is just needed because the + and - symbols normally get interprted in lm() as adding and removing predictors. \nmod_8 <- lm(HR ~ I(hrs_sleep-8), data = hrdat) \n\n# all models are identical fit\nanova(mod_orig, mod_mc, mod_z, mod_8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: HR ~ hrs_sleep\nModel 2: HR ~ scale(hrs_sleep, scale = FALSE)\nModel 3: HR ~ scale(hrs_sleep)\nModel 4: HR ~ I(hrs_sleep - 8)\n  Res.Df    RSS Df   Sum of Sq F Pr(>F)\n1     68 3524.5                        \n2     68 3524.5  0 -4.5475e-13         \n3     68 3524.5  0  0.0000e+00         \n4     68 3524.5  0  0.0000e+00         \n```\n:::\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n#### Centering when we have interactions\n\nWhen we have an interactions in a model such as `lm(y~x+z+x:z)`, the individual coefficients for `x` and `z` are specifically the associations \"when the other variable included in the interaction is zero\". Because re-centering a variable changes the meaning of \"zero\", this means that these two coefficients will change.  \n\nFor instance, a model of heart rates (`HR`) that includes an interaction between `hrs_sleep` and whether someone smokes, our coefficient for `smoke` estimates the difference in HR between smokers vs non-smokers who get zero hours of sleep (red to blue point in the left-hand plot of @fig-intcent). If we mean-center the `hrs_sleep` variable in our model, then it becomes the estimated difference in HR between smokers vs non-smokers who get the average hours of sleep (red to blue point in the right-hand plot of @fig-intcent). \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![mean-centering a variable that is involved in an interaction will change the point at which the marginal effect of other variable is estimated at](08_centering_files/figure-html/fig-intcent-1.png){#fig-intcent fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n## Centering predictors in multilevel models\n\nIn multilevel models, things can be a little bit different. We have already seen in the longitudinal example ([Chapter 5](05_long.html#modelling-change-over-time){target=\"_blank\"}) that recentering a variable can have certain advantages in allowing us to fit multilevel models.  \n\nBecause multilevel models involve estimating group-level variability in intercepts and slopes, if our intercept is very far away from our data (e.g., if all our data is from ages 60 to 80, and we are estimating variability at age 0), then slight changes in a slope can have huge influences on intercepts (@fig-centconv), resulting in models that don't converge.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![lines indicate predicted values from the model with random intercepts and random slopes of age. Due to how age is coded, the 'intercept' is estimated back at age 0](08_centering_files/figure-html/fig-centconv-1.png){#fig-centconv fig-align='center' width=80%}\n:::\n:::\n\n\nSo re-centering a predictor can help both to allow models to converge and to provide estimates of variability that are at more meaningful places (e.g., we could recenter the `age` variable on 60 and the intercept variation would become the variability in peoples' cognition _at the start of the study period_). Beyond that, re-centering on a constant number (such as the overall mean, min or max) doesn't change much in terms of our fixed effects. \n\nHowever, in some cases (typically in observational, rather than experimental studies), having multi-level data may mean that we don't just have _one_ overall mean for a predictor, but also a separate mean for each group.  \n\n### group mean centering\n\nLet's suppose we have a \n\n\n### within and between effects\n\n\n\n\n### the within between model\n\ny ~ 1 + xbarj + (x-xbarj) + ( 1 + (x-xbarj) | g )\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngconf = function(){\n  N = 200\n  n_groups = 20\n  g = rep(1:n_groups, e = N/n_groups)\n  u = rnorm(n_groups,0,5)[g]\n  x = rnorm(N,u)\n\n  re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,0,0,.5),ncol=2))\n  re1 = re[,1][g]\n  re_x = re[,2][g]\n  lp = (0 + re1) + (1 + re_x) * x + -3*u\n  y = rnorm(N, mean = lp, sd = 1)\n  \n  df = data.frame(x, g = factor(g), y, y)\n  # ggplot(df,aes(x=x,y=y,col=g))+\n  #   geom_point()+guides(col=\"none\")+\n  #   geom_smooth(method=lm,se=F)\n  \n  c(\n    ri=fixef(lmer(y~1+x+(1|g),df))['x'],\n    rs=fixef(lmer(y~1+x+(1+x|g),df))['x'],\n    mui=fixef(lmer(y~1+x+xm+(1|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mu=fixef(lmer(y~1+x+xm+(1+x|g),df |> group_by(g) |>mutate(xm=mean(x))))['x'],\n    mwb=fixef(lmer(y~1+xd+xm+(1+xd|g),df |> group_by(g) |>mutate(xm=mean(x),xd=x-xm)))['xd']\n  )\n}\n\nres = t(replicate(100,gconf()))\npar(mfrow=c(3,2))\nhist(res[,1]);hist(res[,2]);hist(res[,3]);hist(res[,4]);hist(res[,5])\npar(mfrow=c(1,1))\ncolMeans(res)\napply(res,2,sd)\n```\n:::\n",
    "supporting": [
      "08_centering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.2.6/panelset.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/panelset-0.2.6/panelset.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}